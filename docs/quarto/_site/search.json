[
  {
    "objectID": "quickstart.html",
    "href": "quickstart.html",
    "title": "Quickstart",
    "section": "",
    "text": "This guide will help you get started with fsspec-utils by demonstrating how to create and interact with a directory-based filesystem for local paths.\n\n\nFirst, ensure you have fsspec-utils installed.\npip install fsspec-utils\n\n\n\nfsspec-utils simplifies working with various file systems by providing a unified interface. Here, we’ll create a DirFileSystem for a local directory.\nThe filesystem function from fsspec_utils allows you to instantiate a file system object. By setting dirfs=True, you indicate that you want a directory-based filesystem, which treats directories as files themselves.\nLet’s create a local directory and then instantiate a DirFileSystem for it:\nimport os\nfrom fsspec_utils import filesystem\n\n# Define a local directory path\nlocal_dir_path = \"./my_local_data/\"\n\n# Ensure the directory exists\nos.makedirs(local_dir_path, exist_ok=True)\n\n# Create a DirFileSystem for the local path\nfs_dir_local = filesystem(local_dir_path, dirfs=True)\n\nprint(f\"Local DirFileSystem created: {fs_dir_local}\")\n\n# You can now use the fs_dir_local object to interact with the directory\n# For example, to list its contents (initially empty)\nprint(f\"Contents of {local_dir_path}: {fs_dir_local.ls('/')}\")\n\n# Let's create a dummy file inside the directory\nwith fs_dir_local.open(\"test_file.txt\", \"w\") as f:\n    f.write(\"Hello, fsspec-utils!\")\n\nprint(f\"Contents after creating test_file.txt: {fs_dir_local.ls('/')}\")\n\n# Read the content of the dummy file\nwith fs_dir_local.open(\"test_file.txt\", \"r\") as f:\n    content = f.read()\nprint(f\"Content of test_file.txt: {content}\")\n\n# Clean up the created directory and file\nfs_dir_local.rm(\"test_file.txt\")\nos.rmdir(local_dir_path)\nprint(f\"Cleaned up {local_dir_path}\")\n\n\n\nimport os and from fsspec_utils import filesystem: We import the necessary modules. os is used here to ensure the local directory exists, and filesystem is the core function from fsspec-utils.\nlocal_dir_path = \"./my_local_data/\": We define a relative path for our local directory.\nos.makedirs(local_dir_path, exist_ok=True): This line creates the my_local_data directory if it doesn’t already exist.\nfs_dir_local = filesystem(local_dir_path, dirfs=True): This is where fsspec-utils comes into play. We create a DirFileSystem instance pointing to our local directory. The dirfs=True argument is crucial for enabling directory-level operations.\nfs_dir_local.ls('/'): We use the ls method of our fs_dir_local object to list the contents of the root of our my_local_data directory. Initially, it will be empty.\nfs_dir_local.open(\"test_file.txt\", \"w\"): We demonstrate writing a file within our DirFileSystem using the open method, similar to Python’s built-in open.\nfs_dir_local.open(\"test_file.txt\", \"r\"): We demonstrate reading the content of the file we just created.\nfs_dir_local.rm(\"test_file.txt\") and os.rmdir(local_dir_path): Finally, we clean up by removing the created file and the directory.\n\nThis example provides a basic overview of how to use fsspec-utils to interact with a local directory as a filesystem. The same filesystem function can be used for various other storage backends like S3, GCS, HDFS, etc., by simply changing the path and providing appropriate storage_options."
  },
  {
    "objectID": "quickstart.html#installation",
    "href": "quickstart.html#installation",
    "title": "Quickstart",
    "section": "",
    "text": "First, ensure you have fsspec-utils installed.\npip install fsspec-utils"
  },
  {
    "objectID": "quickstart.html#basic-usage-local-directory-filesystem",
    "href": "quickstart.html#basic-usage-local-directory-filesystem",
    "title": "Quickstart",
    "section": "",
    "text": "fsspec-utils simplifies working with various file systems by providing a unified interface. Here, we’ll create a DirFileSystem for a local directory.\nThe filesystem function from fsspec_utils allows you to instantiate a file system object. By setting dirfs=True, you indicate that you want a directory-based filesystem, which treats directories as files themselves.\nLet’s create a local directory and then instantiate a DirFileSystem for it:\nimport os\nfrom fsspec_utils import filesystem\n\n# Define a local directory path\nlocal_dir_path = \"./my_local_data/\"\n\n# Ensure the directory exists\nos.makedirs(local_dir_path, exist_ok=True)\n\n# Create a DirFileSystem for the local path\nfs_dir_local = filesystem(local_dir_path, dirfs=True)\n\nprint(f\"Local DirFileSystem created: {fs_dir_local}\")\n\n# You can now use the fs_dir_local object to interact with the directory\n# For example, to list its contents (initially empty)\nprint(f\"Contents of {local_dir_path}: {fs_dir_local.ls('/')}\")\n\n# Let's create a dummy file inside the directory\nwith fs_dir_local.open(\"test_file.txt\", \"w\") as f:\n    f.write(\"Hello, fsspec-utils!\")\n\nprint(f\"Contents after creating test_file.txt: {fs_dir_local.ls('/')}\")\n\n# Read the content of the dummy file\nwith fs_dir_local.open(\"test_file.txt\", \"r\") as f:\n    content = f.read()\nprint(f\"Content of test_file.txt: {content}\")\n\n# Clean up the created directory and file\nfs_dir_local.rm(\"test_file.txt\")\nos.rmdir(local_dir_path)\nprint(f\"Cleaned up {local_dir_path}\")\n\n\n\nimport os and from fsspec_utils import filesystem: We import the necessary modules. os is used here to ensure the local directory exists, and filesystem is the core function from fsspec-utils.\nlocal_dir_path = \"./my_local_data/\": We define a relative path for our local directory.\nos.makedirs(local_dir_path, exist_ok=True): This line creates the my_local_data directory if it doesn’t already exist.\nfs_dir_local = filesystem(local_dir_path, dirfs=True): This is where fsspec-utils comes into play. We create a DirFileSystem instance pointing to our local directory. The dirfs=True argument is crucial for enabling directory-level operations.\nfs_dir_local.ls('/'): We use the ls method of our fs_dir_local object to list the contents of the root of our my_local_data directory. Initially, it will be empty.\nfs_dir_local.open(\"test_file.txt\", \"w\"): We demonstrate writing a file within our DirFileSystem using the open method, similar to Python’s built-in open.\nfs_dir_local.open(\"test_file.txt\", \"r\"): We demonstrate reading the content of the file we just created.\nfs_dir_local.rm(\"test_file.txt\") and os.rmdir(local_dir_path): Finally, we clean up by removing the created file and the directory.\n\nThis example provides a basic overview of how to use fsspec-utils to interact with a local directory as a filesystem. The same filesystem function can be used for various other storage backends like S3, GCS, HDFS, etc., by simply changing the path and providing appropriate storage_options."
  },
  {
    "objectID": "api/fsspec_utils.utils.sql.html",
    "href": "api/fsspec_utils.utils.sql.html",
    "title": "fsspec_utils.utils.sql API Reference",
    "section": "",
    "text": "Generates a filter expression for PyArrow based on a given string and schema.\nParameters:\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nstring\nstr\nThe string containing the filter expression.\n\n\nschema\npyarrow.Schema\nThe PyArrow schema used to validate the filter expression.\n\n\n\nReturns:\n\npyarrow.compute.Expression: The generated filter expression.\n\nRaises:\n\nValueError: If the input string is invalid or contains unsupported operations.\n\n\n\n\nGenerates a filter expression for Polars based on a given string and schema.\nParameters:\n\n\n\nName\nType\nDescription\n\n\n\n\n\nstring | str | The string containing the filter expression. |\nschema | polars.Schema | The Polars schema used to validate the filter expression. |\nReturns:\n\npolars.Expr: The generated filter expression.\n\nRaises:\n\nValueError: If the input string is invalid or contains unsupported operations.\n\n\n\n\nParameters:\n\n\n\nName\nType\nDescription\n\n\n\n\nsql_query\nstr\nThe SQL query string to parse.\n\n\n\nExample:\nfrom fsspec_utils.utils.sql import get_table_names\n\nquery = \"SELECT a FROM my_table WHERE b &gt; 10\"\ntables = get_table_names(query)\nprint(tables)\n# Expected: ['my_table']\n\nquery_join = \"SELECT t1.a, t2.b FROM table1 AS t1 JOIN table2 AS t2 ON t1.id = t2.id\"\ntables_join = get_table_names(query_join)\nprint(tables_join)\n# Expected: ['table1', 'table2']\nReturns:\n\nNone"
  },
  {
    "objectID": "api/fsspec_utils.utils.sql.html#sql2pyarrow_filter",
    "href": "api/fsspec_utils.utils.sql.html#sql2pyarrow_filter",
    "title": "fsspec_utils.utils.sql API Reference",
    "section": "",
    "text": "Generates a filter expression for PyArrow based on a given string and schema.\nParameters:\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nstring\nstr\nThe string containing the filter expression.\n\n\nschema\npyarrow.Schema\nThe PyArrow schema used to validate the filter expression.\n\n\n\nReturns:\n\npyarrow.compute.Expression: The generated filter expression.\n\nRaises:\n\nValueError: If the input string is invalid or contains unsupported operations."
  },
  {
    "objectID": "api/fsspec_utils.utils.sql.html#sql2polars_filter",
    "href": "api/fsspec_utils.utils.sql.html#sql2polars_filter",
    "title": "fsspec_utils.utils.sql API Reference",
    "section": "",
    "text": "Generates a filter expression for Polars based on a given string and schema.\nParameters:\n\n\n\nName\nType\nDescription\n\n\n\n\n\nstring | str | The string containing the filter expression. |\nschema | polars.Schema | The Polars schema used to validate the filter expression. |\nReturns:\n\npolars.Expr: The generated filter expression.\n\nRaises:\n\nValueError: If the input string is invalid or contains unsupported operations."
  },
  {
    "objectID": "api/fsspec_utils.utils.sql.html#get_table_names",
    "href": "api/fsspec_utils.utils.sql.html#get_table_names",
    "title": "fsspec_utils.utils.sql API Reference",
    "section": "",
    "text": "Parameters:\n\n\n\nName\nType\nDescription\n\n\n\n\nsql_query\nstr\nThe SQL query string to parse.\n\n\n\nExample:\nfrom fsspec_utils.utils.sql import get_table_names\n\nquery = \"SELECT a FROM my_table WHERE b &gt; 10\"\ntables = get_table_names(query)\nprint(tables)\n# Expected: ['my_table']\n\nquery_join = \"SELECT t1.a, t2.b FROM table1 AS t1 JOIN table2 AS t2 ON t1.id = t2.id\"\ntables_join = get_table_names(query_join)\nprint(tables_join)\n# Expected: ['table1', 'table2']\nReturns:\n\nNone"
  },
  {
    "objectID": "api/fsspec_utils.core.ext.html",
    "href": "api/fsspec_utils.core.ext.html",
    "title": "fsspec_utils.core.ext API Documentation",
    "section": "",
    "text": "This module provides extended functionalities for fsspec.AbstractFileSystem, including methods for reading and writing various file formats (JSON, CSV, Parquet) with advanced options like batch processing, parallelization, and data type optimization. It also includes functions for creating PyArrow and Pydala datasets.\n\n\n\nConvert a path to a glob pattern for file matching.\nIntelligently converts paths to glob patterns that match files of the specified format, handling various directory and wildcard patterns.\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\n\n\n\n\npath\nstr\nBase path to convert. Can include wildcards (* or **). Examples: “data/”, “data/*.json”, “data/**”\n\n\nformat\nstr | None\nFile format to match (without dot). If None, inferred from path. Examples: “json”, “csv”, “parquet”\n\n\n\n\n\n\n\n\n\n\n\nReturns\nType\nDescription\n\n\n\n\nstr\nstr\nGlob pattern that matches files of specified format. Examples: “data/**/.json”, “data/.csv”\n\n\n\nExample:\n# Basic directory\npath_to_glob(\"data\", \"json\")\n# 'data/**/*.json'\n\n# With wildcards\npath_to_glob(\"data/**\", \"csv\")\n# 'data/**/*.csv'\n\n# Format inference\npath_to_glob(\"data/file.parquet\")\n# 'data/file.parquet'\n\n\n\n\nRead a single JSON file from any filesystem.\nA public wrapper around _read_json_file providing a clean interface for reading individual JSON files.\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\n\n\n\n\nself\nAbstractFileSystem\nFilesystem instance to use for reading\n\n\npath\nstr\nPath to JSON file to read\n\n\ninclude_file_path\nbool\nWhether to return dict with ilepath as key\n\n\njsonlines\nbool\nWhether to read as JSON Lines format\n\n\n\n\n\n\n\n\n\n\n\nReturns\nType\nDescription\n\n\n\n\ndict or list[dict]\ndict or list[dict]\nParsed JSON data. For regular JSON, returns a dict. For JSON Lines, returns a list of dicts. If include_file_path=True, returns {filepath: data}.\n\n\n\nExample:\nfrom fsspec.implementations.local import LocalFileSystem\n\nfs = LocalFileSystem()\n# Read regular JSON\ndata = fs.read_json_file(\"config.json\")\nprint(data[\"setting\"])\n# 'value'\n\n# Read JSON Lines with filepath\ndata = fs.read_json_file(\n    \"logs.jsonl\",\n    include_file_path=True,\n    jsonlines=True\n)\nprint(list(data.keys())[0])\n# 'logs.jsonl'\n\n\n\n\nRead JSON data from one or more files with powerful options.\nProvides a flexible interface for reading JSON data with support for:\n\nSingle file or multiple f\nRegular JSON or JSON Lines format\nBatch processing for large datasets\nParallel processing\nDataFrame conversion\nFile path tracking\n\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\n\n\n\n\npath\nstr or list[str]\nPath(s) to JSON file(s). Can be: - Single path string (globs supported) - List of path strings\n\n\nbatch_size\nint | None\nIf set, enables batch reading with this many files per batch\n\n\ninclude_file_path\nbool\nInclude source filepath in output\n\n\njsonlines\nbool\nWhether to read as JSON Lines format\n\n\nas_dataframe\nbool\nConvert output to Polars DataFrame(s)\n\n\nconcat\nbool\nCombine multiple files/batches into single result\n\n\nuse_threads\nbool\nEnable parallel file reading\n\n\nverbose\nbool\nPrint progress information\n\n\nopt_dtypes\nbool\nOptimize DataFrame dtypes for performance\n\n\n**kwargs\nAny\nAdditional arguments passed to DataFrame conversion\n\n\n\n\n\n\n\n\n\n\n\nReturns\nType\nDescription\n\n\n\n\ndict or list[dict] or pl.DataFrame or list[pl.DataFrame] or Generator\nVarious types depending on arguments: - dict: Single JSON file as dictionary - list[dict]: Multiple JSON files as list of dictionaries - pl.DataFrame: Single or concatenated DataFrame - list[pl.DataFrame]: List of Dataframes (if concat=False) - Generator: If batch_size set, yields batches of above types\n\n\n\n\nExample:\nfrom fsspec.implementations.local import LocalFileSystem\n\nfs = LocalFileSystem()\n# Read all JSON files in directory\ndf = fs.read_json(\n    \"data/*.json\",\n    as_dataframe=True,\n    concat=True\n)\nprint(df.shape)\n# (1000, 5)  # Combined data from all files\n\n# Batch process large dataset\nfor batch_df in fs.read_json(\n    \"logs/*.jsonl\",\n    batch_size=100,\n    jsonlines=True,\n    include_file_path=True\n):\n    print(f\"Processing {len(batch_df)} records\")\n\n# Parallel read with custom options\ndfs = fs.read_json(\n    [\"file1.json\", \"file2.json\"],\n    use_threads=True,\n    concat=False,\n    verbose=True\n)\nprint(f\"Read {len(dfs)} files\")\n\n\n\n\nRead a single CSV file from any filesystem.\nInternal function that handles reading individual CSV files and optionally adds the source filepath as a column.\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\n\n\n\n\nself\nAbstractFileSystem\nFilesystem instance to use for reading\n\n\npath\nstr\nPath to CSV file\n\n\ninclude_file_path\nbool\nAdd source filepath as a column\n\n\nopt_dtypes\nbool\nOptimize DataFrame dtypes\n\n\n**kwargs\nAny\nAdditional arguments passed to pl.read_csv()\n\n\n\n\n\n\nReturns\nType\nDescription\n\n\n\n\npl.DataFrame\npl.DataFrame\nDataFrame containing CSV data\n\n\n\nExample:\nfrom fsspec.implementations.local import LocalFileSystem\n\nfs = LocalFileSystem()\n# This example assumes _read_csv_file is an internal method or needs to be called differently.\n# For public use, you would typically use fs.read_csv().\n# df = fs.read_csv_file(\n#     \"data.csv\",\n#     include_file_path=True,\n#     delimiter=\"|\"\n# )\n# print(\"file_path\" in df.columns)\n# True\n\n\n\n\nRead CSV data from one or more files with powerful options.\nProvides a flexible interface for reading CSV files with support for:\n\nSingle file or multiple files\nBatch processing for large datasets\nParallel processing\nFile path tracking\nPolars DataFrame output\n\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\n\n\n\n\npath\nstr or list[str]\nPath(s) to CSV file(s). Can be: - Single path string (globs supported) - List of path strings\n\n\nbatch_size\nint | None\nIf set, enables batch reading with this many files per batch\n\n\ninclude_file_path\nbool\nAdd source filepath as a column\n\n\nconcat\nbool\nCombine multiple files/batches into single DataFrame\n\n\nuse_threads\nbool\nEnable parallel file reading\n\n\nverbose\nbool\nPrint progress information\n\n\n**kwargs\nAny\nAdditional arguments passed to pl.read_csv()\n\n\n\n\n\n\n\n\n\n\n\nReturns\nType\nDescription\n\n\n\n\npl.DataFrame or list[pl.DataFrame] or Generator\nVarious types depending on arguments: - pl.DataFrame: Single or concatenated DataFrame - list[pl.DataFrame]: List of DataFrames (if concat=False) - Generator: If batch_size set, yields batches of above types\n\n\n\n\nExample:\nfrom fsspec.implementations.local import LocalFileSystem\n\nfs = LocalFileSystem()\n# Read all CSVs in directory\ndf = fs.read_csv(\n    \"data/*.csv\",\n    include_file_path=True\n)\nprint(df.columns)\n# ['file_path', 'col1', 'col2', ...]\n\n# Batch process large dataset\nfor batch_df in fs.read_csv(\n    \"logs/*.csv\",\n    batch_size=100,\n    use_threads=True,\n    verbose=True\n):\n    print(f\"Processing {len(batch_df)} rows\")\n\n# Multiple files without concatenation\ndfs = fs.read_csv(\n    [\"file1.csv\", \"file2.csv\"],\n    concat=False,\n    use_threads=True\n)\nprint(f\"Read {len(dfs)} files\")\n\n\n\n\nRead a single Parquet file from any filesystem.\nInternal function that handles reading individual Parquet files and optionally adds the source filepath as a column.\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\n\n\n\n\nself\nAbstractFileSystem\nFilesystem instance to use for reading\n\n\npath\nstr\nPath to Parquet file\n\n\ninclude_file_path\nbool\nAdd source filepath as a column\n\n\nopt_dtypes\nbool\nOptimize DataFrame dtypes\n\n\n**kwargs\nAny\nAdditional arguments passed to pq.read_table()\n\n\n\n\n\n\nReturns\nType\nDescription\n\n\n\n\npa.Table\npa.Table\nPyArrow Table containing Parquet data\n\n\n\nExample:\nfrom fsspec.implementations.local import LocalFileSystem\n\nfs = LocalFileSystem()\n# This example assumes _read_parquet_file is an internal method or needs to be called differently.\n# For public use, you would typically use fs.read_parquet().\n# table = fs.read_parquet_file(\n#     \"data.parquet\",\n#     include_file_path=True,\n#     use_threads=True\n# )\n# print(\"file_path\" in table.column_names)\n# True\n\n\n\n\nRead Parquet data with advanced features and optimizations.\nProvides a high-performance interface for reading Parquet files with support for:\n\nSingle file or multiple files\nBatch processing for large datasets\nParallel processing\nFile path tracking\nAutomatic concatenation\nPyArrow Table output\n\nThe function automatically uses optimal reading strategies:\n\nDirect dataset reading for simple cases\nParallel processing for multiple files\nBatched reading for memory efficiency\n\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\n\n\n\n\npath\nstr or list[str]\nPth(s) to Parquet file(s). Can be: - Single path string (globs supported) - List of path strings - Directory containing _metadata file\n\n\nbatch_size\nint | None\nIf set, enables batch reading with this many files per batch\n\n\ninclude_file_path\nbool\nAdd source filepath as a column\n\n\nconcat\nbool\nCombine multiple files/batches into single Table\n\n\nuse_threads\nbool\nEnable parallel file reading\n\n\nverbose\nbool\nPrint progress information\n\n\nopt_dtypes\nbool\nOptimize Table dtypes for performance\n\n\n**kwargs\nAny\nAdditional arguments passed to pq.read_table()\n\n\n\n\n\n\n\n\n\n\n\nReturns\nType\nDescription\n\n\n\n\npa.Table or list[pa.Table] or Generator\nVarious types depending on arguments: - pa.Table: Single or concatenated Table - list[pa.Table]: List of Tables (if concat=False) - Generator: If batch_size set, yields batches of above types\n\n\n\n\nExample:\nfrom fsspec.implementations.local import LocalFileSystem\n\nfs = LocalFileSystem()\n# Read all Parquet files in directory\ntable = fs.read_parquet(\n    \"data/*.parquet\",\n    include_file_path=True\n)\nprint(table.column_names)\n# ['file_path', 'col1', 'col2', ...]\n\n# Batch process large dataset\nfor batch in fs.read_parquet(\n    \"data/*.parquet\",\n    batch_size=100,\n    use_threads=True\n):\n    print(f\"Processing {batch.num_rows} rows\")\n\n# Read from directory with metadata\ntable = fs.read_parquet(\n    \"data/\",  # Contains _metadata\n    use_threads=True\n)\nprint(f\"Total rows: {table.num_rows}\")\n\n\n\n\nUniversal interface for reading data files of any supported format.\nA unified API that automatically delegates to the appropriate reading function based on file format, while preserving all advanced features like:\n\nBatch processing\nParallel reading\nFile path tracking\nFormat-specific optimizations\n\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\n\n\n\n\npath\nstr or list[str]\nPath(s) to data file(s). Can be: - Single path string (globs supported) - List of path strings\n\n\nformat\nstr| File format to read. Supported values: - \"json\": Regular JSON or JSON Lines - \"csv\": CSV files - \"parquet\": Parquet files | |batch_size|int\nNone| If set, enables batch reading with this many files per batch | |include_file_path|bool| Add source filepath as column/field | |concat|bool| Combine multiple files/batches into single result | |jsonlines|bool| For JSON format, whether to read as JSON Lines | |use_threads|bool| Enable parallel file reading | |verbose|bool| Print progress information | |opt_dtypes|bool| Optimize DataFrame/Arrow Table dtypes for performance | |**kwargs|Any`\n\n\n\n\n\n\n\n\n\n\n\nReturns\nType\nDescription\n\n\n\n\npl.DataFrame or pa.Table or list[pl.DataFrame] or list[pa.Table] or Generator\nVarious types depending on format and arguments: - pl.DataFrame: For CSV and optionally JSON - pa.Table: For Parquet - list[pl.DataFrame or pa.Table]: Without concatenation - Generator: If batch_size set, yields batches\n\n\n\n\nExample:\nfrom fsspec.implementations.local import LocalFileSystem\n\nfs = LocalFileSystem()\n# Read CSV files\ndf = fs.read_files(\n    \"data/*.csv\",\n    format=\"csv\",\n    include_file_path=True\n)\nprint(type(df))\n# &lt;class 'polars.DataFrame'&gt;\n\n# Batch process Parquet files\nfor batch in fs.read_files(\n    \"data/*.parquet\",\n    format=\"parquet\",\n    batch_size=100,\n    use_threads=True\n):\n    print(f\"Batch type: {type(batch)}\")\n\n# Read JSON Lines\ndf = fs.read_files(\n    \"logs/*.jsonl\",\n    format=\"json\",\n    jsonlines=True,\n    concat=True\n)\nprint(df.columns)\n\n\n\n\nCreate a PyArrow dataset from files in any supported format.\nCreates a dataset that provides optimized reading and querying capabilities including:\n\nSchema inference and enforcement\nPartition discovery and pruning\nPredicate pushdown\nColumn projection\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\n\n\n\n\npath\nstr\nBase path to dataset files\n\n\nformat\nstr\nFile format. Currently supports: - “parquet” (default) - “csv” - “json” (experimental)\n\n\nschema\npa.Schema | None\nOptional schema to enforce. If None, inferred from data.\n\n\n\n\npartitioning | str or list[str] or pds.Partitioning | How the dataset is partitoned. Can be: - str: Single partition field - list[str]: Multiple partition fields - pds.Partitioning: Custom partitioning scheme |\n**kwargs | Any | Additional arguments for dataset creation |\nReturns | Type | Description |\n:—— | :— | :———- |\npds.Dataset | pds.Dataset | PyArrow dataset instance |\nExample:\nfrom fsspec.implementations.local import LocalFileSystem\n\nfs = LocalFileSystem()\n# Simple Parquet dataset\nds = fs.pyarrow_dataset(\"data/\")\nprint(ds.schema)\n\n# Partitioned dataset\nds = fs.pyarrow_dataset(\n    \"events/\",\n    partitioning=[\"year\", \"month\"]\n)\n# Query with partition pruning\ntable = ds.to_table(\n    filter=(ds.field(\"year\") == 2024)\n)\n\n# CSV with schema\nds = fs.pyarrow_dataset(\n    \"logs/\",\n    format=\"csv\",\n    schema=pa.schema([\n        (\"timestamp\", pa.timestamp(\"s\")),\n        (\"level\", pa.string()),\n        (\"message\", pa.string())\n    ])\n)\n\n\n\n\nCreate a PyArrow dataset optimized for Parquet files.\nCreates a dataset specifically for Parquet data, automatically handling _metadata files for optimized reading.\nThis function is particularly useful for:\n\nDatasets with existing _metadata files\nMulti-file datasets that should be treated as one\nPartitioned Parquet datasets\n\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\n\n\n\n\npath\nstr\nPath to dataset directory or _metadata file\n\n\nschema\npa.Schema | None\nOptional schema to enforce. If None, inferred from data.\n\n\npartitioning\nstr or list[str] or pds.Partitioning\nHow the dataset is partitoned. Can be: - str: Single partition field - list[str]: Multiple partition fields - pds.Partitioning: Custom partitioning scheme\n\n\n**kwargs\nAny\nAdditional dataset arguments\n\n\n\n\n\n\nReturns\nType\nDescription\n\n\n\n\npds.Dataset\npds.Dataset\nPyArrow dataset instance\n\n\n\nExample:\nfrom fsspec.implementations.local import LocalFileSystem\n\nfs = LocalFileSystem()\n# Dataset with _metadata\nds = fs.pyarrow_parquet_dataset(\"data/_metadata\")\nprint(ds.files)  # Shows all data files\n\n# Partitioned dataset directory\nds = fs.pyarrow_parquet_dataset(\n    \"sales/\",\n    partitioning=[\"year\", \"region\"]\n)\n# Query with partition pruning\ntable = ds.to_table(\n    filter=(\n        (ds.field(\"year\") == 2024) &\n        (ds.field(\"region\") == \"EMEA\")\n    )\n)\n\n\n\n\nCreate a Pydala dataset for advanced Parquet operations.\nCreates a dataset with additional features beyond PyArrow including:\n\nDelta table support\nSchema evolution\nAdvanced partitioning\nMetadata management\nSort key optimization\n\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\n\n\n\n\npath\nstr\nPath to dataset directory\n\n\npartitioning\nstr or list[str] or pds.Partitioning\nHow the dataset is partitoned. Can be: - str: Single partition field - list[str]: Multiple partition fields - pds.Partitioning: Custom partitioning scheme\n\n\n**kwargs\nAny\nAdditional dataset configuration\n\n\n\n\n\n\nReturns\nType\nDescription\n\n\n\n\nParquetDataset\nParquetDataset\nPydala dataset instance\n\n\n\nExample:\nfrom fsspec.implementations.local import LocalFileSystem\n\nfs = LocalFileSystem()\n# Create dataset\nds = fs.pydala_dataset(\n    \"data/\",\n    partitioning=[\"date\"]\n)\n\n# Write with delta support\nds.write_to_dataset(\n    new_data,\n    mode=\"delta\",\n    delta_subset=[\"id\"]\n)\n\n# Read with metadata\ndf = ds.to_polars()\nprint(df.columns)\n\n\n\n\nWrite data to a Parquet file with automatic format conversion.\nHandles writing data from multiple input formats to Parquet with:\n\nAutomatic conversion to PyArrow\nSchema validation/coercion\nMetadata collection\nCompression and encoding options\n\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\n\n\n\n\ndata\npl.DataFrame or pl.LazyFrame or pa.Table or pd.DataFrame or dict o list[dict]\nInput data in various formats: - Polars DataFrame/LazyFrame - PyArrow Table - Pandas DataFrame - Dict or list of dicts\n\n\npath\nstr\nOutput Parquet file path\n\n\nschema\npa.Schema | None\nOptional schema to enforce on write\n\n\n**kwargs\nAny\nAdditional arguments for pq.write_table()\n\n\n\n\n\n\n\n\n\n\n\nReturns\nType\nDescription\n\n\n\n\npq.FileMetaData\npq.FileMetaData\nMetadata of written Parquet file\n\n\n\n\n\n\nRaises\nType\nDescription\n\n\n\n\nSchemaError\nSchemaError\nIf data doesn’t match schema\n\n\nValueError\nValueError\nIf data cannot be converted\n\n\n\nExample:\nfrom fsspec.implementations.local import LocalFileSystem\nimport polars as pl\nimport numpy as np\nimport pyarrow as pa\n\nfs = LocalFileSystem()\n# Write Polars DataFrame\ndf = pl.DataFrame({\n    \"id\": range(1000),\n    \"value\": pl.Series(np.random.randn(1000))\n})\nmetadata = fs.write_parquet(\n    df,\n    \"data.parquet\",\n    compression=\"zstd\",\n    compression_level=3\n)\nprint(f\"Rows: {metadata.num_rows}\")\n\n# Write with schema\nschema = pa.schema([\n    (\"id\", pa.int64()),\n    (\"value\", pa.float64())\n])\nmetadata = fs.write_parquet(\n    {\"id\": [1, 2], \"value\": [0.1, 0.2]},\n    \"data.parquet\",\n    schema=schema\n)\n\n\n\n\nWrite data to a JSON file with flexible input support.\nHandles writing data in various formats to JSON or JSON Lines, with optional appending for streaming writes.\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\n\n\n\n\ndata\ndict or pl.DataFrame or pl.LazyFrame or pa.Table or pd.DataFrame or dict or list[dict]\nInput data in various formats: - Dict or list of dicts - Polars DataFrame/LazyFrame - PyArrow Table - Pandas DataFrame\n\n\npath\nstr\nOutput JSON file path\n\n\nappend\nbool\nWhether to append to existing file (JSON Lines mode)\n\n\n\nExample:\nfrom fsspec.implementations.local import LocalFileSystem\nimport polars as pl\nimport pyarrow as pa\n\nfs = LocalFileSystem()\n# Write dictionary\ndata = {\"name\": \"test\", \"values\": [1, 2, 3]}\nfs.write_json(data, \"config.json\")\n\n# Stream records\ndf1 = pl.DataFrame({\"id\": [1], \"value\": [\"first\"]})\ndf2 = pl.DataFrame({\"id\": [2], \"value\": [\"second\"]})\nfs.write_json(df1, \"stream.jsonl\", append=False)\nfs.write_json(df2, \"stream.jsonl\", append=True)\n\n# Convert PyArrow\ntable = pa.table({\"a\": [1, 2], \"b\": [\"x\", \"y\"]})\nfs.write_json(table, \"data.json\")\n\n\n\n\nWrite data to a CSV file with flexible input support.\nHandles writing data from multiple formats to CSV with options for:\n\nAppending to existing files\nCustom delimiters and formatting\nAutomatic type conversion\nHeader handling\n\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\n\n\n\n\ndata\npl.DataFrame or pl.LazyFrame or pa.Table or pd.DataFrame or dict or list[dict]\nInput data in various formats: - Polars DataFrame/LazyFrame - PyArrow Table - Pandas DataFrame - Dict or list of dicts\n\n\npath\nstr\nOutput CSV file path\n\n\nappend\nbool\nWhether to append to existing file\n\n\n**kwargs\nAny\nAdditional arguments for CSV writing: - delimiter: Field separator (default “,”) - header: Whether to write header row - quote_char: Character for quoting fields - date_format: Format for date/time fields - float_precision: Decimal places for floats\n\n\n\nExample:\nfrom fsspec.implementations.local import LocalFileSystem\nimport polars as pl\nfrom datetime import datetime\nimport pyarrow as pa\n\nfs = LocalFileSystem()\n# Write Polars DataFrame\ndf = pl.DataFrame({\n    \"id\": range(100),\n    \"name\": [\"item_\" + str(i) for i in range(100)]\n})\nfs.write_csv(df, \"items.csv\")\n\n# Append records\nnew_items = pl.DataFrame({\n    \"id\": range(100, 200),\n    \"name\": [\"item_\" + str(i) for i in range(100, 200)]\n})\nfs.write_csv(\n    new_items,\n    \"items.csv\",\n    append=True,\n    header=False\n)\n\n# Custom formatting\ndata = pa.table({\n    \"date\": [datetime.now()],\n    \"value\": [123.456]\n})\nfs.write_csv(\n    data,\n    \"formatted.csv\",\n    date_format=\"%Y-%m-%d\",\n    float_precision=2\n)\n\n\n\n\nWrite a DataFrame to a file in the given format.\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\n\n\n\n\ndata\npl.DataFrame or pl.LazyFrame or pa.Table or pd.DataFrame or dict\nData to write.\n\n\npath\nstr\nPath to write the data.\n\n\nformat\nstr\nFormat of the file.\n\n\n**kwargs\nAny\nAdditional keyword arguments.\n\n\n\n\n\n\nReturns\nType\nDescription\n\n\n\n\nNone\nNone\n\n\n\n\n\n\n\n\nWrite a DataFrame or a list of DataFrames to a file or a list of files.\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\n\n\n\n\ndata\npl.DataFrame or pl.LazyFrame or pa.Table or pa.RecordBatch or pa.RecordBatchReader or pd.DataFrame or dict or list[pl.DataFrame or pl.LazyFrame or pa.Table or pa.RecordBatch or pa.RecordBatchReader or pd.DataFrame or dict]\nData to write.\n\n\npath\nstr or list[str]\nPath to write the data.\n\n\nbasename\nstr\nBasename of the files. Defaults to None.\n\n\nformat\nstr\nFormat of the data. Defaults to None.\n\n\nconcat\nbool\nIf True, concatenate the DataFrames. Defaults to True.\n\n\nunique\nbool or list[str] or str\nIf True, remove duplicates. Defaults to False.\n\n\nmode\nstr\nWrite mode. Defaults to ‘append’. Options: ‘append’, ‘overwrite’, ‘delete_matching’, ‘error_if_exists’.\n\n\nuse_threads\nbool\nIf True, use parallel processing. Defaults to True.\n\n\nverbose\nbool\nIf True, print verbose output Defaults to True.\n\n\n**kwargs\nAny\nAdditional keyword arguments.\n\n\n\n\n\n\nReturns\nType\nDescription\n\n\n\n\nNone\nNone\n\n\n\n\n\n\n\n\n\n\n\n\nRaises\nType\nDescription\n\n\n\n\nFileExistsError\nFileExistsError\nIf file already exists and mode is ‘error_if_exists’.\n\n\n\n\n\n\n\nWrite a tabular data to a PyArrow dataset.\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\n\n\n\n\ndata\npl.DataFrame or pa.Table or pa.RecordBatch or pa.RecordBatchReader or pd.DataFrame or list[pl.DataFrame] or list[pa.Table] or list[pa.RecordBatch] or list[pa.RecordBatchReader] or list[pd.DataFrame]\nData to write.\n\n\npath\nstr\nPath to write the data.\n\n\nbasename\nstr | None\nBasename of the files. Defaults to None.\n\n\nschema\npa.Schema | None\nSchema of the data. Defaults to None.\n\n\npartition_by\nstr or list[str] or pds.Partitioning or None\nPartitioning of the data. Defaults to None.\n\n\npartitioning_flavor\nstr\nPartitioning flavor. Defaults to ‘hive’.\n\n\nmode\nstr\nWrite mode. Defaults to ‘append’.\n\n\nformat\nstr | None\nFormat of the data. Defaults to ‘parquet’.\n\n\ncompression\nstr\nCompression algorithm. Defaults to ‘zstd’.\n\n\nmax_rows_per_file\nint | None\nMaximum number of rows per file. Defaults to 2,500,000.\n\n\nrow_group_size\nint | None\nRow group size. Defaults to 250,000.\n\n\nconcat\nbool\nIf True, concatenate the DataFrames. Defaults to True.\n\n\nunique\nbool or str or list[str]\nIf True, remove duplicates. Deaults to False.\n\n\n**kwargs\nAny\nAdditional keyword arguments for pds.write_dataset.\n\n\n\n\n\n\n\n\n\n\n\nReturns\nType\nDescription\n\n\n\n\nlist[pq.FileMetaData] or None\nList of Parquet file metadata or None.\n\n\n\n\n\n\n\n\nWrite a tabular data to a Pydala dataset.\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\n\n\n\n\ndata\npl.DataFrame or pa.Table or pa.RecordBatch or pa.RecordBatchReader or pd.DataFrame or list[pl.DataFrame] or list[pa.Table] or list[pa.RecordBatch] or list[pa.RecordBatchReader] or list[pd.DataFrame]\nData to write.\n\n\npath\nstr\nPath to write the data.\n\n\nmode\nstr\nWrite mode. Defaults to ‘append’. Options: ‘delta’, ‘overwrite’.\n\n\nbasename\nstr | None\nBasename of the files. Defaults to None.\n\n\npartition_by\nstr or list[str] or None\nPartitioning of the data. Defaults to None.\n\n\npartitioning_flavor\nstr\nPartitioning flavor. Defaults to ‘hive’.\n\n\nmax_rows_per_file\nint | None\nMaximum number of rows per file. Defaults to 2,500,000.\n\n\nrow_group_size\nint | None\nRow group size. Defaults to 250,000.\n\n\ncompression\nstr\nCompression algorithm. Defaults to ‘zstd’.\n\n\nsort_by\nstr or list[str] or list[tuple[str, str]] or None\nColumns to sort by. Defaults to None.\n\n\nunique\nbool or str or list[str]\nIf True, ensure unique values. Defaults to False.\n\n\ndelta_subset\nstr or list[str] or None\nSubset of columns to include in delta table. Defaults to None.\n\n\nupdate_metadata\nbool\nIf True, update metadata. Defaults to True.\n\n\nalter_schema\nbool\nIf True, alter schema. Defaults to False.\n\n\ntimestamp_column\nstr or None\nTimestamp column. Defaults to None.\n\n\nverbose\nbool\nIf True, print verbose output. Defaults to True.\n\n\n**kwargs\nAny\nAdditional keyword arguments for ParquetDataset.write_to_dataset.\n\n\n\n\n\n\nReturns\nType\nDescription\n\n\n\n\nNone\nNone"
  },
  {
    "objectID": "api/fsspec_utils.core.ext.html#path_to_glob",
    "href": "api/fsspec_utils.core.ext.html#path_to_glob",
    "title": "fsspec_utils.core.ext API Documentation",
    "section": "",
    "text": "Convert a path to a glob pattern for file matching.\nIntelligently converts paths to glob patterns that match files of the specified format, handling various directory and wildcard patterns.\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\n\n\n\n\npath\nstr\nBase path to convert. Can include wildcards (* or **). Examples: “data/”, “data/*.json”, “data/**”\n\n\nformat\nstr | None\nFile format to match (without dot). If None, inferred from path. Examples: “json”, “csv”, “parquet”\n\n\n\n\n\n\n\n\n\n\n\nReturns\nType\nDescription\n\n\n\n\nstr\nstr\nGlob pattern that matches files of specified format. Examples: “data/**/.json”, “data/.csv”\n\n\n\nExample:\n# Basic directory\npath_to_glob(\"data\", \"json\")\n# 'data/**/*.json'\n\n# With wildcards\npath_to_glob(\"data/**\", \"csv\")\n# 'data/**/*.csv'\n\n# Format inference\npath_to_glob(\"data/file.parquet\")\n# 'data/file.parquet'"
  },
  {
    "objectID": "api/fsspec_utils.core.ext.html#read_json_file",
    "href": "api/fsspec_utils.core.ext.html#read_json_file",
    "title": "fsspec_utils.core.ext API Documentation",
    "section": "",
    "text": "Read a single JSON file from any filesystem.\nA public wrapper around _read_json_file providing a clean interface for reading individual JSON files.\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\n\n\n\n\nself\nAbstractFileSystem\nFilesystem instance to use for reading\n\n\npath\nstr\nPath to JSON file to read\n\n\ninclude_file_path\nbool\nWhether to return dict with ilepath as key\n\n\njsonlines\nbool\nWhether to read as JSON Lines format\n\n\n\n\n\n\n\n\n\n\n\nReturns\nType\nDescription\n\n\n\n\ndict or list[dict]\ndict or list[dict]\nParsed JSON data. For regular JSON, returns a dict. For JSON Lines, returns a list of dicts. If include_file_path=True, returns {filepath: data}.\n\n\n\nExample:\nfrom fsspec.implementations.local import LocalFileSystem\n\nfs = LocalFileSystem()\n# Read regular JSON\ndata = fs.read_json_file(\"config.json\")\nprint(data[\"setting\"])\n# 'value'\n\n# Read JSON Lines with filepath\ndata = fs.read_json_file(\n    \"logs.jsonl\",\n    include_file_path=True,\n    jsonlines=True\n)\nprint(list(data.keys())[0])\n# 'logs.jsonl'"
  },
  {
    "objectID": "api/fsspec_utils.core.ext.html#read_json",
    "href": "api/fsspec_utils.core.ext.html#read_json",
    "title": "fsspec_utils.core.ext API Documentation",
    "section": "",
    "text": "Read JSON data from one or more files with powerful options.\nProvides a flexible interface for reading JSON data with support for:\n\nSingle file or multiple f\nRegular JSON or JSON Lines format\nBatch processing for large datasets\nParallel processing\nDataFrame conversion\nFile path tracking\n\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\n\n\n\n\npath\nstr or list[str]\nPath(s) to JSON file(s). Can be: - Single path string (globs supported) - List of path strings\n\n\nbatch_size\nint | None\nIf set, enables batch reading with this many files per batch\n\n\ninclude_file_path\nbool\nInclude source filepath in output\n\n\njsonlines\nbool\nWhether to read as JSON Lines format\n\n\nas_dataframe\nbool\nConvert output to Polars DataFrame(s)\n\n\nconcat\nbool\nCombine multiple files/batches into single result\n\n\nuse_threads\nbool\nEnable parallel file reading\n\n\nverbose\nbool\nPrint progress information\n\n\nopt_dtypes\nbool\nOptimize DataFrame dtypes for performance\n\n\n**kwargs\nAny\nAdditional arguments passed to DataFrame conversion\n\n\n\n\n\n\n\n\n\n\n\nReturns\nType\nDescription\n\n\n\n\ndict or list[dict] or pl.DataFrame or list[pl.DataFrame] or Generator\nVarious types depending on arguments: - dict: Single JSON file as dictionary - list[dict]: Multiple JSON files as list of dictionaries - pl.DataFrame: Single or concatenated DataFrame - list[pl.DataFrame]: List of Dataframes (if concat=False) - Generator: If batch_size set, yields batches of above types\n\n\n\n\nExample:\nfrom fsspec.implementations.local import LocalFileSystem\n\nfs = LocalFileSystem()\n# Read all JSON files in directory\ndf = fs.read_json(\n    \"data/*.json\",\n    as_dataframe=True,\n    concat=True\n)\nprint(df.shape)\n# (1000, 5)  # Combined data from all files\n\n# Batch process large dataset\nfor batch_df in fs.read_json(\n    \"logs/*.jsonl\",\n    batch_size=100,\n    jsonlines=True,\n    include_file_path=True\n):\n    print(f\"Processing {len(batch_df)} records\")\n\n# Parallel read with custom options\ndfs = fs.read_json(\n    [\"file1.json\", \"file2.json\"],\n    use_threads=True,\n    concat=False,\n    verbose=True\n)\nprint(f\"Read {len(dfs)} files\")"
  },
  {
    "objectID": "api/fsspec_utils.core.ext.html#read_csv_file",
    "href": "api/fsspec_utils.core.ext.html#read_csv_file",
    "title": "fsspec_utils.core.ext API Documentation",
    "section": "",
    "text": "Read a single CSV file from any filesystem.\nInternal function that handles reading individual CSV files and optionally adds the source filepath as a column.\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\n\n\n\n\nself\nAbstractFileSystem\nFilesystem instance to use for reading\n\n\npath\nstr\nPath to CSV file\n\n\ninclude_file_path\nbool\nAdd source filepath as a column\n\n\nopt_dtypes\nbool\nOptimize DataFrame dtypes\n\n\n**kwargs\nAny\nAdditional arguments passed to pl.read_csv()\n\n\n\n\n\n\nReturns\nType\nDescription\n\n\n\n\npl.DataFrame\npl.DataFrame\nDataFrame containing CSV data\n\n\n\nExample:\nfrom fsspec.implementations.local import LocalFileSystem\n\nfs = LocalFileSystem()\n# This example assumes _read_csv_file is an internal method or needs to be called differently.\n# For public use, you would typically use fs.read_csv().\n# df = fs.read_csv_file(\n#     \"data.csv\",\n#     include_file_path=True,\n#     delimiter=\"|\"\n# )\n# print(\"file_path\" in df.columns)\n# True"
  },
  {
    "objectID": "api/fsspec_utils.core.ext.html#read_csv",
    "href": "api/fsspec_utils.core.ext.html#read_csv",
    "title": "fsspec_utils.core.ext API Documentation",
    "section": "",
    "text": "Read CSV data from one or more files with powerful options.\nProvides a flexible interface for reading CSV files with support for:\n\nSingle file or multiple files\nBatch processing for large datasets\nParallel processing\nFile path tracking\nPolars DataFrame output\n\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\n\n\n\n\npath\nstr or list[str]\nPath(s) to CSV file(s). Can be: - Single path string (globs supported) - List of path strings\n\n\nbatch_size\nint | None\nIf set, enables batch reading with this many files per batch\n\n\ninclude_file_path\nbool\nAdd source filepath as a column\n\n\nconcat\nbool\nCombine multiple files/batches into single DataFrame\n\n\nuse_threads\nbool\nEnable parallel file reading\n\n\nverbose\nbool\nPrint progress information\n\n\n**kwargs\nAny\nAdditional arguments passed to pl.read_csv()\n\n\n\n\n\n\n\n\n\n\n\nReturns\nType\nDescription\n\n\n\n\npl.DataFrame or list[pl.DataFrame] or Generator\nVarious types depending on arguments: - pl.DataFrame: Single or concatenated DataFrame - list[pl.DataFrame]: List of DataFrames (if concat=False) - Generator: If batch_size set, yields batches of above types\n\n\n\n\nExample:\nfrom fsspec.implementations.local import LocalFileSystem\n\nfs = LocalFileSystem()\n# Read all CSVs in directory\ndf = fs.read_csv(\n    \"data/*.csv\",\n    include_file_path=True\n)\nprint(df.columns)\n# ['file_path', 'col1', 'col2', ...]\n\n# Batch process large dataset\nfor batch_df in fs.read_csv(\n    \"logs/*.csv\",\n    batch_size=100,\n    use_threads=True,\n    verbose=True\n):\n    print(f\"Processing {len(batch_df)} rows\")\n\n# Multiple files without concatenation\ndfs = fs.read_csv(\n    [\"file1.csv\", \"file2.csv\"],\n    concat=False,\n    use_threads=True\n)\nprint(f\"Read {len(dfs)} files\")"
  },
  {
    "objectID": "api/fsspec_utils.core.ext.html#read_parquet_file",
    "href": "api/fsspec_utils.core.ext.html#read_parquet_file",
    "title": "fsspec_utils.core.ext API Documentation",
    "section": "",
    "text": "Read a single Parquet file from any filesystem.\nInternal function that handles reading individual Parquet files and optionally adds the source filepath as a column.\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\n\n\n\n\nself\nAbstractFileSystem\nFilesystem instance to use for reading\n\n\npath\nstr\nPath to Parquet file\n\n\ninclude_file_path\nbool\nAdd source filepath as a column\n\n\nopt_dtypes\nbool\nOptimize DataFrame dtypes\n\n\n**kwargs\nAny\nAdditional arguments passed to pq.read_table()\n\n\n\n\n\n\nReturns\nType\nDescription\n\n\n\n\npa.Table\npa.Table\nPyArrow Table containing Parquet data\n\n\n\nExample:\nfrom fsspec.implementations.local import LocalFileSystem\n\nfs = LocalFileSystem()\n# This example assumes _read_parquet_file is an internal method or needs to be called differently.\n# For public use, you would typically use fs.read_parquet().\n# table = fs.read_parquet_file(\n#     \"data.parquet\",\n#     include_file_path=True,\n#     use_threads=True\n# )\n# print(\"file_path\" in table.column_names)\n# True"
  },
  {
    "objectID": "api/fsspec_utils.core.ext.html#read_parquet",
    "href": "api/fsspec_utils.core.ext.html#read_parquet",
    "title": "fsspec_utils.core.ext API Documentation",
    "section": "",
    "text": "Read Parquet data with advanced features and optimizations.\nProvides a high-performance interface for reading Parquet files with support for:\n\nSingle file or multiple files\nBatch processing for large datasets\nParallel processing\nFile path tracking\nAutomatic concatenation\nPyArrow Table output\n\nThe function automatically uses optimal reading strategies:\n\nDirect dataset reading for simple cases\nParallel processing for multiple files\nBatched reading for memory efficiency\n\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\n\n\n\n\npath\nstr or list[str]\nPth(s) to Parquet file(s). Can be: - Single path string (globs supported) - List of path strings - Directory containing _metadata file\n\n\nbatch_size\nint | None\nIf set, enables batch reading with this many files per batch\n\n\ninclude_file_path\nbool\nAdd source filepath as a column\n\n\nconcat\nbool\nCombine multiple files/batches into single Table\n\n\nuse_threads\nbool\nEnable parallel file reading\n\n\nverbose\nbool\nPrint progress information\n\n\nopt_dtypes\nbool\nOptimize Table dtypes for performance\n\n\n**kwargs\nAny\nAdditional arguments passed to pq.read_table()\n\n\n\n\n\n\n\n\n\n\n\nReturns\nType\nDescription\n\n\n\n\npa.Table or list[pa.Table] or Generator\nVarious types depending on arguments: - pa.Table: Single or concatenated Table - list[pa.Table]: List of Tables (if concat=False) - Generator: If batch_size set, yields batches of above types\n\n\n\n\nExample:\nfrom fsspec.implementations.local import LocalFileSystem\n\nfs = LocalFileSystem()\n# Read all Parquet files in directory\ntable = fs.read_parquet(\n    \"data/*.parquet\",\n    include_file_path=True\n)\nprint(table.column_names)\n# ['file_path', 'col1', 'col2', ...]\n\n# Batch process large dataset\nfor batch in fs.read_parquet(\n    \"data/*.parquet\",\n    batch_size=100,\n    use_threads=True\n):\n    print(f\"Processing {batch.num_rows} rows\")\n\n# Read from directory with metadata\ntable = fs.read_parquet(\n    \"data/\",  # Contains _metadata\n    use_threads=True\n)\nprint(f\"Total rows: {table.num_rows}\")"
  },
  {
    "objectID": "api/fsspec_utils.core.ext.html#read_files",
    "href": "api/fsspec_utils.core.ext.html#read_files",
    "title": "fsspec_utils.core.ext API Documentation",
    "section": "",
    "text": "Universal interface for reading data files of any supported format.\nA unified API that automatically delegates to the appropriate reading function based on file format, while preserving all advanced features like:\n\nBatch processing\nParallel reading\nFile path tracking\nFormat-specific optimizations\n\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\n\n\n\n\npath\nstr or list[str]\nPath(s) to data file(s). Can be: - Single path string (globs supported) - List of path strings\n\n\nformat\nstr| File format to read. Supported values: - \"json\": Regular JSON or JSON Lines - \"csv\": CSV files - \"parquet\": Parquet files | |batch_size|int\nNone| If set, enables batch reading with this many files per batch | |include_file_path|bool| Add source filepath as column/field | |concat|bool| Combine multiple files/batches into single result | |jsonlines|bool| For JSON format, whether to read as JSON Lines | |use_threads|bool| Enable parallel file reading | |verbose|bool| Print progress information | |opt_dtypes|bool| Optimize DataFrame/Arrow Table dtypes for performance | |**kwargs|Any`\n\n\n\n\n\n\n\n\n\n\n\nReturns\nType\nDescription\n\n\n\n\npl.DataFrame or pa.Table or list[pl.DataFrame] or list[pa.Table] or Generator\nVarious types depending on format and arguments: - pl.DataFrame: For CSV and optionally JSON - pa.Table: For Parquet - list[pl.DataFrame or pa.Table]: Without concatenation - Generator: If batch_size set, yields batches\n\n\n\n\nExample:\nfrom fsspec.implementations.local import LocalFileSystem\n\nfs = LocalFileSystem()\n# Read CSV files\ndf = fs.read_files(\n    \"data/*.csv\",\n    format=\"csv\",\n    include_file_path=True\n)\nprint(type(df))\n# &lt;class 'polars.DataFrame'&gt;\n\n# Batch process Parquet files\nfor batch in fs.read_files(\n    \"data/*.parquet\",\n    format=\"parquet\",\n    batch_size=100,\n    use_threads=True\n):\n    print(f\"Batch type: {type(batch)}\")\n\n# Read JSON Lines\ndf = fs.read_files(\n    \"logs/*.jsonl\",\n    format=\"json\",\n    jsonlines=True,\n    concat=True\n)\nprint(df.columns)"
  },
  {
    "objectID": "api/fsspec_utils.core.ext.html#pyarrow_dataset",
    "href": "api/fsspec_utils.core.ext.html#pyarrow_dataset",
    "title": "fsspec_utils.core.ext API Documentation",
    "section": "",
    "text": "Create a PyArrow dataset from files in any supported format.\nCreates a dataset that provides optimized reading and querying capabilities including:\n\nSchema inference and enforcement\nPartition discovery and pruning\nPredicate pushdown\nColumn projection\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\n\n\n\n\npath\nstr\nBase path to dataset files\n\n\nformat\nstr\nFile format. Currently supports: - “parquet” (default) - “csv” - “json” (experimental)\n\n\nschema\npa.Schema | None\nOptional schema to enforce. If None, inferred from data.\n\n\n\n\npartitioning | str or list[str] or pds.Partitioning | How the dataset is partitoned. Can be: - str: Single partition field - list[str]: Multiple partition fields - pds.Partitioning: Custom partitioning scheme |\n**kwargs | Any | Additional arguments for dataset creation |\nReturns | Type | Description |\n:—— | :— | :———- |\npds.Dataset | pds.Dataset | PyArrow dataset instance |\nExample:\nfrom fsspec.implementations.local import LocalFileSystem\n\nfs = LocalFileSystem()\n# Simple Parquet dataset\nds = fs.pyarrow_dataset(\"data/\")\nprint(ds.schema)\n\n# Partitioned dataset\nds = fs.pyarrow_dataset(\n    \"events/\",\n    partitioning=[\"year\", \"month\"]\n)\n# Query with partition pruning\ntable = ds.to_table(\n    filter=(ds.field(\"year\") == 2024)\n)\n\n# CSV with schema\nds = fs.pyarrow_dataset(\n    \"logs/\",\n    format=\"csv\",\n    schema=pa.schema([\n        (\"timestamp\", pa.timestamp(\"s\")),\n        (\"level\", pa.string()),\n        (\"message\", pa.string())\n    ])\n)"
  },
  {
    "objectID": "api/fsspec_utils.core.ext.html#pyarrow_parquet_dataset",
    "href": "api/fsspec_utils.core.ext.html#pyarrow_parquet_dataset",
    "title": "fsspec_utils.core.ext API Documentation",
    "section": "",
    "text": "Create a PyArrow dataset optimized for Parquet files.\nCreates a dataset specifically for Parquet data, automatically handling _metadata files for optimized reading.\nThis function is particularly useful for:\n\nDatasets with existing _metadata files\nMulti-file datasets that should be treated as one\nPartitioned Parquet datasets\n\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\n\n\n\n\npath\nstr\nPath to dataset directory or _metadata file\n\n\nschema\npa.Schema | None\nOptional schema to enforce. If None, inferred from data.\n\n\npartitioning\nstr or list[str] or pds.Partitioning\nHow the dataset is partitoned. Can be: - str: Single partition field - list[str]: Multiple partition fields - pds.Partitioning: Custom partitioning scheme\n\n\n**kwargs\nAny\nAdditional dataset arguments\n\n\n\n\n\n\nReturns\nType\nDescription\n\n\n\n\npds.Dataset\npds.Dataset\nPyArrow dataset instance\n\n\n\nExample:\nfrom fsspec.implementations.local import LocalFileSystem\n\nfs = LocalFileSystem()\n# Dataset with _metadata\nds = fs.pyarrow_parquet_dataset(\"data/_metadata\")\nprint(ds.files)  # Shows all data files\n\n# Partitioned dataset directory\nds = fs.pyarrow_parquet_dataset(\n    \"sales/\",\n    partitioning=[\"year\", \"region\"]\n)\n# Query with partition pruning\ntable = ds.to_table(\n    filter=(\n        (ds.field(\"year\") == 2024) &\n        (ds.field(\"region\") == \"EMEA\")\n    )\n)"
  },
  {
    "objectID": "api/fsspec_utils.core.ext.html#pydala_dataset",
    "href": "api/fsspec_utils.core.ext.html#pydala_dataset",
    "title": "fsspec_utils.core.ext API Documentation",
    "section": "",
    "text": "Create a Pydala dataset for advanced Parquet operations.\nCreates a dataset with additional features beyond PyArrow including:\n\nDelta table support\nSchema evolution\nAdvanced partitioning\nMetadata management\nSort key optimization\n\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\n\n\n\n\npath\nstr\nPath to dataset directory\n\n\npartitioning\nstr or list[str] or pds.Partitioning\nHow the dataset is partitoned. Can be: - str: Single partition field - list[str]: Multiple partition fields - pds.Partitioning: Custom partitioning scheme\n\n\n**kwargs\nAny\nAdditional dataset configuration\n\n\n\n\n\n\nReturns\nType\nDescription\n\n\n\n\nParquetDataset\nParquetDataset\nPydala dataset instance\n\n\n\nExample:\nfrom fsspec.implementations.local import LocalFileSystem\n\nfs = LocalFileSystem()\n# Create dataset\nds = fs.pydala_dataset(\n    \"data/\",\n    partitioning=[\"date\"]\n)\n\n# Write with delta support\nds.write_to_dataset(\n    new_data,\n    mode=\"delta\",\n    delta_subset=[\"id\"]\n)\n\n# Read with metadata\ndf = ds.to_polars()\nprint(df.columns)"
  },
  {
    "objectID": "api/fsspec_utils.core.ext.html#write_parquet",
    "href": "api/fsspec_utils.core.ext.html#write_parquet",
    "title": "fsspec_utils.core.ext API Documentation",
    "section": "",
    "text": "Write data to a Parquet file with automatic format conversion.\nHandles writing data from multiple input formats to Parquet with:\n\nAutomatic conversion to PyArrow\nSchema validation/coercion\nMetadata collection\nCompression and encoding options\n\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\n\n\n\n\ndata\npl.DataFrame or pl.LazyFrame or pa.Table or pd.DataFrame or dict o list[dict]\nInput data in various formats: - Polars DataFrame/LazyFrame - PyArrow Table - Pandas DataFrame - Dict or list of dicts\n\n\npath\nstr\nOutput Parquet file path\n\n\nschema\npa.Schema | None\nOptional schema to enforce on write\n\n\n**kwargs\nAny\nAdditional arguments for pq.write_table()\n\n\n\n\n\n\n\n\n\n\n\nReturns\nType\nDescription\n\n\n\n\npq.FileMetaData\npq.FileMetaData\nMetadata of written Parquet file\n\n\n\n\n\n\nRaises\nType\nDescription\n\n\n\n\nSchemaError\nSchemaError\nIf data doesn’t match schema\n\n\nValueError\nValueError\nIf data cannot be converted\n\n\n\nExample:\nfrom fsspec.implementations.local import LocalFileSystem\nimport polars as pl\nimport numpy as np\nimport pyarrow as pa\n\nfs = LocalFileSystem()\n# Write Polars DataFrame\ndf = pl.DataFrame({\n    \"id\": range(1000),\n    \"value\": pl.Series(np.random.randn(1000))\n})\nmetadata = fs.write_parquet(\n    df,\n    \"data.parquet\",\n    compression=\"zstd\",\n    compression_level=3\n)\nprint(f\"Rows: {metadata.num_rows}\")\n\n# Write with schema\nschema = pa.schema([\n    (\"id\", pa.int64()),\n    (\"value\", pa.float64())\n])\nmetadata = fs.write_parquet(\n    {\"id\": [1, 2], \"value\": [0.1, 0.2]},\n    \"data.parquet\",\n    schema=schema\n)"
  },
  {
    "objectID": "api/fsspec_utils.core.ext.html#write_json",
    "href": "api/fsspec_utils.core.ext.html#write_json",
    "title": "fsspec_utils.core.ext API Documentation",
    "section": "",
    "text": "Write data to a JSON file with flexible input support.\nHandles writing data in various formats to JSON or JSON Lines, with optional appending for streaming writes.\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\n\n\n\n\ndata\ndict or pl.DataFrame or pl.LazyFrame or pa.Table or pd.DataFrame or dict or list[dict]\nInput data in various formats: - Dict or list of dicts - Polars DataFrame/LazyFrame - PyArrow Table - Pandas DataFrame\n\n\npath\nstr\nOutput JSON file path\n\n\nappend\nbool\nWhether to append to existing file (JSON Lines mode)\n\n\n\nExample:\nfrom fsspec.implementations.local import LocalFileSystem\nimport polars as pl\nimport pyarrow as pa\n\nfs = LocalFileSystem()\n# Write dictionary\ndata = {\"name\": \"test\", \"values\": [1, 2, 3]}\nfs.write_json(data, \"config.json\")\n\n# Stream records\ndf1 = pl.DataFrame({\"id\": [1], \"value\": [\"first\"]})\ndf2 = pl.DataFrame({\"id\": [2], \"value\": [\"second\"]})\nfs.write_json(df1, \"stream.jsonl\", append=False)\nfs.write_json(df2, \"stream.jsonl\", append=True)\n\n# Convert PyArrow\ntable = pa.table({\"a\": [1, 2], \"b\": [\"x\", \"y\"]})\nfs.write_json(table, \"data.json\")"
  },
  {
    "objectID": "api/fsspec_utils.core.ext.html#write_csv",
    "href": "api/fsspec_utils.core.ext.html#write_csv",
    "title": "fsspec_utils.core.ext API Documentation",
    "section": "",
    "text": "Write data to a CSV file with flexible input support.\nHandles writing data from multiple formats to CSV with options for:\n\nAppending to existing files\nCustom delimiters and formatting\nAutomatic type conversion\nHeader handling\n\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\n\n\n\n\ndata\npl.DataFrame or pl.LazyFrame or pa.Table or pd.DataFrame or dict or list[dict]\nInput data in various formats: - Polars DataFrame/LazyFrame - PyArrow Table - Pandas DataFrame - Dict or list of dicts\n\n\npath\nstr\nOutput CSV file path\n\n\nappend\nbool\nWhether to append to existing file\n\n\n**kwargs\nAny\nAdditional arguments for CSV writing: - delimiter: Field separator (default “,”) - header: Whether to write header row - quote_char: Character for quoting fields - date_format: Format for date/time fields - float_precision: Decimal places for floats\n\n\n\nExample:\nfrom fsspec.implementations.local import LocalFileSystem\nimport polars as pl\nfrom datetime import datetime\nimport pyarrow as pa\n\nfs = LocalFileSystem()\n# Write Polars DataFrame\ndf = pl.DataFrame({\n    \"id\": range(100),\n    \"name\": [\"item_\" + str(i) for i in range(100)]\n})\nfs.write_csv(df, \"items.csv\")\n\n# Append records\nnew_items = pl.DataFrame({\n    \"id\": range(100, 200),\n    \"name\": [\"item_\" + str(i) for i in range(100, 200)]\n})\nfs.write_csv(\n    new_items,\n    \"items.csv\",\n    append=True,\n    header=False\n)\n\n# Custom formatting\ndata = pa.table({\n    \"date\": [datetime.now()],\n    \"value\": [123.456]\n})\nfs.write_csv(\n    data,\n    \"formatted.csv\",\n    date_format=\"%Y-%m-%d\",\n    float_precision=2\n)"
  },
  {
    "objectID": "api/fsspec_utils.core.ext.html#write_file",
    "href": "api/fsspec_utils.core.ext.html#write_file",
    "title": "fsspec_utils.core.ext API Documentation",
    "section": "",
    "text": "Write a DataFrame to a file in the given format.\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\n\n\n\n\ndata\npl.DataFrame or pl.LazyFrame or pa.Table or pd.DataFrame or dict\nData to write.\n\n\npath\nstr\nPath to write the data.\n\n\nformat\nstr\nFormat of the file.\n\n\n**kwargs\nAny\nAdditional keyword arguments.\n\n\n\n\n\n\nReturns\nType\nDescription\n\n\n\n\nNone\nNone"
  },
  {
    "objectID": "api/fsspec_utils.core.ext.html#write_files",
    "href": "api/fsspec_utils.core.ext.html#write_files",
    "title": "fsspec_utils.core.ext API Documentation",
    "section": "",
    "text": "Write a DataFrame or a list of DataFrames to a file or a list of files.\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\n\n\n\n\ndata\npl.DataFrame or pl.LazyFrame or pa.Table or pa.RecordBatch or pa.RecordBatchReader or pd.DataFrame or dict or list[pl.DataFrame or pl.LazyFrame or pa.Table or pa.RecordBatch or pa.RecordBatchReader or pd.DataFrame or dict]\nData to write.\n\n\npath\nstr or list[str]\nPath to write the data.\n\n\nbasename\nstr\nBasename of the files. Defaults to None.\n\n\nformat\nstr\nFormat of the data. Defaults to None.\n\n\nconcat\nbool\nIf True, concatenate the DataFrames. Defaults to True.\n\n\nunique\nbool or list[str] or str\nIf True, remove duplicates. Defaults to False.\n\n\nmode\nstr\nWrite mode. Defaults to ‘append’. Options: ‘append’, ‘overwrite’, ‘delete_matching’, ‘error_if_exists’.\n\n\nuse_threads\nbool\nIf True, use parallel processing. Defaults to True.\n\n\nverbose\nbool\nIf True, print verbose output Defaults to True.\n\n\n**kwargs\nAny\nAdditional keyword arguments.\n\n\n\n\n\n\nReturns\nType\nDescription\n\n\n\n\nNone\nNone\n\n\n\n\n\n\n\n\n\n\n\n\nRaises\nType\nDescription\n\n\n\n\nFileExistsError\nFileExistsError\nIf file already exists and mode is ‘error_if_exists’."
  },
  {
    "objectID": "api/fsspec_utils.core.ext.html#write_pyarrow_dataset",
    "href": "api/fsspec_utils.core.ext.html#write_pyarrow_dataset",
    "title": "fsspec_utils.core.ext API Documentation",
    "section": "",
    "text": "Write a tabular data to a PyArrow dataset.\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\n\n\n\n\ndata\npl.DataFrame or pa.Table or pa.RecordBatch or pa.RecordBatchReader or pd.DataFrame or list[pl.DataFrame] or list[pa.Table] or list[pa.RecordBatch] or list[pa.RecordBatchReader] or list[pd.DataFrame]\nData to write.\n\n\npath\nstr\nPath to write the data.\n\n\nbasename\nstr | None\nBasename of the files. Defaults to None.\n\n\nschema\npa.Schema | None\nSchema of the data. Defaults to None.\n\n\npartition_by\nstr or list[str] or pds.Partitioning or None\nPartitioning of the data. Defaults to None.\n\n\npartitioning_flavor\nstr\nPartitioning flavor. Defaults to ‘hive’.\n\n\nmode\nstr\nWrite mode. Defaults to ‘append’.\n\n\nformat\nstr | None\nFormat of the data. Defaults to ‘parquet’.\n\n\ncompression\nstr\nCompression algorithm. Defaults to ‘zstd’.\n\n\nmax_rows_per_file\nint | None\nMaximum number of rows per file. Defaults to 2,500,000.\n\n\nrow_group_size\nint | None\nRow group size. Defaults to 250,000.\n\n\nconcat\nbool\nIf True, concatenate the DataFrames. Defaults to True.\n\n\nunique\nbool or str or list[str]\nIf True, remove duplicates. Deaults to False.\n\n\n**kwargs\nAny\nAdditional keyword arguments for pds.write_dataset.\n\n\n\n\n\n\n\n\n\n\n\nReturns\nType\nDescription\n\n\n\n\nlist[pq.FileMetaData] or None\nList of Parquet file metadata or None."
  },
  {
    "objectID": "api/fsspec_utils.core.ext.html#write_pydala_dataset",
    "href": "api/fsspec_utils.core.ext.html#write_pydala_dataset",
    "title": "fsspec_utils.core.ext API Documentation",
    "section": "",
    "text": "Write a tabular data to a Pydala dataset.\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\n\n\n\n\ndata\npl.DataFrame or pa.Table or pa.RecordBatch or pa.RecordBatchReader or pd.DataFrame or list[pl.DataFrame] or list[pa.Table] or list[pa.RecordBatch] or list[pa.RecordBatchReader] or list[pd.DataFrame]\nData to write.\n\n\npath\nstr\nPath to write the data.\n\n\nmode\nstr\nWrite mode. Defaults to ‘append’. Options: ‘delta’, ‘overwrite’.\n\n\nbasename\nstr | None\nBasename of the files. Defaults to None.\n\n\npartition_by\nstr or list[str] or None\nPartitioning of the data. Defaults to None.\n\n\npartitioning_flavor\nstr\nPartitioning flavor. Defaults to ‘hive’.\n\n\nmax_rows_per_file\nint | None\nMaximum number of rows per file. Defaults to 2,500,000.\n\n\nrow_group_size\nint | None\nRow group size. Defaults to 250,000.\n\n\ncompression\nstr\nCompression algorithm. Defaults to ‘zstd’.\n\n\nsort_by\nstr or list[str] or list[tuple[str, str]] or None\nColumns to sort by. Defaults to None.\n\n\nunique\nbool or str or list[str]\nIf True, ensure unique values. Defaults to False.\n\n\ndelta_subset\nstr or list[str] or None\nSubset of columns to include in delta table. Defaults to None.\n\n\nupdate_metadata\nbool\nIf True, update metadata. Defaults to True.\n\n\nalter_schema\nbool\nIf True, alter schema. Defaults to False.\n\n\ntimestamp_column\nstr or None\nTimestamp column. Defaults to None.\n\n\nverbose\nbool\nIf True, print verbose output. Defaults to True.\n\n\n**kwargs\nAny\nAdditional keyword arguments for ParquetDataset.write_to_dataset.\n\n\n\n\n\n\nReturns\nType\nDescription\n\n\n\n\nNone\nNone"
  },
  {
    "objectID": "api/fsspec_utils.utils.polars.html",
    "href": "api/fsspec_utils.utils.polars.html",
    "title": "fsspec_utils.utils.polars API Reference",
    "section": "",
    "text": "Optimize data types of a Polars DataFrame for performance and memory efficiency.\nThis function analyzes each column and converts it to the most appropriate data type based on content, handling string-to-type conversions and numeric type downcasting.\nParameters:\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\ndf\npolars.DataFrame\nThe input Polars DataFrame to optimize.\n\n\ninclude\nlist[str] or None\nOptional list of column names to include in the optimization process. If None, all columns are considered.\n\n\nexclude\nlist[str] or None\nOptional list of column names to exclude from the optimization process.\n\n\ntime_zone\nstr or None\nOptional time zone string for datetime parsing.\n\n\nshrink_numerics\nbool\nIf True, numeric columns will be downcasted to smaller data types if possible without losing precision.\n\n\nallow_unsigned\nbool\nIf True, unsigned integer types will be considered for numeric column optimization.\n\n\nallow_null\nbool\nIf True, columns containing only null values will be cast to the Null type.\n\n\nstrict\nbool\nIf True, an error will be raised if any column cannot be optimized (e.g., due to type inference issues).\n\n\n\nExample:\nimport polars as pl\nfrom fsspec_utils.utils.polars import opt_dtype\n\ndf = pl.DataFrame({\n    \"col_int\": [\"1\", \"2\", \"3\"],\n    \"col_float\": [\"1.1\", \"2.2\", \"3.3\"],\n    \"col_bool\": [\"True\", \"False\", \"True\"],\n    \"col_date\": [\"2023-01-01\", \"2023-01-02\", \"2023-01-03\"],\n    \"col_str\": [\"a\", \"b\", \"c\"],\n    \"col_null\": [None, None, None]\n})\noptimized_df = opt_dtype(df, shrink_numerics=True)\nprint(optimized_df.schema)\n# Expected output similar to:\n# Schema({\n#     'col_int': Int8,\n#     'col_float': Float32,\n#     'col_bool': Boolean,\n#     'col_date': Date,\n#     'col_str': Utf8,\n#     'col_null': Null\n# })\nReturns:\n\npolars.DataFrame: DataFrame with optimized data types\n\n\n\n\nParameters:\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\ndf\npolars.DataFrame\nThe input Polars DataFrame.\n\n\nseperator\nstr\nThe separator used to flatten nested column names. Defaults to ’_’.\n\n\nfields\nlist[str] or None\nOptional list of specific fields (structs) to unnest. If None, all struct columns will be unnested.\n\n\n\nExample:\nimport polars as pl\nfrom fsspec_utils.utils.polars import explode_all\n\ndf = pl.DataFrame({\n    \"id\": [1, 2],\n    \"values\": [[10, 20], [30]]\n})\nexploded_df = explode_all(df)\nprint(exploded_df)\n# shape: (3, 2)\n# ┌─────┬────────┐\n# │ id  ┆ values │\n# │ --- ┆ ---    │\n# │ i64 ┆ i64    │\n# ╞═════╪════════╡\n# │ 1   ┆ 10     │\n# │ 1   ┆ 20     │\n# │ 2   ┆ 30     │\n# └─────┴────────┘\nimport polars as pl\nfrom fsspec_utils.utils.polars import unnest_all\n\ndf = pl.DataFrame({\n    \"id\": [1, 2],\n    \"data\": [\n        {\"a\": 1, \"b\": {\"c\": 3}},\n        {\"a\": 4, \"b\": {\"c\": 6}}\n    ]\n})\nunnested_df = unnest_all(df, seperator='__')\nprint(unnested_df)\n# shape: (2, 3)\n# ┌─────┬──────┬───────┐\n# │ id  ┆ data__a ┆ data__b__c │\n# │ --- ┆ ---  ┆ ---     │\n# │ i64 ┆ i64  ┆ i64     │\n# ╞═════╪══════╪═════════╡\n# │ 1   ┆ 1    ┆ 3       │\n# │ 2   ┆ 4    ┆ 6       │\n# └─────┴──────┴─────────┘\nReturns:\n\nNone\n\n\n\n\nParameters:\n\n\n\nName\nType\nDescription\n\n\n\n\ndf\npolars.DataFrame\nThe input Polars DataFrame.\n\n\n\nExample:\nimport polars as pl\nfrom fsspec_utils.utils.polars import drop_null_columns\n\ndf = pl.DataFrame({\n    \"col1\": [1, 2, 3],\n    \"col2\": [None, None, None],\n    \"col3\": [\"a\", None, \"c\"]\n})\ndf_cleaned = drop_null_columns(df)\nprint(df_cleaned)\n# shape: (3, 2)\n# ┌──────┬───────┐\n# │ col1 ┆ col3  │\n# │ ---  ┆ ---   │\n# │ i64  ┆ str   │\n# ╞══════╪═══════╡\n# │ 1    ┆ a     │\n# │ 2    ┆ null  │\n# │ 3    ┆ c     │\n# └──────┴───────┘\nReturns:\n\nNone\n\n\n\n\nParameters:\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\ndf\npolars.DataFrame\nThe input Polars DataFrame.\n\n\nstrftime\nstr\nThe strftime format string (e.g., “%Y-%m-%d” for date, “%H” for hour).\n\n\ntimestamp_column\nstr\nThe name of the timestamp column to use. Defaults to ‘auto’ (attempts to infer).\n\n\ncolumn_names\nlist[str] or None\nOptional list of new column names to use for the generated columns. If None, names are derived from the strftime format.\n\n\n\nReturns:\n\nNone\n\n\n\n\nParameters:\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\ndf\npolars.DataFrame\nThe input Polars DataFrame.\n\n\ntruncate_by\nstr\nThe duration string to truncate by (e.g., “1h”, “1d”, “1mo”).\n\n\ntimestamp_column\nstr\nThe name of the timestamp column to truncate. Defaults to ‘auto’ (attempts to infer).\n\n\ncolumn_names\nlist[str] or None\nOptional list of new column names for the truncated columns. If None, names are derived automatically.\n\n\n\nReturns:\n\nNone\n\n\n\n\nParameters:\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\ndf\npolars.DataFrame\nThe input Polars DataFrame.\n\n\ntimestamp_column\nstr\nThe name of the timestamp column to extract date parts from. Defaults to ‘auto’ (attempts to infer).\n\n\nyear\nbool\nIf True, extract the year as a new column.\n\n\nmonth\nbool\nIf True, extract the month as a new column.\n\n\nweek\nbool\nIf True, extract the week of the year as a new column.\n\n\nyearday\nbool\nIf True, extract the day of the year as a new column.\n\n\nmonthday\nbool\nIf True, extract the day of the month as a new column.\n\n\nday\nbool\nIf True, extract the day of the week (1-7, Monday=1) as a new column.\n\n\nweekday\nbool\nIf True, extract the weekday (0-6, Monday=0) as a new column.\n\n\nhour\nbool\nIf True, extract the hour as a new column.\n\n\nminute\nbool\nIf True, extract the minute as a new column.\n\n\nstrftime\nstr or None\nOptional strftime format string to apply to the timestamp column before extracting parts.\n\n\n\nReturns:\n\nNone\n\n\n\n\nParameters:\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\ndf\npolars.DataFrame\nThe input Polars DataFrame.\n\n\nover\nlist[str] or None\nOptional list of column names to partition the data by before adding row counts. If None, a global row count is added.\n\n\n\nReturns:\n\nNone\n\n\n\n\nRemove columns with all null values from the DataFrame.\nParameters:\n\n\n\nName\nType\nDescription\n\n\n\n\ndf\npolars.DataFrame\nThe input Polars DataFrame.\n\n\n\nReturns:\n\nNone\n\n\n\n\nParameters:\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\ndfs\nlist[polars.DataFrame]\nA list of Polars DataFrames to unify their schemas.\n\n\n\nReturns:\n\nNone\n\n\n\n\nParameters:\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\ndf\npolars.DataFrame\nThe input Polars DataFrame to cast.\n\n\nschema\ndict or polars.Schema\nThe target schema to cast the DataFrame to. Can be a dictionary mapping column names to data types or a Polars Schema object.\n\n\n\nReturns:\n\nNone\n\n\n\n\nParameters:\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\ndf1\npolars.DataFrame\nThe first Polars DataFrame.\n\n\ndf2\npolars.DataFrame\nThe second Polars DataFrame.\n\n\nsubset\nlist[str] or None\nOptional list of column names to consider when calculating the delta. If None, all columns are used.\n\n\neager\nbool\nIf True, the delta calculation is performed eagerly. Defaults to False (lazy).\n\n\n\nReturns:\n\nNone\n\n\n\n\nParameters:\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\ndf\npolars.DataFrame\nThe input Polars DataFrame to partition.\n\n\ntimestamp_column\nstr or None\nThe name of the timestamp column to use for time-based partitioning. Defaults to None.\n\n\ncolumns\nlist[str] or None\nOptional list of column names to partition by. Defaults to None.\n\n\nstrftime\nstr or None\nOptional strftime format string for time-based partitioning. Defaults to None.\n\n\ntimedelta\nstr or None\nOptional timedelta string (e.g., “1h”, “1d”) for time-based partitioning. Defaults to None.\n\n\nnum_rows\nint or None\nOptional number of rows per partition for row-based partitioning. Defaults to None.\n\n\n\nReturns:\n\nNone"
  },
  {
    "objectID": "api/fsspec_utils.utils.polars.html#opt_dtype",
    "href": "api/fsspec_utils.utils.polars.html#opt_dtype",
    "title": "fsspec_utils.utils.polars API Reference",
    "section": "",
    "text": "Optimize data types of a Polars DataFrame for performance and memory efficiency.\nThis function analyzes each column and converts it to the most appropriate data type based on content, handling string-to-type conversions and numeric type downcasting.\nParameters:\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\ndf\npolars.DataFrame\nThe input Polars DataFrame to optimize.\n\n\ninclude\nlist[str] or None\nOptional list of column names to include in the optimization process. If None, all columns are considered.\n\n\nexclude\nlist[str] or None\nOptional list of column names to exclude from the optimization process.\n\n\ntime_zone\nstr or None\nOptional time zone string for datetime parsing.\n\n\nshrink_numerics\nbool\nIf True, numeric columns will be downcasted to smaller data types if possible without losing precision.\n\n\nallow_unsigned\nbool\nIf True, unsigned integer types will be considered for numeric column optimization.\n\n\nallow_null\nbool\nIf True, columns containing only null values will be cast to the Null type.\n\n\nstrict\nbool\nIf True, an error will be raised if any column cannot be optimized (e.g., due to type inference issues).\n\n\n\nExample:\nimport polars as pl\nfrom fsspec_utils.utils.polars import opt_dtype\n\ndf = pl.DataFrame({\n    \"col_int\": [\"1\", \"2\", \"3\"],\n    \"col_float\": [\"1.1\", \"2.2\", \"3.3\"],\n    \"col_bool\": [\"True\", \"False\", \"True\"],\n    \"col_date\": [\"2023-01-01\", \"2023-01-02\", \"2023-01-03\"],\n    \"col_str\": [\"a\", \"b\", \"c\"],\n    \"col_null\": [None, None, None]\n})\noptimized_df = opt_dtype(df, shrink_numerics=True)\nprint(optimized_df.schema)\n# Expected output similar to:\n# Schema({\n#     'col_int': Int8,\n#     'col_float': Float32,\n#     'col_bool': Boolean,\n#     'col_date': Date,\n#     'col_str': Utf8,\n#     'col_null': Null\n# })\nReturns:\n\npolars.DataFrame: DataFrame with optimized data types"
  },
  {
    "objectID": "api/fsspec_utils.utils.polars.html#unnest_all",
    "href": "api/fsspec_utils.utils.polars.html#unnest_all",
    "title": "fsspec_utils.utils.polars API Reference",
    "section": "",
    "text": "Parameters:\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\ndf\npolars.DataFrame\nThe input Polars DataFrame.\n\n\nseperator\nstr\nThe separator used to flatten nested column names. Defaults to ’_’.\n\n\nfields\nlist[str] or None\nOptional list of specific fields (structs) to unnest. If None, all struct columns will be unnested.\n\n\n\nExample:\nimport polars as pl\nfrom fsspec_utils.utils.polars import explode_all\n\ndf = pl.DataFrame({\n    \"id\": [1, 2],\n    \"values\": [[10, 20], [30]]\n})\nexploded_df = explode_all(df)\nprint(exploded_df)\n# shape: (3, 2)\n# ┌─────┬────────┐\n# │ id  ┆ values │\n# │ --- ┆ ---    │\n# │ i64 ┆ i64    │\n# ╞═════╪════════╡\n# │ 1   ┆ 10     │\n# │ 1   ┆ 20     │\n# │ 2   ┆ 30     │\n# └─────┴────────┘\nimport polars as pl\nfrom fsspec_utils.utils.polars import unnest_all\n\ndf = pl.DataFrame({\n    \"id\": [1, 2],\n    \"data\": [\n        {\"a\": 1, \"b\": {\"c\": 3}},\n        {\"a\": 4, \"b\": {\"c\": 6}}\n    ]\n})\nunnested_df = unnest_all(df, seperator='__')\nprint(unnested_df)\n# shape: (2, 3)\n# ┌─────┬──────┬───────┐\n# │ id  ┆ data__a ┆ data__b__c │\n# │ --- ┆ ---  ┆ ---     │\n# │ i64 ┆ i64  ┆ i64     │\n# ╞═════╪══════╪═════════╡\n# │ 1   ┆ 1    ┆ 3       │\n# │ 2   ┆ 4    ┆ 6       │\n# └─────┴──────┴─────────┘\nReturns:\n\nNone"
  },
  {
    "objectID": "api/fsspec_utils.utils.polars.html#explode_all",
    "href": "api/fsspec_utils.utils.polars.html#explode_all",
    "title": "fsspec_utils.utils.polars API Reference",
    "section": "",
    "text": "Parameters:\n\n\n\nName\nType\nDescription\n\n\n\n\ndf\npolars.DataFrame\nThe input Polars DataFrame.\n\n\n\nExample:\nimport polars as pl\nfrom fsspec_utils.utils.polars import drop_null_columns\n\ndf = pl.DataFrame({\n    \"col1\": [1, 2, 3],\n    \"col2\": [None, None, None],\n    \"col3\": [\"a\", None, \"c\"]\n})\ndf_cleaned = drop_null_columns(df)\nprint(df_cleaned)\n# shape: (3, 2)\n# ┌──────┬───────┐\n# │ col1 ┆ col3  │\n# │ ---  ┆ ---   │\n# │ i64  ┆ str   │\n# ╞══════╪═══════╡\n# │ 1    ┆ a     │\n# │ 2    ┆ null  │\n# │ 3    ┆ c     │\n# └──────┴───────┘\nReturns:\n\nNone"
  },
  {
    "objectID": "api/fsspec_utils.utils.polars.html#with_strftime_columns",
    "href": "api/fsspec_utils.utils.polars.html#with_strftime_columns",
    "title": "fsspec_utils.utils.polars API Reference",
    "section": "",
    "text": "Parameters:\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\ndf\npolars.DataFrame\nThe input Polars DataFrame.\n\n\nstrftime\nstr\nThe strftime format string (e.g., “%Y-%m-%d” for date, “%H” for hour).\n\n\ntimestamp_column\nstr\nThe name of the timestamp column to use. Defaults to ‘auto’ (attempts to infer).\n\n\ncolumn_names\nlist[str] or None\nOptional list of new column names to use for the generated columns. If None, names are derived from the strftime format.\n\n\n\nReturns:\n\nNone"
  },
  {
    "objectID": "api/fsspec_utils.utils.polars.html#with_truncated_columns",
    "href": "api/fsspec_utils.utils.polars.html#with_truncated_columns",
    "title": "fsspec_utils.utils.polars API Reference",
    "section": "",
    "text": "Parameters:\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\ndf\npolars.DataFrame\nThe input Polars DataFrame.\n\n\ntruncate_by\nstr\nThe duration string to truncate by (e.g., “1h”, “1d”, “1mo”).\n\n\ntimestamp_column\nstr\nThe name of the timestamp column to truncate. Defaults to ‘auto’ (attempts to infer).\n\n\ncolumn_names\nlist[str] or None\nOptional list of new column names for the truncated columns. If None, names are derived automatically.\n\n\n\nReturns:\n\nNone"
  },
  {
    "objectID": "api/fsspec_utils.utils.polars.html#with_datepart_columns",
    "href": "api/fsspec_utils.utils.polars.html#with_datepart_columns",
    "title": "fsspec_utils.utils.polars API Reference",
    "section": "",
    "text": "Parameters:\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\ndf\npolars.DataFrame\nThe input Polars DataFrame.\n\n\ntimestamp_column\nstr\nThe name of the timestamp column to extract date parts from. Defaults to ‘auto’ (attempts to infer).\n\n\nyear\nbool\nIf True, extract the year as a new column.\n\n\nmonth\nbool\nIf True, extract the month as a new column.\n\n\nweek\nbool\nIf True, extract the week of the year as a new column.\n\n\nyearday\nbool\nIf True, extract the day of the year as a new column.\n\n\nmonthday\nbool\nIf True, extract the day of the month as a new column.\n\n\nday\nbool\nIf True, extract the day of the week (1-7, Monday=1) as a new column.\n\n\nweekday\nbool\nIf True, extract the weekday (0-6, Monday=0) as a new column.\n\n\nhour\nbool\nIf True, extract the hour as a new column.\n\n\nminute\nbool\nIf True, extract the minute as a new column.\n\n\nstrftime\nstr or None\nOptional strftime format string to apply to the timestamp column before extracting parts.\n\n\n\nReturns:\n\nNone"
  },
  {
    "objectID": "api/fsspec_utils.utils.polars.html#with_row_count",
    "href": "api/fsspec_utils.utils.polars.html#with_row_count",
    "title": "fsspec_utils.utils.polars API Reference",
    "section": "",
    "text": "Parameters:\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\ndf\npolars.DataFrame\nThe input Polars DataFrame.\n\n\nover\nlist[str] or None\nOptional list of column names to partition the data by before adding row counts. If None, a global row count is added.\n\n\n\nReturns:\n\nNone"
  },
  {
    "objectID": "api/fsspec_utils.utils.polars.html#drop_null_columns",
    "href": "api/fsspec_utils.utils.polars.html#drop_null_columns",
    "title": "fsspec_utils.utils.polars API Reference",
    "section": "",
    "text": "Remove columns with all null values from the DataFrame.\nParameters:\n\n\n\nName\nType\nDescription\n\n\n\n\ndf\npolars.DataFrame\nThe input Polars DataFrame.\n\n\n\nReturns:\n\nNone"
  },
  {
    "objectID": "api/fsspec_utils.utils.polars.html#unify_schemas",
    "href": "api/fsspec_utils.utils.polars.html#unify_schemas",
    "title": "fsspec_utils.utils.polars API Reference",
    "section": "",
    "text": "Parameters:\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\ndfs\nlist[polars.DataFrame]\nA list of Polars DataFrames to unify their schemas.\n\n\n\nReturns:\n\nNone"
  },
  {
    "objectID": "api/fsspec_utils.utils.polars.html#cast_relaxed",
    "href": "api/fsspec_utils.utils.polars.html#cast_relaxed",
    "title": "fsspec_utils.utils.polars API Reference",
    "section": "",
    "text": "Parameters:\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\ndf\npolars.DataFrame\nThe input Polars DataFrame to cast.\n\n\nschema\ndict or polars.Schema\nThe target schema to cast the DataFrame to. Can be a dictionary mapping column names to data types or a Polars Schema object.\n\n\n\nReturns:\n\nNone"
  },
  {
    "objectID": "api/fsspec_utils.utils.polars.html#delta",
    "href": "api/fsspec_utils.utils.polars.html#delta",
    "title": "fsspec_utils.utils.polars API Reference",
    "section": "",
    "text": "Parameters:\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\ndf1\npolars.DataFrame\nThe first Polars DataFrame.\n\n\ndf2\npolars.DataFrame\nThe second Polars DataFrame.\n\n\nsubset\nlist[str] or None\nOptional list of column names to consider when calculating the delta. If None, all columns are used.\n\n\neager\nbool\nIf True, the delta calculation is performed eagerly. Defaults to False (lazy).\n\n\n\nReturns:\n\nNone"
  },
  {
    "objectID": "api/fsspec_utils.utils.polars.html#partition_by",
    "href": "api/fsspec_utils.utils.polars.html#partition_by",
    "title": "fsspec_utils.utils.polars API Reference",
    "section": "",
    "text": "Parameters:\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\ndf\npolars.DataFrame\nThe input Polars DataFrame to partition.\n\n\ntimestamp_column\nstr or None\nThe name of the timestamp column to use for time-based partitioning. Defaults to None.\n\n\ncolumns\nlist[str] or None\nOptional list of column names to partition by. Defaults to None.\n\n\nstrftime\nstr or None\nOptional strftime format string for time-based partitioning. Defaults to None.\n\n\ntimedelta\nstr or None\nOptional timedelta string (e.g., “1h”, “1d”) for time-based partitioning. Defaults to None.\n\n\nnum_rows\nint or None\nOptional number of rows per partition for row-based partitioning. Defaults to None.\n\n\n\nReturns:\n\nNone"
  },
  {
    "objectID": "api/index.html",
    "href": "api/index.html",
    "title": "fsspec-utils API Reference",
    "section": "",
    "text": "Welcome to the fsspec-utils API reference documentation. This section provides detailed information on the various modules, classes, and functions available in the library.\n\n\n\nfsspec_utils.core.base\nfsspec_utils.core.ext\n\n\n\n\n\nfsspec_utils.storage_options.base\nfsspec_utils.storage_options.cloud\nfsspec_utils.storage_options.core\nfsspec_utils.storage_options.git\n\n\n\n\n\nfsspec_utils.utils.datetime\nfsspec_utils.utils.logging\nfsspec_utils.utils.misc\nfsspec_utils.utils.polars\nfsspec_utils.utils.pyarrow\nfsspec_utils.utils.sql\nfsspec_utils.utils.types"
  },
  {
    "objectID": "api/index.html#core-modules",
    "href": "api/index.html#core-modules",
    "title": "fsspec-utils API Reference",
    "section": "",
    "text": "fsspec_utils.core.base\nfsspec_utils.core.ext"
  },
  {
    "objectID": "api/index.html#storage-options",
    "href": "api/index.html#storage-options",
    "title": "fsspec-utils API Reference",
    "section": "",
    "text": "fsspec_utils.storage_options.base\nfsspec_utils.storage_options.cloud\nfsspec_utils.storage_options.core\nfsspec_utils.storage_options.git"
  },
  {
    "objectID": "api/index.html#utilities",
    "href": "api/index.html#utilities",
    "title": "fsspec-utils API Reference",
    "section": "",
    "text": "fsspec_utils.utils.datetime\nfsspec_utils.utils.logging\nfsspec_utils.utils.misc\nfsspec_utils.utils.polars\nfsspec_utils.utils.pyarrow\nfsspec_utils.utils.sql\nfsspec_utils.utils.types"
  },
  {
    "objectID": "api/fsspec_utils.utils.logging.html",
    "href": "api/fsspec_utils.utils.logging.html",
    "title": "fsspec_utils.utils.logging API Reference",
    "section": "",
    "text": "Configure the Loguru logger for fsspec-utils.\nRemoves the default handler and adds a new one targeting stderr with customizable level and format.\nParameters:\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nlevel\nstr, optional\nLog level (DEBUG, INFO, WARNING, ERROR, CRITICAL). If None, uses FSSPEC_UTILS_LOG_LEVEL environment variable or defaults to “INFO”.\n\n\ndisable\nbool\nWhether to disable logging for fsspec-utils package.\n\n\nformat_string\nstr, optional\nCustom format string for log messages. If None, uses a default comprehensive format.\n\n\n\nReturns:\n\nNone\n\nExample:\n# Basic setup\nsetup_logging()\n\n# Custom level and format\nsetup_logging(level=\"DEBUG\", format_string=\"{time} | {level} | {message}\")\n\n# Disable logging\nsetup_logging(disable=True)\n\n\n\nGet a logger instance for the given name.\nParameters:\n\n\n\nName\nType\nDescription\n\n\n\n\nname\nstr\nLogger name, typically the module name.\n\n\n\nReturns:\n\nLogger: Configured logger instance.\n\nExample:\nlogger = get_logger(__name__)\nlogger.info(\"This is a log message\")"
  },
  {
    "objectID": "api/fsspec_utils.utils.logging.html#setup_logging",
    "href": "api/fsspec_utils.utils.logging.html#setup_logging",
    "title": "fsspec_utils.utils.logging API Reference",
    "section": "",
    "text": "Configure the Loguru logger for fsspec-utils.\nRemoves the default handler and adds a new one targeting stderr with customizable level and format.\nParameters:\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nlevel\nstr, optional\nLog level (DEBUG, INFO, WARNING, ERROR, CRITICAL). If None, uses FSSPEC_UTILS_LOG_LEVEL environment variable or defaults to “INFO”.\n\n\ndisable\nbool\nWhether to disable logging for fsspec-utils package.\n\n\nformat_string\nstr, optional\nCustom format string for log messages. If None, uses a default comprehensive format.\n\n\n\nReturns:\n\nNone\n\nExample:\n# Basic setup\nsetup_logging()\n\n# Custom level and format\nsetup_logging(level=\"DEBUG\", format_string=\"{time} | {level} | {message}\")\n\n# Disable logging\nsetup_logging(disable=True)"
  },
  {
    "objectID": "api/fsspec_utils.utils.logging.html#get_logger",
    "href": "api/fsspec_utils.utils.logging.html#get_logger",
    "title": "fsspec_utils.utils.logging API Reference",
    "section": "",
    "text": "Get a logger instance for the given name.\nParameters:\n\n\n\nName\nType\nDescription\n\n\n\n\nname\nstr\nLogger name, typically the module name.\n\n\n\nReturns:\n\nLogger: Configured logger instance.\n\nExample:\nlogger = get_logger(__name__)\nlogger.info(\"This is a log message\")"
  },
  {
    "objectID": "api/fsspec_utils.storage_options.cloud.html",
    "href": "api/fsspec_utils.storage_options.cloud.html",
    "title": "fsspec_utils.storage_options.cloud API Documentation",
    "section": "",
    "text": "This module defines storage option classes for various cloud providers, including Azure, Google Cloud Storage (GCS), and Amazon Web Services (AWS) S3. These classes provide structured ways to configure access to cloud storage, supporting different authentication methods and specific cloud service parameters.\n\n\n\nAzure Storage configuration options.\nProvides configuration for Azure storage services:\n\nAzure Blob Storage (az://)\nAzure Data Lake Storage Gen2 (abfs://)\nAzure Data Lake Storage Gen1 (adl://)\n\nSupports multiple authentication methods:\n\nConnection string\nAccount key\nService principal\nManaged identity\nSAS token\n\nAttributes:\n\nprotocol (str): Storage protocol (“az”, “abfs”, or “adl”)\naccount_name (str): Storage account name\naccount_key (str): Storage account access key\nconnection_string (str): Full connection string\ntenant_id (str): Azure AD tenant ID\nclient_id (str): Service principal client ID\nclient_secret (str): Service principal client secret\nsas_token (str): SAS token for limited access\n\nExample:\nfrom fsspec_utils.storage_options.cloud import AzureStorageOptions\n\n# Blob Storage with account key\noptions = AzureStorageOptions(\n    protocol=\"az\",\n    account_name=\"mystorageacct\",\n    account_key=\"key123...\"\n)\n\n# Data Lake with service principal\noptions = AzureStorageOptions(\n    protocol=\"abfs\",\n    account_name=\"mydatalake\",\n    tenant_id=\"tenant123\",\n    client_id=\"client123\",\n    client_secret=\"secret123\"\n)\n\n# Simple connection string auth\noptions = AzureStorageOptions(\n    protocol=\"az\",\n    connection_string=\"DefaultEndpoints...\"\n)\n\n\nCreate storage options from environment variables.\nReads standard Azure environment variables:\n\nAZURE_STORAGE_PROTOCOL\nAZURE_STORAGE_ACCOUNT_NAME\nAZURE_STORAGE_ACCOUNT_KEY\nAZURE_STORAGE_CONNECTION_STRING\nAZURE_TENANT_ID\nAZURE_CLIENT_ID\nAZURE_CLIENT_SECRET\nAZURE_STORAGE_SAS_TOKEN\n\n\n\n\n\n\n\n\n\nReturns\nType\nDescription\n\n\n\n\nAzureStorageOptions\nAzureStorageOptions\nConfigured storage options\n\n\n\nExample:\n# With environment variables set:\nfrom fsspec_utils.storage_options.cloud import AzureStorageOptions\nimport os\n\n# Set environment variables for testing (replace with actual values if needed)\nos.environ[\"AZURE_STORAGE_ACCOUNT_NAME\"] = \"mystorageacct\"\nos.environ[\"AZURE_STORAGE_ACCOUNT_KEY\"] = \"dummy_key\" # Dummy key for example\n\noptions = AzureStorageOptions.from_env()\nprint(options.account_name)  # From AZURE_STORAGE_ACCOUNT_NAME\n# 'mystorageacct'\n\n# Clean up environment variables\ndel os.environ[\"AZURE_STORAGE_ACCOUNT_NAME\"]\ndel os.environ[\"AZURE_STORAGE_ACCOUNT_KEY\"]\n\n\n\nExport options to environment variables.\nSets standard Azure environment variables.\nExample:\nfrom fsspec_utils.storage_options.cloud import AzureStorageOptions\nimport os\n\noptions = AzureStorageOptions(\n    protocol=\"az\",\n    account_name=\"mystorageacct\",\n    account_key=\"key123\"\n)\noptions.to_env()\nprint(os.getenv(\"AZURE_STORAGE_ACCOUNT_NAME\"))\n# 'mystorageacct'\n\n# Clean up environment variables\ndel os.environ[\"AZURE_STORAGE_ACCOUNT_NAME\"]\ndel os.environ[\"AZURE_STORAGE_ACCOUNT_KEY\"]\n\n\n\n\n\nGoogle Cloud Storage configuration options.\nProvides configuration for GCS access with support for:\n\nService account authentication\nDefault application credentials\nToken-based authentication\nProject configuration\nCustom endpoints\n\nAttributes:\n\nprotocol (str): Storage protocol (“gs” or “gcs”)\ntoken (str): Path to service account JSON file\nproject (str): Google Cloud project ID\naccess_token (str): OAuth2 access token\nendpoint_url (str): Custom storage endpoint\ntimeout (int): Request timeout in seconds\n\nExample:\nfrom fsspec_utils.storage_options.cloud import GcsStorageOptions\n\n# Service account auth\noptions = GcsStorageOptions(\n    protocol=\"gs\",\n    token=\"path/to/service-account.json\",\n    project=\"my-project-123\"\n)\n\n# Application default credentials\noptions = GcsStorageOptions(\n    protocol=\"gcs\",\n    project=\"my-project-123\"\n)\n\n# Custom endpoint (e.g., test server)\noptions = GcsStorageOptions(\n    protocol=\"gs\",\n    endpoint_url=\"http://localhost:4443\",\n    token=\"test-token.json\"\n)\n\n\nCreate storage options from environment variables.\nReads standard GCP environment variables:\n\nGOOGLE_CLOUD_PROJECT: Project\nGOOGLE_APPLICATION_CREDENTIALS: Service account file path\nSTORAGE_EMULATOR_HOST: Custom endpoint (for testing)\nGCS_OAUTH_TOKEN: OAuth2 access token\n\n\n\n\n\n\n\n\n\nReturns\nType\nDescription\n\n\n\n\nGcsStorageOptions\nGcsStorageOptions\nConfigured storage options\n\n\n\nExample:\n# With environment variables set:\nfrom fsspec_utils.storage_options.cloud import GcsStorageOptions\nimport os\n\n# Set environment variables for testing (replace with actual values if needed)\nos.environ[\"GOOGLE_CLOUD_PROJECT\"] = \"my-project-123\"\n\noptions = GcsStorageOptions.from_env()\nprint(options.project)  # From GOOGLE_CLOUD_PROJECT\n# 'my-project-123'\n\n# Clean up environment variables\ndel os.environ[\"GOOGLE_CLOUD_PROJECT\"]\n\n\n\nExport options to environment variables.\nSets standard GCP environment variables.\nExample:\nfrom fsspec_utils.storage_options.cloud import GcsStorageOptions\nimport os\n\noptions = GcsStorageOptions(\n    protocol=\"gs\",\n    project=\"my-project\",\n    token=\"service-account.json\"\n)\noptions.to_env()\nprint(os.getenv(\"GOOGLE_CLOUD_PROJECT\"))\n# 'my-project'\n\n# Clean up environment variables\ndel os.environ[\"GOOGLE_CLOUD_PROJECT\"]\n\n\n\nConvert options to fsspec filesystem arguments.\n\n\n\nReturns\nType\nDescription\n\n\n\n\ndict\ndict\nArguments suitable for GCSFileSystem\n\n\n\nExample:\nfrom fsspec_utils.storage_options.cloud import GcsStorageOptions\nfrom fsspec_utils.core.base import filesystem\n\noptions = GcsStorageOptions(\n    protocol=\"gs\",\n    token=\"service-account.json\",\n    project=\"my-project\"\n)\nkwargs = options.to_fsspec_kwargs()\nfs = filesystem(\"gcs\", **kwargs)\n\n\n\n\n\nAWS S3 storage configuration options.\nProvides comprehensive configuration for S3 access with support for:\n\nMultiple authentication methods (keys, profiles, environment)\nCustom endpoints for S3-compatible services\nRegion configuration\nSSL/TLS settings\n\nAttributes:\n\nprotocol (str): Always “s3” for S3 storage\naccess_key_id (str): AWS access key ID\nsecret_access_key (str): AWS secret access key\nsession_token (str): AWS session token\nendpoint_url (str): Custom S3 endpoint URL\nregion (str): AWS region name\nallow_invalid_certificates (bool): Skip SSL certificate validation\nallow_http (bool): Allow unencrypted HTTP connections\n\nExample:\n# Basic credentials\noptions = AwsStorageOptions(\n    access_key_id=\"AKIAXXXXXXXX\",\n    secret_access_key=\"SECRETKEY\",\n    region=\"us-east-1\"\n)\n\n# Profile-based auth\noptions = AwsStorageOptions.create(profile=\"dev\")\n\n# S3-compatible service (MinIO)\noptions = AwsStorageOptions(\n    endpoint_url=\"http://localhost:9000\",\n    access_key_id=\"minioadmin\",\n    secret_access_key=\"minioadmin\",\n    allow_http=True\n)\n\n\nCreates an AwsStorageOptions instance, handling aliases and profile loading.\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\n\n\n\n\nprotocol\nstr\nStorage protocol, defaults to “s3”.\n\n\naccess_key_id\nstr | None\nAWS access key ID.\n\n\nsecret_access_key\nstr | None\nAWS secret access key.\n\n\nsession_token\nstr | None\nAWS session token.\n\n\nendpoint_url\nstr | None\nCustom S3 endpoint URL.\n\n\nregion\nstr | None\nAWS region name.\n\n\nallow_invalid_certificates\nbool | None\nSkip SSL certificate validation.\n\n\nallow_http\nbool | None\nAllow unencrypted HTTP connections.\n\n\nkey\nstr | None\nAlias for access_key_id.\n\n\nsecret\nstr | None\nAlias for secret_access_key.\n\n\ntoken\nstr | None\nAlias for session_token.\n\n\nprofile\nstr | None\nAWS credentials profile name to load credentials from.\n\n\n\n\n\n\n\n\n\n\n\nReturns\nType\nDescription\n\n\n\n\nAwsStorageOptions\nAwsStorageOptions\nAn initialized AwsStorageOptions instance.\n\n\n\n\n\n\nCreate storage options from AWS credentials file.\nLoads credentials from ~/.aws/credentials and ~/.aws/config files.\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\n\n\n\n\nprofile\nstr\nAWS credentials profile name\n\n\nallow_invalid_certificates\nbool\nSkip SSL certificate validation\n\n\nallow_http\nbool\nAllow unencrypted HTTP connections\n\n\n\n\n\n\n\n\n\n\n\nReturns\nType\nDescription\n\n\n\n\nAwsStorageOptions\nAwsStorageOptions\nConfigured storage options\n\n\n\n\n\n\n\n\n\n\n\nRaises\nType\nDescription\n\n\n\n\nValueError\nValueError\nIf profile not found\n\n\nFileNotFoundError\nFileNotFoundError\nIf credentials files missing\n\n\n\nExample:\n# Load developer profile\noptions = AwsStorageOptions.from_aws_credentials(\n    profile=\"dev\",\n    allow_http=True  # For local testing\n)\n\n\n\nCreate storage options from environment variables.\nReads standard AWS environment variables:\n\nAWS_ACCESS_KEY_ID\nAWS_SECRET_ACCESS_KEY\nAWS_SESSION_TOKEN\nAWS_ENDPOINT_URL\nAWS_DEFAULT_REGION\nALLOW_INVALID_CERTIFICATE\nAWS_ALLOW_HTTP\n\n\n\n\n\n\n\n\n\nReturns\nType\nDescription\n\n\n\n\nAwsStorageOptions\nAwsStorageOptions\nConfigured storage options\n\n\n\nExample:\n# Load from environment\nfrom fsspec_utils.storage_options.cloud import AwsStorageOptions\nimport os\n\n# Set environment variables for testing (replace with actual values if needed)\nos.environ[\"AWS_DEFAULT_REGION\"] = \"us-east-1\"\n\noptions = AwsStorageOptions.from_env()\nprint(options.region)\n# 'us-east-1'  # From AWS_DEFAULT_REGION\n\n# Clean up environment variables\ndel os.environ[\"AWS_DEFAULT_REGION\"]\n\n\n\nConvert options to fsspec filesystem arguments.\n\n\n\nReturns\nType\nDescription\n\n\n\n\ndict\ndict\nArguments suitable for fsspec S3FileSystem\n\n\n\nExample:\noptions = AwsStorageOptions(\n    access_key_id=\"KEY\",\n    secret_access_key=\"SECRET\",\n    region=\"us-west-2\"\n)\nkwargs = options.to_fsspec_kwargs()\nfs = filesystem(\"s3\", **kwargs)\n\n\n\nConvert options to object store arguments.\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\n\n\n\n\nwith_conditional_put\nbool\nAdd etag-based conditional put support\n\n\n\n\n\n\nReturns\nType\nDescription\n\n\n\n\ndict\ndict\nArguments suitable for object store clients\n\n\n\nExample:\nfrom fsspec_utils.storage_options.cloud import AwsStorageOptions\n# Assuming ObjectStore is a hypothetical client for demonstration\n# from some_object_store_library import ObjectStore\n\noptions = AwsStorageOptions(\n    access_key_id=\"KEY\",\n    secret_access_key=\"SECRET\"\n)\nkwargs = options.to_object_store_kwargs()\n# client = ObjectStore(**kwargs)\n\n\n\nExport options to environment variables.\nSets standard AWS environment variables.\nExample:\nfrom fsspec_utils.storage_options.cloud import AwsStorageOptions\nimport os\n\noptions = AwsStorageOptions(\n    access_key_id=\"KEY\",\n    secret_access_key=\"SECRET\",\n    region=\"us-east-1\"\n)\noptions.to_env()\nprint(os.getenv(\"AWS_ACCESS_KEY_ID\"))\n# 'KEY'\n\n# Clean up environment variables\ndel os.environ[\"AWS_ACCESS_KEY_ID\"]\ndel os.environ[\"AWS_SECRET_ACCESS_KEY\"]\nif \"AWS_DEFAULT_REGION\" in os.environ: # Only delete if it was set\n    del os.environ[\"AWS_DEFAULT_REGION\"]\n\n\n\nCreate fsspec filesystem instance from options.\n\n\n\n\n\n\n\n\nReturns\nType\nDescription\n\n\n\n\nAbstractFileSystem\nAbstractFileSystem\nConfigured filesystem instance"
  },
  {
    "objectID": "api/fsspec_utils.storage_options.cloud.html#azurestorageoptions",
    "href": "api/fsspec_utils.storage_options.cloud.html#azurestorageoptions",
    "title": "fsspec_utils.storage_options.cloud API Documentation",
    "section": "",
    "text": "Azure Storage configuration options.\nProvides configuration for Azure storage services:\n\nAzure Blob Storage (az://)\nAzure Data Lake Storage Gen2 (abfs://)\nAzure Data Lake Storage Gen1 (adl://)\n\nSupports multiple authentication methods:\n\nConnection string\nAccount key\nService principal\nManaged identity\nSAS token\n\nAttributes:\n\nprotocol (str): Storage protocol (“az”, “abfs”, or “adl”)\naccount_name (str): Storage account name\naccount_key (str): Storage account access key\nconnection_string (str): Full connection string\ntenant_id (str): Azure AD tenant ID\nclient_id (str): Service principal client ID\nclient_secret (str): Service principal client secret\nsas_token (str): SAS token for limited access\n\nExample:\nfrom fsspec_utils.storage_options.cloud import AzureStorageOptions\n\n# Blob Storage with account key\noptions = AzureStorageOptions(\n    protocol=\"az\",\n    account_name=\"mystorageacct\",\n    account_key=\"key123...\"\n)\n\n# Data Lake with service principal\noptions = AzureStorageOptions(\n    protocol=\"abfs\",\n    account_name=\"mydatalake\",\n    tenant_id=\"tenant123\",\n    client_id=\"client123\",\n    client_secret=\"secret123\"\n)\n\n# Simple connection string auth\noptions = AzureStorageOptions(\n    protocol=\"az\",\n    connection_string=\"DefaultEndpoints...\"\n)\n\n\nCreate storage options from environment variables.\nReads standard Azure environment variables:\n\nAZURE_STORAGE_PROTOCOL\nAZURE_STORAGE_ACCOUNT_NAME\nAZURE_STORAGE_ACCOUNT_KEY\nAZURE_STORAGE_CONNECTION_STRING\nAZURE_TENANT_ID\nAZURE_CLIENT_ID\nAZURE_CLIENT_SECRET\nAZURE_STORAGE_SAS_TOKEN\n\n\n\n\n\n\n\n\n\nReturns\nType\nDescription\n\n\n\n\nAzureStorageOptions\nAzureStorageOptions\nConfigured storage options\n\n\n\nExample:\n# With environment variables set:\nfrom fsspec_utils.storage_options.cloud import AzureStorageOptions\nimport os\n\n# Set environment variables for testing (replace with actual values if needed)\nos.environ[\"AZURE_STORAGE_ACCOUNT_NAME\"] = \"mystorageacct\"\nos.environ[\"AZURE_STORAGE_ACCOUNT_KEY\"] = \"dummy_key\" # Dummy key for example\n\noptions = AzureStorageOptions.from_env()\nprint(options.account_name)  # From AZURE_STORAGE_ACCOUNT_NAME\n# 'mystorageacct'\n\n# Clean up environment variables\ndel os.environ[\"AZURE_STORAGE_ACCOUNT_NAME\"]\ndel os.environ[\"AZURE_STORAGE_ACCOUNT_KEY\"]\n\n\n\nExport options to environment variables.\nSets standard Azure environment variables.\nExample:\nfrom fsspec_utils.storage_options.cloud import AzureStorageOptions\nimport os\n\noptions = AzureStorageOptions(\n    protocol=\"az\",\n    account_name=\"mystorageacct\",\n    account_key=\"key123\"\n)\noptions.to_env()\nprint(os.getenv(\"AZURE_STORAGE_ACCOUNT_NAME\"))\n# 'mystorageacct'\n\n# Clean up environment variables\ndel os.environ[\"AZURE_STORAGE_ACCOUNT_NAME\"]\ndel os.environ[\"AZURE_STORAGE_ACCOUNT_KEY\"]"
  },
  {
    "objectID": "api/fsspec_utils.storage_options.cloud.html#gcsstorageoptions",
    "href": "api/fsspec_utils.storage_options.cloud.html#gcsstorageoptions",
    "title": "fsspec_utils.storage_options.cloud API Documentation",
    "section": "",
    "text": "Google Cloud Storage configuration options.\nProvides configuration for GCS access with support for:\n\nService account authentication\nDefault application credentials\nToken-based authentication\nProject configuration\nCustom endpoints\n\nAttributes:\n\nprotocol (str): Storage protocol (“gs” or “gcs”)\ntoken (str): Path to service account JSON file\nproject (str): Google Cloud project ID\naccess_token (str): OAuth2 access token\nendpoint_url (str): Custom storage endpoint\ntimeout (int): Request timeout in seconds\n\nExample:\nfrom fsspec_utils.storage_options.cloud import GcsStorageOptions\n\n# Service account auth\noptions = GcsStorageOptions(\n    protocol=\"gs\",\n    token=\"path/to/service-account.json\",\n    project=\"my-project-123\"\n)\n\n# Application default credentials\noptions = GcsStorageOptions(\n    protocol=\"gcs\",\n    project=\"my-project-123\"\n)\n\n# Custom endpoint (e.g., test server)\noptions = GcsStorageOptions(\n    protocol=\"gs\",\n    endpoint_url=\"http://localhost:4443\",\n    token=\"test-token.json\"\n)\n\n\nCreate storage options from environment variables.\nReads standard GCP environment variables:\n\nGOOGLE_CLOUD_PROJECT: Project\nGOOGLE_APPLICATION_CREDENTIALS: Service account file path\nSTORAGE_EMULATOR_HOST: Custom endpoint (for testing)\nGCS_OAUTH_TOKEN: OAuth2 access token\n\n\n\n\n\n\n\n\n\nReturns\nType\nDescription\n\n\n\n\nGcsStorageOptions\nGcsStorageOptions\nConfigured storage options\n\n\n\nExample:\n# With environment variables set:\nfrom fsspec_utils.storage_options.cloud import GcsStorageOptions\nimport os\n\n# Set environment variables for testing (replace with actual values if needed)\nos.environ[\"GOOGLE_CLOUD_PROJECT\"] = \"my-project-123\"\n\noptions = GcsStorageOptions.from_env()\nprint(options.project)  # From GOOGLE_CLOUD_PROJECT\n# 'my-project-123'\n\n# Clean up environment variables\ndel os.environ[\"GOOGLE_CLOUD_PROJECT\"]\n\n\n\nExport options to environment variables.\nSets standard GCP environment variables.\nExample:\nfrom fsspec_utils.storage_options.cloud import GcsStorageOptions\nimport os\n\noptions = GcsStorageOptions(\n    protocol=\"gs\",\n    project=\"my-project\",\n    token=\"service-account.json\"\n)\noptions.to_env()\nprint(os.getenv(\"GOOGLE_CLOUD_PROJECT\"))\n# 'my-project'\n\n# Clean up environment variables\ndel os.environ[\"GOOGLE_CLOUD_PROJECT\"]\n\n\n\nConvert options to fsspec filesystem arguments.\n\n\n\nReturns\nType\nDescription\n\n\n\n\ndict\ndict\nArguments suitable for GCSFileSystem\n\n\n\nExample:\nfrom fsspec_utils.storage_options.cloud import GcsStorageOptions\nfrom fsspec_utils.core.base import filesystem\n\noptions = GcsStorageOptions(\n    protocol=\"gs\",\n    token=\"service-account.json\",\n    project=\"my-project\"\n)\nkwargs = options.to_fsspec_kwargs()\nfs = filesystem(\"gcs\", **kwargs)"
  },
  {
    "objectID": "api/fsspec_utils.storage_options.cloud.html#awsstorageoptions",
    "href": "api/fsspec_utils.storage_options.cloud.html#awsstorageoptions",
    "title": "fsspec_utils.storage_options.cloud API Documentation",
    "section": "",
    "text": "AWS S3 storage configuration options.\nProvides comprehensive configuration for S3 access with support for:\n\nMultiple authentication methods (keys, profiles, environment)\nCustom endpoints for S3-compatible services\nRegion configuration\nSSL/TLS settings\n\nAttributes:\n\nprotocol (str): Always “s3” for S3 storage\naccess_key_id (str): AWS access key ID\nsecret_access_key (str): AWS secret access key\nsession_token (str): AWS session token\nendpoint_url (str): Custom S3 endpoint URL\nregion (str): AWS region name\nallow_invalid_certificates (bool): Skip SSL certificate validation\nallow_http (bool): Allow unencrypted HTTP connections\n\nExample:\n# Basic credentials\noptions = AwsStorageOptions(\n    access_key_id=\"AKIAXXXXXXXX\",\n    secret_access_key=\"SECRETKEY\",\n    region=\"us-east-1\"\n)\n\n# Profile-based auth\noptions = AwsStorageOptions.create(profile=\"dev\")\n\n# S3-compatible service (MinIO)\noptions = AwsStorageOptions(\n    endpoint_url=\"http://localhost:9000\",\n    access_key_id=\"minioadmin\",\n    secret_access_key=\"minioadmin\",\n    allow_http=True\n)\n\n\nCreates an AwsStorageOptions instance, handling aliases and profile loading.\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\n\n\n\n\nprotocol\nstr\nStorage protocol, defaults to “s3”.\n\n\naccess_key_id\nstr | None\nAWS access key ID.\n\n\nsecret_access_key\nstr | None\nAWS secret access key.\n\n\nsession_token\nstr | None\nAWS session token.\n\n\nendpoint_url\nstr | None\nCustom S3 endpoint URL.\n\n\nregion\nstr | None\nAWS region name.\n\n\nallow_invalid_certificates\nbool | None\nSkip SSL certificate validation.\n\n\nallow_http\nbool | None\nAllow unencrypted HTTP connections.\n\n\nkey\nstr | None\nAlias for access_key_id.\n\n\nsecret\nstr | None\nAlias for secret_access_key.\n\n\ntoken\nstr | None\nAlias for session_token.\n\n\nprofile\nstr | None\nAWS credentials profile name to load credentials from.\n\n\n\n\n\n\n\n\n\n\n\nReturns\nType\nDescription\n\n\n\n\nAwsStorageOptions\nAwsStorageOptions\nAn initialized AwsStorageOptions instance.\n\n\n\n\n\n\nCreate storage options from AWS credentials file.\nLoads credentials from ~/.aws/credentials and ~/.aws/config files.\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\n\n\n\n\nprofile\nstr\nAWS credentials profile name\n\n\nallow_invalid_certificates\nbool\nSkip SSL certificate validation\n\n\nallow_http\nbool\nAllow unencrypted HTTP connections\n\n\n\n\n\n\n\n\n\n\n\nReturns\nType\nDescription\n\n\n\n\nAwsStorageOptions\nAwsStorageOptions\nConfigured storage options\n\n\n\n\n\n\n\n\n\n\n\nRaises\nType\nDescription\n\n\n\n\nValueError\nValueError\nIf profile not found\n\n\nFileNotFoundError\nFileNotFoundError\nIf credentials files missing\n\n\n\nExample:\n# Load developer profile\noptions = AwsStorageOptions.from_aws_credentials(\n    profile=\"dev\",\n    allow_http=True  # For local testing\n)\n\n\n\nCreate storage options from environment variables.\nReads standard AWS environment variables:\n\nAWS_ACCESS_KEY_ID\nAWS_SECRET_ACCESS_KEY\nAWS_SESSION_TOKEN\nAWS_ENDPOINT_URL\nAWS_DEFAULT_REGION\nALLOW_INVALID_CERTIFICATE\nAWS_ALLOW_HTTP\n\n\n\n\n\n\n\n\n\nReturns\nType\nDescription\n\n\n\n\nAwsStorageOptions\nAwsStorageOptions\nConfigured storage options\n\n\n\nExample:\n# Load from environment\nfrom fsspec_utils.storage_options.cloud import AwsStorageOptions\nimport os\n\n# Set environment variables for testing (replace with actual values if needed)\nos.environ[\"AWS_DEFAULT_REGION\"] = \"us-east-1\"\n\noptions = AwsStorageOptions.from_env()\nprint(options.region)\n# 'us-east-1'  # From AWS_DEFAULT_REGION\n\n# Clean up environment variables\ndel os.environ[\"AWS_DEFAULT_REGION\"]\n\n\n\nConvert options to fsspec filesystem arguments.\n\n\n\nReturns\nType\nDescription\n\n\n\n\ndict\ndict\nArguments suitable for fsspec S3FileSystem\n\n\n\nExample:\noptions = AwsStorageOptions(\n    access_key_id=\"KEY\",\n    secret_access_key=\"SECRET\",\n    region=\"us-west-2\"\n)\nkwargs = options.to_fsspec_kwargs()\nfs = filesystem(\"s3\", **kwargs)\n\n\n\nConvert options to object store arguments.\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\n\n\n\n\nwith_conditional_put\nbool\nAdd etag-based conditional put support\n\n\n\n\n\n\nReturns\nType\nDescription\n\n\n\n\ndict\ndict\nArguments suitable for object store clients\n\n\n\nExample:\nfrom fsspec_utils.storage_options.cloud import AwsStorageOptions\n# Assuming ObjectStore is a hypothetical client for demonstration\n# from some_object_store_library import ObjectStore\n\noptions = AwsStorageOptions(\n    access_key_id=\"KEY\",\n    secret_access_key=\"SECRET\"\n)\nkwargs = options.to_object_store_kwargs()\n# client = ObjectStore(**kwargs)\n\n\n\nExport options to environment variables.\nSets standard AWS environment variables.\nExample:\nfrom fsspec_utils.storage_options.cloud import AwsStorageOptions\nimport os\n\noptions = AwsStorageOptions(\n    access_key_id=\"KEY\",\n    secret_access_key=\"SECRET\",\n    region=\"us-east-1\"\n)\noptions.to_env()\nprint(os.getenv(\"AWS_ACCESS_KEY_ID\"))\n# 'KEY'\n\n# Clean up environment variables\ndel os.environ[\"AWS_ACCESS_KEY_ID\"]\ndel os.environ[\"AWS_SECRET_ACCESS_KEY\"]\nif \"AWS_DEFAULT_REGION\" in os.environ: # Only delete if it was set\n    del os.environ[\"AWS_DEFAULT_REGION\"]\n\n\n\nCreate fsspec filesystem instance from options.\n\n\n\n\n\n\n\n\nReturns\nType\nDescription\n\n\n\n\nAbstractFileSystem\nAbstractFileSystem\nConfigured filesystem instance"
  },
  {
    "objectID": "api/fsspec_utils.utils.misc.html",
    "href": "api/fsspec_utils.utils.misc.html",
    "title": "fsspec_utils.utils.misc API Reference",
    "section": "",
    "text": "Run a function for a list of parameters in parallel.\nProvides parallel execution with progress tracking and flexible argument handling.\nParameters:\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nfunc\nCallable\nThe function to be executed in parallel.\n\n\n*args\nAny\nPositional arguments to pass to func. If an iterable, func will be called for each item.\n\n\nn_jobs\nint\nThe number of CPU cores to use. -1 means all available cores.\n\n\nbackend\nstr\nThe backend to use for parallel processing. Options include ‘loky’, ‘threading’, ‘multiprocessing’, and ‘sequential’.\n\n\nverbose\nbool\nIf True, a progress bar will be displayed during execution.\n\n\n**kwargs\nAny\nKeyword arguments to pass to func. If an iterable, func will be called for each item.\n\n\n\nReturns:\n\nlist: List of function outputs in the same order as inputs.\n\nRaises:\n\nValueError: If no iterable arguments provided or length mismatch.\n\nExamples:\n# Single iterable argument\nrun_parallel(str.upper, [\"hello\", \"world\"])\n\n# Multiple iterables in args and kwargs\ndef add(x, y, offset=0):\n    return x + y + offset\nrun_parallel(add, [1, 2, 3], y=[4, 5, 6], offset=10)\n\n# Fixed and iterable arguments\nrun_parallel(pow, [2, 3, 4], exp=2)\n\n\n\nExtract dataset partitions from a file path.\nParses file paths to extract partition information based on different partitioning schemes.\nParameters:\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\npath\nstr\nThe file path from which to extract partition information.\n\n\npartitioning\nstr or list[str] or None\nThe partitioning scheme to use. Can be “hive” for Hive-style, a string for a single partition column, a list of strings for multiple partition columns, or None for no specific partitioning.\n\n\n\nReturns:\n\nlist[tuple[str, str]]: List of tuples containing (column, value) pairs.\n\nExamples:\n# Hive-style partitioning\nget_partitions_from_path(\"data/year=2023/month=01/file.parquet\", \"hive\")\n\n# Single partition column\nget_partitions_from_path(\"data/2023/01/file.parquet\", \"year\")\n\n# Multiple partition columns\nget_partitions_from_path(\"data/2023/01/file.parquet\", [\"year\", \"month\"])\n\n\n\nConvert a path to a glob pattern for file matching.\nIntelligently converts paths to glob patterns that match files of the specified format, handling various directory and wildcard patterns.\nParameters:\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\npath\nstr\nThe file or directory path to convert into a glob pattern.\n\n\nformat\nstr or None\nThe desired file format or extension to match (e.g., “parquet”, “csv”, “json”). If None, the format is inferred from the path.\n\n\n\nReturns:\n\nstr: Glob pattern for matching files\n\nExample:\n# Directory to parquet files glob\npath_to_glob(\"data/\", \"parquet\")\n\n# Already a glob pattern\npath_to_glob(\"data/*.csv\")\n\n# Specific file\npath_to_glob(\"data/file.json\")\n\n\n\nCheck if an optional dependency is available.\nParameters:\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\npackage_name\nstr\nThe name of the optional package to check for availability.\n\n\nfeature_name\nstr\nA descriptive name of the feature that requires this package.\n\n\n\nRaises:\n\nImportError: If the package is not available"
  },
  {
    "objectID": "api/fsspec_utils.utils.misc.html#run_parallel",
    "href": "api/fsspec_utils.utils.misc.html#run_parallel",
    "title": "fsspec_utils.utils.misc API Reference",
    "section": "",
    "text": "Run a function for a list of parameters in parallel.\nProvides parallel execution with progress tracking and flexible argument handling.\nParameters:\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nfunc\nCallable\nThe function to be executed in parallel.\n\n\n*args\nAny\nPositional arguments to pass to func. If an iterable, func will be called for each item.\n\n\nn_jobs\nint\nThe number of CPU cores to use. -1 means all available cores.\n\n\nbackend\nstr\nThe backend to use for parallel processing. Options include ‘loky’, ‘threading’, ‘multiprocessing’, and ‘sequential’.\n\n\nverbose\nbool\nIf True, a progress bar will be displayed during execution.\n\n\n**kwargs\nAny\nKeyword arguments to pass to func. If an iterable, func will be called for each item.\n\n\n\nReturns:\n\nlist: List of function outputs in the same order as inputs.\n\nRaises:\n\nValueError: If no iterable arguments provided or length mismatch.\n\nExamples:\n# Single iterable argument\nrun_parallel(str.upper, [\"hello\", \"world\"])\n\n# Multiple iterables in args and kwargs\ndef add(x, y, offset=0):\n    return x + y + offset\nrun_parallel(add, [1, 2, 3], y=[4, 5, 6], offset=10)\n\n# Fixed and iterable arguments\nrun_parallel(pow, [2, 3, 4], exp=2)"
  },
  {
    "objectID": "api/fsspec_utils.utils.misc.html#get_partitions_from_path",
    "href": "api/fsspec_utils.utils.misc.html#get_partitions_from_path",
    "title": "fsspec_utils.utils.misc API Reference",
    "section": "",
    "text": "Extract dataset partitions from a file path.\nParses file paths to extract partition information based on different partitioning schemes.\nParameters:\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\npath\nstr\nThe file path from which to extract partition information.\n\n\npartitioning\nstr or list[str] or None\nThe partitioning scheme to use. Can be “hive” for Hive-style, a string for a single partition column, a list of strings for multiple partition columns, or None for no specific partitioning.\n\n\n\nReturns:\n\nlist[tuple[str, str]]: List of tuples containing (column, value) pairs.\n\nExamples:\n# Hive-style partitioning\nget_partitions_from_path(\"data/year=2023/month=01/file.parquet\", \"hive\")\n\n# Single partition column\nget_partitions_from_path(\"data/2023/01/file.parquet\", \"year\")\n\n# Multiple partition columns\nget_partitions_from_path(\"data/2023/01/file.parquet\", [\"year\", \"month\"])"
  },
  {
    "objectID": "api/fsspec_utils.utils.misc.html#path_to_glob",
    "href": "api/fsspec_utils.utils.misc.html#path_to_glob",
    "title": "fsspec_utils.utils.misc API Reference",
    "section": "",
    "text": "Convert a path to a glob pattern for file matching.\nIntelligently converts paths to glob patterns that match files of the specified format, handling various directory and wildcard patterns.\nParameters:\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\npath\nstr\nThe file or directory path to convert into a glob pattern.\n\n\nformat\nstr or None\nThe desired file format or extension to match (e.g., “parquet”, “csv”, “json”). If None, the format is inferred from the path.\n\n\n\nReturns:\n\nstr: Glob pattern for matching files\n\nExample:\n# Directory to parquet files glob\npath_to_glob(\"data/\", \"parquet\")\n\n# Already a glob pattern\npath_to_glob(\"data/*.csv\")\n\n# Specific file\npath_to_glob(\"data/file.json\")"
  },
  {
    "objectID": "api/fsspec_utils.utils.misc.html#check_optional_dependency",
    "href": "api/fsspec_utils.utils.misc.html#check_optional_dependency",
    "title": "fsspec_utils.utils.misc API Reference",
    "section": "",
    "text": "Check if an optional dependency is available.\nParameters:\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\npackage_name\nstr\nThe name of the optional package to check for availability.\n\n\nfeature_name\nstr\nA descriptive name of the feature that requires this package.\n\n\n\nRaises:\n\nImportError: If the package is not available"
  },
  {
    "objectID": "architecture.html",
    "href": "architecture.html",
    "title": "Architecture Overview",
    "section": "",
    "text": "fsspec-utils is designed to extend and enhance the capabilities of fsspec, providing a robust and flexible framework for interacting with various filesystems and data formats. Its architecture is modular, built around core components that abstract away complexities and offer specialized functionalities.\n\n\nAt its core, fsspec-utils builds upon the fsspec (Filesystem Spec) library, which provides a unified Pythonic interface to various storage backends. fsspec-utils extends this functionality by:\n\nSimplifying Storage Configuration: It offers StorageOptions classes for various cloud providers (AWS S3, Google Cloud Storage, Azure Storage) and Git platforms (GitHub, GitLab), allowing for easier and more consistent configuration of filesystem access.\nEnhancing I/O Operations: It provides extended read/write capabilities for common data formats like JSON, CSV, and Parquet, with integrations for high-performance libraries like Polars and PyArrow.\nImproving Caching: The library includes an enhanced caching mechanism that preserves directory structures and offers better monitoring.\n\n\n\n\nThe fsspec-utils library is organized into several key modules:\n\n\nThis module contains the fundamental extensions to fsspec. It includes the filesystem function, which acts as a central factory for creating fsspec compatible filesystem objects, potentially with enhanced features like caching and extended I/O. The DirFileSystem class is also part of this module, providing specialized handling for directory-based filesystems.\n\n\n\nThis module is dedicated to managing storage configurations for different backends. It defines various StorageOptions classes (e.g., AwsStorageOptions, GcsStorageOptions, AzureStorageOptions, GitHubStorageOptions, GitLabStorageOptions) that encapsulate the necessary parameters for connecting to specific storage services. It also includes utility functions for inferring protocols from URIs and merging storage options.\n\n\n\nThe utils module provides a collection of general-purpose utility functions that support various operations within fsspec-utils. These include:\n\nParallel Processing: Functions like run_parallel for executing tasks concurrently.\nType Conversion: Utilities such as dict_to_dataframe and to_pyarrow_table for data manipulation.\nLogging: A setup for consistent logging across the library.\nPyArrow and Polars Integration: A lot of utility functions, e.g. for optimizing data types and schemas when working with PyArrow tables and Polars DataFrames.\n\n\n\n\n\n\n\n\n\n\n\ngraph TD\n    A[fsspec-utils] --&gt; B(Core Module)\n    A --&gt; C(Storage Options Module)\n    A --&gt; D(Utils Module)\n    B --&gt; E[Extends fsspec]\n    C --&gt; F{Cloud Providers}\n    C --&gt; G{Git Platforms}\n    D --&gt; H[Parallel Processing]\n    D --&gt; I[Type Conversion]\n    D --&gt; J[Logging]\n    D --&gt; K[PyArrow/Polars Integration]\n    F --&gt; L(AWS S3)\n    F --&gt; M(Google Cloud Storage)\n    F --&gt; N(Azure Storage)\n    G --&gt; O(GitHub)\n    G --&gt; P(GitLab)"
  },
  {
    "objectID": "architecture.html#extending-fsspec",
    "href": "architecture.html#extending-fsspec",
    "title": "Architecture Overview",
    "section": "",
    "text": "At its core, fsspec-utils builds upon the fsspec (Filesystem Spec) library, which provides a unified Pythonic interface to various storage backends. fsspec-utils extends this functionality by:\n\nSimplifying Storage Configuration: It offers StorageOptions classes for various cloud providers (AWS S3, Google Cloud Storage, Azure Storage) and Git platforms (GitHub, GitLab), allowing for easier and more consistent configuration of filesystem access.\nEnhancing I/O Operations: It provides extended read/write capabilities for common data formats like JSON, CSV, and Parquet, with integrations for high-performance libraries like Polars and PyArrow.\nImproving Caching: The library includes an enhanced caching mechanism that preserves directory structures and offers better monitoring."
  },
  {
    "objectID": "architecture.html#core-components",
    "href": "architecture.html#core-components",
    "title": "Architecture Overview",
    "section": "",
    "text": "The fsspec-utils library is organized into several key modules:\n\n\nThis module contains the fundamental extensions to fsspec. It includes the filesystem function, which acts as a central factory for creating fsspec compatible filesystem objects, potentially with enhanced features like caching and extended I/O. The DirFileSystem class is also part of this module, providing specialized handling for directory-based filesystems.\n\n\n\nThis module is dedicated to managing storage configurations for different backends. It defines various StorageOptions classes (e.g., AwsStorageOptions, GcsStorageOptions, AzureStorageOptions, GitHubStorageOptions, GitLabStorageOptions) that encapsulate the necessary parameters for connecting to specific storage services. It also includes utility functions for inferring protocols from URIs and merging storage options.\n\n\n\nThe utils module provides a collection of general-purpose utility functions that support various operations within fsspec-utils. These include:\n\nParallel Processing: Functions like run_parallel for executing tasks concurrently.\nType Conversion: Utilities such as dict_to_dataframe and to_pyarrow_table for data manipulation.\nLogging: A setup for consistent logging across the library.\nPyArrow and Polars Integration: A lot of utility functions, e.g. for optimizing data types and schemas when working with PyArrow tables and Polars DataFrames."
  },
  {
    "objectID": "architecture.html#diagrams",
    "href": "architecture.html#diagrams",
    "title": "Architecture Overview",
    "section": "",
    "text": "graph TD\n    A[fsspec-utils] --&gt; B(Core Module)\n    A --&gt; C(Storage Options Module)\n    A --&gt; D(Utils Module)\n    B --&gt; E[Extends fsspec]\n    C --&gt; F{Cloud Providers}\n    C --&gt; G{Git Platforms}\n    D --&gt; H[Parallel Processing]\n    D --&gt; I[Type Conversion]\n    D --&gt; J[Logging]\n    D --&gt; K[PyArrow/Polars Integration]\n    F --&gt; L(AWS S3)\n    F --&gt; M(Google Cloud Storage)\n    F --&gt; N(Azure Storage)\n    G --&gt; O(GitHub)\n    G --&gt; P(GitLab)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to fsspec-utils!",
    "section": "",
    "text": "fsspec-utils is a powerful library designed to enhance fsspec (Filesystem Spec) with advanced utilities and extensions, making multi-format I/O, cloud storage configuration, caching, monitoring, and batch processing more streamlined and efficient.\n\n\nThis library aims to simplify complex data operations across various file systems, providing a unified and extended interface for handling diverse data formats and storage solutions. Whether you’re working with local files, cloud storage like AWS S3, Azure Blob Storage, or Google Cloud Storage, fsspec-utils provides the tools to manage your data effectively.\n\n\n\n\nMulti-format Data I/O: Seamlessly read and write data in various formats, including JSON, CSV, and Parquet.\nCloud Storage Configuration: Simplified utilities for configuring and interacting with different cloud storage providers.\nEnhanced Caching and Monitoring: Improve performance and gain insights into your data operations with built-in caching mechanisms and monitoring capabilities.\nBatch Processing and Parallel Operations: Efficiently handle large datasets and execute operations in parallel for improved throughput.\nDirectory-like Filesystem: Interact with nested data structures as if they were traditional directories, even on object stores.\n\n\n\n\nReady to dive in? Check out our Quickstart Guide to begin using fsspec-utils in your projects."
  },
  {
    "objectID": "index.html#purpose",
    "href": "index.html#purpose",
    "title": "Welcome to fsspec-utils!",
    "section": "",
    "text": "This library aims to simplify complex data operations across various file systems, providing a unified and extended interface for handling diverse data formats and storage solutions. Whether you’re working with local files, cloud storage like AWS S3, Azure Blob Storage, or Google Cloud Storage, fsspec-utils provides the tools to manage your data effectively."
  },
  {
    "objectID": "index.html#key-features",
    "href": "index.html#key-features",
    "title": "Welcome to fsspec-utils!",
    "section": "",
    "text": "Multi-format Data I/O: Seamlessly read and write data in various formats, including JSON, CSV, and Parquet.\nCloud Storage Configuration: Simplified utilities for configuring and interacting with different cloud storage providers.\nEnhanced Caching and Monitoring: Improve performance and gain insights into your data operations with built-in caching mechanisms and monitoring capabilities.\nBatch Processing and Parallel Operations: Efficiently handle large datasets and execute operations in parallel for improved throughput.\nDirectory-like Filesystem: Interact with nested data structures as if they were traditional directories, even on object stores."
  },
  {
    "objectID": "index.html#get-started",
    "href": "index.html#get-started",
    "title": "Welcome to fsspec-utils!",
    "section": "",
    "text": "Ready to dive in? Check out our Quickstart Guide to begin using fsspec-utils in your projects."
  },
  {
    "objectID": "examples.html",
    "href": "examples.html",
    "title": "Examples",
    "section": "",
    "text": "This section provides a series of examples demonstrating basic and common functionalities of fsspec-utils. Each example includes an executable Python code block and a clear explanation of its purpose. For more advanced features, performance optimizations, and integrations with specialized data systems, please refer to the Advanced Usage documentation.\n\n\nfsspec-utils simplifies configuring connections to various storage systems, including local filesystems, AWS S3, Azure Storage, and Google Cloud Storage, using StorageOptions classes. These options can then be converted into fsspec filesystems.\n\n\nThis example demonstrates how to initialize LocalStorageOptions and use it to interact with the local filesystem.\nStep-by-step walkthrough:\n\nCreate a temporary directory for our test\nCreate a test file and write content to it\nList files in the directory to verify our file was created\nRead the content back to verify it was written correctly\nClean up the temporary directory\n\nStorageOptions classes simplify configuration for different storage systems and provide a consistent interface for creating fsspec filesystem objects.\nimport os\nimport tempfile\nimport shutil\nfrom fsspec_utils.storage_options import LocalStorageOptions\n\nprint(\"=== LocalStorageOptions Example ===\\n\")\n\nlocal_options = LocalStorageOptions(auto_mkdir=True)\nlocal_fs = local_options.to_filesystem()\n\ntemp_dir = tempfile.mkdtemp()\nprint(f\"Working in temporary directory: {temp_dir}\")\n\ntemp_file = os.path.join(temp_dir, \"test_file.txt\")\nwith local_fs.open(temp_file, \"w\") as f:\n    f.write(\"Hello, LocalStorageOptions!\")\nprint(f\"Created test file: {temp_file}\")\n\nfiles = local_fs.ls(temp_dir)\nprint(f\"Files in {temp_dir}: {[os.path.basename(f) for f in files]}\")\n\nwith local_fs.open(temp_file, \"r\") as f:\n    content = f.read()\nprint(f\"File content: '{content}'\")\n\nshutil.rmtree(temp_dir)\nprint(f\"Cleaned up temporary directory: {temp_dir}\")\nprint(\"Local storage example completed.\\n\")\n\n\n\nThis example demonstrates the configuration pattern for AwsStorageOptions. It is expected to fail when attempting to connect to actual cloud services because it uses dummy credentials.\nNote: The to_filesystem() method converts StorageOptions into fsspec-compatible objects, allowing seamless integration with any fsspec-compatible library.\nfrom fsspec_utils.storage_options import AwsStorageOptions\n\nprint(\"=== Conceptual AwsStorageOptions Example (using a dummy endpoint) ===\\n\")\n\naws_options = AwsStorageOptions(\n    endpoint_url=\"http://s3.dummy-endpoint.com\",\n    access_key_id=\"DUMMY_KEY\",\n    secret_access_key=\"DUMMY_SECRET\",\n    allow_http=True,\n    region=\"us-east-1\"\n)\n\naws_fs = aws_options.to_filesystem()\nprint(f\"Created fsspec filesystem for S3: {type(aws_fs).__name__}\")\nprint(\"AWS storage example completed.\\n\")\n\n\n\nThis example shows how to configure AzureStorageOptions. It is expected to fail when attempting to connect to actual cloud services because it uses dummy credentials.\nfrom fsspec_utils.storage_options import AzureStorageOptions\n\nprint(\"=== Conceptual AzureStorageOptions Example (using a dummy connection string) ===\\n\")\nazure_options = AzureStorageOptions(\n    protocol=\"az\",\n    account_name=\"demoaccount\",\n    connection_string=\"DefaultEndpointsProtocol=https;AccountName=demoaccount;AccountKey=demokey==;EndpointSuffix=core.windows.net\"\n)\n\nazure_fs = azure_options.to_filesystem()\nprint(f\"Created fsspec filesystem for Azure: {type(azure_fs).__name__}\")\nprint(\"Azure storage example completed.\\n\")\n\n\n\nThis example shows how to configure GcsStorageOptions. It is expected to fail when attempting to connect to actual cloud services because it uses dummy credentials.\nStorageOptions classes provide a simplified, consistent interface for configuring connections to various storage systems. They abstract away the complexity of different storage backends and provide a unified way to create fsspec filesystem objects.\nThe to_filesystem() method converts these options into fsspec compatible objects, enabling seamless integration with any fsspec-compatible library or tool.\nImportant Note: The AWS, Azure, and GCS examples use dummy credentials and are for illustrative purposes only. These examples are expected to fail when attempting to connect to actual cloud services because:\n\nThe endpoint URLs are not real service endpoints\nThe credentials are placeholder values that don’t correspond to actual accounts\nThe connection strings and tokens are examples, not valid credentials\n\nThis approach allows you to understand the configuration pattern without needing actual cloud credentials. When using these examples in production, you would replace the dummy values with your real credentials and service endpoints.\n```python from fsspec_utils.storage_options import GcsStorageOptions\nprint(“=== Conceptual GcsStorageOptions Example (using a dummy token path) ===”) gcs_options = GcsStorageOptions( protocol=“gs”, project=“demo-project”, token=“path/to/dummy-service-account.json” )\ngcs_fs = gcs_options.to_filesystem() print(f”Created fsspec filesystem for GCS: {type(gcs_fs).__name__}“) print(”GCS storage example completed.“)"
  },
  {
    "objectID": "examples.html#flexible-storage-configuration",
    "href": "examples.html#flexible-storage-configuration",
    "title": "Examples",
    "section": "",
    "text": "fsspec-utils simplifies configuring connections to various storage systems, including local filesystems, AWS S3, Azure Storage, and Google Cloud Storage, using StorageOptions classes. These options can then be converted into fsspec filesystems.\n\n\nThis example demonstrates how to initialize LocalStorageOptions and use it to interact with the local filesystem.\nStep-by-step walkthrough:\n\nCreate a temporary directory for our test\nCreate a test file and write content to it\nList files in the directory to verify our file was created\nRead the content back to verify it was written correctly\nClean up the temporary directory\n\nStorageOptions classes simplify configuration for different storage systems and provide a consistent interface for creating fsspec filesystem objects.\nimport os\nimport tempfile\nimport shutil\nfrom fsspec_utils.storage_options import LocalStorageOptions\n\nprint(\"=== LocalStorageOptions Example ===\\n\")\n\nlocal_options = LocalStorageOptions(auto_mkdir=True)\nlocal_fs = local_options.to_filesystem()\n\ntemp_dir = tempfile.mkdtemp()\nprint(f\"Working in temporary directory: {temp_dir}\")\n\ntemp_file = os.path.join(temp_dir, \"test_file.txt\")\nwith local_fs.open(temp_file, \"w\") as f:\n    f.write(\"Hello, LocalStorageOptions!\")\nprint(f\"Created test file: {temp_file}\")\n\nfiles = local_fs.ls(temp_dir)\nprint(f\"Files in {temp_dir}: {[os.path.basename(f) for f in files]}\")\n\nwith local_fs.open(temp_file, \"r\") as f:\n    content = f.read()\nprint(f\"File content: '{content}'\")\n\nshutil.rmtree(temp_dir)\nprint(f\"Cleaned up temporary directory: {temp_dir}\")\nprint(\"Local storage example completed.\\n\")\n\n\n\nThis example demonstrates the configuration pattern for AwsStorageOptions. It is expected to fail when attempting to connect to actual cloud services because it uses dummy credentials.\nNote: The to_filesystem() method converts StorageOptions into fsspec-compatible objects, allowing seamless integration with any fsspec-compatible library.\nfrom fsspec_utils.storage_options import AwsStorageOptions\n\nprint(\"=== Conceptual AwsStorageOptions Example (using a dummy endpoint) ===\\n\")\n\naws_options = AwsStorageOptions(\n    endpoint_url=\"http://s3.dummy-endpoint.com\",\n    access_key_id=\"DUMMY_KEY\",\n    secret_access_key=\"DUMMY_SECRET\",\n    allow_http=True,\n    region=\"us-east-1\"\n)\n\naws_fs = aws_options.to_filesystem()\nprint(f\"Created fsspec filesystem for S3: {type(aws_fs).__name__}\")\nprint(\"AWS storage example completed.\\n\")\n\n\n\nThis example shows how to configure AzureStorageOptions. It is expected to fail when attempting to connect to actual cloud services because it uses dummy credentials.\nfrom fsspec_utils.storage_options import AzureStorageOptions\n\nprint(\"=== Conceptual AzureStorageOptions Example (using a dummy connection string) ===\\n\")\nazure_options = AzureStorageOptions(\n    protocol=\"az\",\n    account_name=\"demoaccount\",\n    connection_string=\"DefaultEndpointsProtocol=https;AccountName=demoaccount;AccountKey=demokey==;EndpointSuffix=core.windows.net\"\n)\n\nazure_fs = azure_options.to_filesystem()\nprint(f\"Created fsspec filesystem for Azure: {type(azure_fs).__name__}\")\nprint(\"Azure storage example completed.\\n\")\n\n\n\nThis example shows how to configure GcsStorageOptions. It is expected to fail when attempting to connect to actual cloud services because it uses dummy credentials.\nStorageOptions classes provide a simplified, consistent interface for configuring connections to various storage systems. They abstract away the complexity of different storage backends and provide a unified way to create fsspec filesystem objects.\nThe to_filesystem() method converts these options into fsspec compatible objects, enabling seamless integration with any fsspec-compatible library or tool.\nImportant Note: The AWS, Azure, and GCS examples use dummy credentials and are for illustrative purposes only. These examples are expected to fail when attempting to connect to actual cloud services because:\n\nThe endpoint URLs are not real service endpoints\nThe credentials are placeholder values that don’t correspond to actual accounts\nThe connection strings and tokens are examples, not valid credentials\n\nThis approach allows you to understand the configuration pattern without needing actual cloud credentials. When using these examples in production, you would replace the dummy values with your real credentials and service endpoints.\n```python from fsspec_utils.storage_options import GcsStorageOptions\nprint(“=== Conceptual GcsStorageOptions Example (using a dummy token path) ===”) gcs_options = GcsStorageOptions( protocol=“gs”, project=“demo-project”, token=“path/to/dummy-service-account.json” )\ngcs_fs = gcs_options.to_filesystem() print(f”Created fsspec filesystem for GCS: {type(gcs_fs).__name__}“) print(”GCS storage example completed.“)"
  },
  {
    "objectID": "contributing.html",
    "href": "contributing.html",
    "title": "Contributing to fsspec-utils",
    "section": "",
    "text": "We welcome contributions to fsspec-utils! Your help makes this project better. This guide outlines how you can contribute, from reporting issues to submitting pull requests.\n\n\n\n\nIf you encounter any bugs, unexpected behavior, or have suggestions for new features, please open an issue on our GitHub Issues page.\nWhen reporting an issue, please include: - A clear and concise description of the problem. - Steps to reproduce the behavior. - Expected behavior. - Screenshots or error messages if applicable. - Your fsspec-utils version and Python environment details.\n\n\n\nWe gladly accept pull requests for bug fixes, new features, and improvements. To submit a pull request:\n\nFork the Repository: Start by forking the fsspec-utils repository on GitHub.\nClone Your Fork: Clone your forked repository to your local machine. bash     git clone https://github.com/your-username/fsspec-utils.git     cd fsspec-utils\nCreate a New Branch: Create a new branch for your changes. bash     git checkout -b feature/your-feature-name     # or     git checkout -b bugfix/issue-description\nMake Your Changes: Implement your bug fix or feature.\nWrite Tests: Ensure your changes are covered by appropriate unit tests.\nRun Tests: Verify all tests pass before submitting. bash     uv run pytest\nFormat Code: Ensure your code adheres to the project’s style guidelines. The project uses ruff for linting and formatting. bash     uv run ruff check . --fix     uv run ruff format .\nCommit Your Changes: Write clear and concise commit messages. bash     git commit -m \"feat: Add new awesome feature\"\nPush to Your Fork: Push your branch to your forked repository. bash     git push origin feature/your-feature-name\nOpen a Pull Request: Go to the original fsspec-utils repository on GitHub and open a pull request from your new branch. Provide a detailed description of your changes.\n\n\n\n\n\nTo set up your development environment, follow these steps:\n\nClone the repository: bash     git clone https://github.com/fsspec/fsspec-utils.git     cd fsspec-utils\nInstall uv: fsspec-utils uses uv for dependency management and running commands. If you don’t have uv installed, you can install it via pip: bash     pip install uv\nInstall Development Dependencies: The project uses uv to manage dependencies. Install the dev dependency group which includes tools for testing, linting, and documentation generation. bash     uv pip install -e \".[dev]\" This command installs the project in editable mode (-e) and includes all development-related dependencies specified in pyproject.toml under the [project.optional-dependencies] dev section.\n\n\n\n\n\nCode Style: Adhere to the existing code style. We use ruff for linting and formatting.\nTesting: All new features and bug fixes should be accompanied by relevant unit tests.\nDocumentation: If your changes introduce new features or modify existing behavior, please update the documentation accordingly.\nCommit Messages: Write descriptive commit messages that explain the purpose of your changes.\nAtomic Commits: Try to keep your commits focused on a single logical change.\nBranch Naming: Use clear and concise branch names (e.g., feature/new-feature, bugfix/fix-issue-123)."
  },
  {
    "objectID": "contributing.html#how-to-contribute",
    "href": "contributing.html#how-to-contribute",
    "title": "Contributing to fsspec-utils",
    "section": "",
    "text": "If you encounter any bugs, unexpected behavior, or have suggestions for new features, please open an issue on our GitHub Issues page.\nWhen reporting an issue, please include: - A clear and concise description of the problem. - Steps to reproduce the behavior. - Expected behavior. - Screenshots or error messages if applicable. - Your fsspec-utils version and Python environment details.\n\n\n\nWe gladly accept pull requests for bug fixes, new features, and improvements. To submit a pull request:\n\nFork the Repository: Start by forking the fsspec-utils repository on GitHub.\nClone Your Fork: Clone your forked repository to your local machine. bash     git clone https://github.com/your-username/fsspec-utils.git     cd fsspec-utils\nCreate a New Branch: Create a new branch for your changes. bash     git checkout -b feature/your-feature-name     # or     git checkout -b bugfix/issue-description\nMake Your Changes: Implement your bug fix or feature.\nWrite Tests: Ensure your changes are covered by appropriate unit tests.\nRun Tests: Verify all tests pass before submitting. bash     uv run pytest\nFormat Code: Ensure your code adheres to the project’s style guidelines. The project uses ruff for linting and formatting. bash     uv run ruff check . --fix     uv run ruff format .\nCommit Your Changes: Write clear and concise commit messages. bash     git commit -m \"feat: Add new awesome feature\"\nPush to Your Fork: Push your branch to your forked repository. bash     git push origin feature/your-feature-name\nOpen a Pull Request: Go to the original fsspec-utils repository on GitHub and open a pull request from your new branch. Provide a detailed description of your changes."
  },
  {
    "objectID": "contributing.html#development-setup",
    "href": "contributing.html#development-setup",
    "title": "Contributing to fsspec-utils",
    "section": "",
    "text": "To set up your development environment, follow these steps:\n\nClone the repository: bash     git clone https://github.com/fsspec/fsspec-utils.git     cd fsspec-utils\nInstall uv: fsspec-utils uses uv for dependency management and running commands. If you don’t have uv installed, you can install it via pip: bash     pip install uv\nInstall Development Dependencies: The project uses uv to manage dependencies. Install the dev dependency group which includes tools for testing, linting, and documentation generation. bash     uv pip install -e \".[dev]\" This command installs the project in editable mode (-e) and includes all development-related dependencies specified in pyproject.toml under the [project.optional-dependencies] dev section."
  },
  {
    "objectID": "contributing.html#best-practices-for-contributions",
    "href": "contributing.html#best-practices-for-contributions",
    "title": "Contributing to fsspec-utils",
    "section": "",
    "text": "Code Style: Adhere to the existing code style. We use ruff for linting and formatting.\nTesting: All new features and bug fixes should be accompanied by relevant unit tests.\nDocumentation: If your changes introduce new features or modify existing behavior, please update the documentation accordingly.\nCommit Messages: Write descriptive commit messages that explain the purpose of your changes.\nAtomic Commits: Try to keep your commits focused on a single logical change.\nBranch Naming: Use clear and concise branch names (e.g., feature/new-feature, bugfix/fix-issue-123)."
  },
  {
    "objectID": "advanced.html",
    "href": "advanced.html",
    "title": "Advanced Usage",
    "section": "",
    "text": "fsspec-utils extends the capabilities of fsspec to provide a more robust and feature-rich experience for handling diverse file systems and data formats. This section delves into advanced features, configurations, and performance tips to help you get the most out of the library.\n\n\nThe fsspec_utils.core.filesystem function offers a centralized and enhanced way to instantiate fsspec filesystem objects. It supports:\n\nIntelligent Caching: Automatically wraps filesystems with MonitoredSimpleCacheFileSystem for improved performance and verbose logging of cache operations.\nStructured Storage Options: Integrates seamlessly with fsspec_utils.storage_options classes, allowing for type-safe and organized configuration of cloud and Git-based storage.\nProtocol Inference: Can infer the filesystem protocol directly from a URI or path, reducing boilerplate.\n\nExample: Cached S3 Filesystem with Structured Options\nfrom fsspec_utils.core import filesystem\nfrom fsspec_utils.storage_options import AwsStorageOptions\n\n# Configure S3 options using the structured class\ns3_opts = AwsStorageOptions(\n    region=\"us-east-1\",\n    access_key_id=\"YOUR_ACCESS_KEY\",\n    secret_access_key=\"YOUR_SECRET_KEY\"\n)\n\n# Create a cached S3 filesystem using the 'filesystem' helper\nfs = filesystem(\n    \"s3\",\n    storage_options=s3_opts,\n    cached=True,\n    cache_storage=\"/tmp/s3_cache\", # Optional: specify cache directory\n    verbose=True # Enable verbose cache logging\n)\n\n# Use the filesystem as usual\nprint(fs.ls(\"s3://your-bucket/\"))\n\n\nfsspec-utils provides an enhanced caching mechanism that improves performance for repeated file operations, especially useful for remote filesystems.\nThis example demonstrates how caching improves read performance. The first read populates the cache, while subsequent reads retrieve data directly from the cache, significantly reducing access time. It also shows that data can still be retrieved from the cache even if the original source becomes unavailable.\nCaching in fsspec-utils is an enhanced mechanism that improves performance for repeated file operations, especially useful for remote filesystems where network latency can significantly impact performance.\nThe filesystem() function provides several parameters for configuring caching:\n\ncached: When set to True, enables caching for all read operations\ncache_storage: Specifies the directory where cached files will be stored\nverbose: When set to True, provides detailed logging about cache operations\n\nStep-by-step walkthrough:\n\nFirst read (populating cache): When reading a file for the first time, the data is retrieved from the source (disk, network, etc.) and stored in the cache directory. This takes longer than subsequent reads because it involves both reading from the source and writing to the cache.\nSecond read (using cache): When the same file is read again, the data is retrieved directly from the cache instead of the source. This is significantly faster because it avoids network latency or disk I/O.\nDemonstrating cache effectiveness: Even after the original file is removed, the cached version can still be accessed. This demonstrates that the cache acts as a persistent copy of the data, independent of the source file.\nPerformance comparison: The timing results clearly show the performance benefits of caching, with subsequent reads being orders of magnitude faster than the initial read.\n\nThis caching mechanism is particularly valuable when working with:\n\nRemote filesystems (S3, GCS, Azure) where network latency is a bottleneck\nFrequently accessed files that don’t change often\nApplications that read the same data multiple times\nEnvironments with unreliable network connections\n\n\n\nIn this step, we create a sample JSON file and initialize the fsspec-utils filesystem with caching enabled. The first read operation retrieves data from the source and populates the cache.\nSetup steps:\n\nCreate a temporary directory for our example\nCreate sample data file\nConfigure filesystem with caching\n\nimport tempfile\nimport time\nimport os\nimport shutil\nfrom fsspec_utils import filesystem\nfrom examples.caching.setup_data import create_sample_data_file\n\ntmpdir = tempfile.mkdtemp()\nprint(f\"Created temporary directory: {tmpdir}\")\n\nsample_file = create_sample_data_file(tmpdir)\n\ncache_dir = os.path.join(tmpdir, \"cache\")\nfs = filesystem(\n    protocol_or_path=\"file\",\n    cached=True,\n    cache_storage=cache_dir,\n    verbose=True\n)\n\nprint(\"\\n=== First read (populating cache) ===\")\nstart_time = time.time()\ndata1 = fs.read_json(sample_file)\nfirst_read_time = time.time() - start_time\nprint(f\"First read completed in {first_read_time:.4f} seconds\")\n\n\n\nNow, let’s read the same file again to see the performance improvement from using the cache.\nprint(\"\\n=== Second read (using cache) ===\")\nstart_time = time.time()\ndata2 = fs.read_json(sample_file)\nsecond_read_time = time.time() - start_time\nprint(f\"Second read completed in {second_read_time:.4f} seconds\")\nThe second read retrieves data directly from the cache, which is significantly faster than reading from the source again.\n\n\n\nTo demonstrate that the cache is persistent, we’ll remove the original file and try to read it again.\nprint(\"\\n=== Demonstrating cache effectiveness ===\")\nprint(\"Removing original file...\")\nos.remove(sample_file)\nprint(f\"Original file exists: {os.path.exists(sample_file)}\")\n\nprint(\"\\n=== Third read (from cache only) ===\")\nstart_time = time.time()\ndata3 = fs.read_json(sample_file)\nthird_read_time = time.time() - start_time\nprint(f\"Third read completed in {third_read_time:.4f} seconds\")\nprint(\"✓ Successfully read from cache even after original file was removed\")\n\nprint(\"\\n=== Performance Comparison ===\")\nprint(f\"First read (from disk): {first_read_time:.4f} seconds\")\nprint(f\"Second read (from cache): {second_read_time:.4f} seconds\")\nprint(f\"Third read (from cache): {third_read_time:.4f} seconds\")\n\nshutil.rmtree(tmpdir)\nprint(f\"Cleaned up temporary directory: {tmpdir}\")\nThis step proves that the cache acts as a persistent copy of the data, allowing access even if the original source is unavailable.\n\n\n\n\n\nfsspec-utils provides specialized filesystem implementations for unique use cases:\n\n\nAccess files directly from GitLab repositories. This is particularly useful for configuration files, datasets, or code stored in private or public GitLab instances.\nExample: Reading from a GitLab Repository\nfrom fsspec_utils.core import filesystem\n\n# Instantiate a GitLab filesystem\ngitlab_fs = filesystem(\n    \"gitlab\",\n    storage_options={\n        \"project_name\": \"your-group/your-project\", # Or \"project_id\": 12345\n        \"ref\": \"main\", # Branch, tag, or commit SHA\n        \"token\": \"glpat_YOUR_PRIVATE_TOKEN\" # Required for private repos\n    }\n)\n\n# List files in the repository root\nprint(gitlab_fs.ls(\"/\"))\n\n# Read a specific file\ncontent = gitlab_fs.cat(\"README.md\").decode(\"utf-8\")\nprint(content[:200]) # Print first 200 characters\n\n\n\n\nThe fsspec_utils.core.ext module (exposed via AbstractFileSystem extensions) provides powerful functions for reading and writing various data formats (JSON, CSV, Parquet) with advanced features like:\n\nBatch Processing: Efficiently handle large datasets by processing files in configurable batches.\nParallel Processing: Leverage multi-threading to speed up file I/O operations.\nSchema Unification & Optimization: Automatically unifies schemas when concatenating multiple files and optimizes data types for memory efficiency (e.g., using Polars’ opt_dtypes or PyArrow’s schema casting).\nFile Path Tracking: Optionally include the source file path as a column in the resulting DataFrame/Table.\n\n\n\nThe read_files function acts as a universal reader, delegating to format-specific readers (JSON, CSV, Parquet) while maintaining consistent options.\nExample: Reading CSVs in Batches with Parallelism\nfrom fsspec_utils.core import filesystem\n\n# Assuming you have multiple CSV files like 'data/part_0.csv', 'data/part_1.csv', etc.\n# on your local filesystem\nfs = filesystem(\"file\")\n\n# Read CSV files in batches of 10, using multiple threads, and including file path\nfor batch_df in fs.read_files(\n    \"data/*.csv\",\n    format=\"csv\",\n    batch_size=10,\n    include_file_path=True,\n    use_threads=True,\n    verbose=True\n):\n    print(f\"Processed batch with {len(batch_df)} rows. Columns: {batch_df.columns}\")\n    print(batch_df.head(2))\n\n\n\nfsspec-utils simplifies reading multiple files of various formats (Parquet, CSV, JSON) from a folder into a single PyArrow Table or Polars DataFrame.\nReading multiple files into a single table is a powerful feature that allows you to efficiently process data distributed across multiple files. This is particularly useful when dealing with large datasets that are split into smaller files for better organization or parallel processing.\nKey concepts demonstrated:\n\nGlob patterns: The **/*.parquet, **/*.csv, and **/*.json patterns are used to select files recursively from the directory and its subdirectories. The ** pattern matches any directories, allowing the function to find files in nested directories.\nConcat parameter: The concat=True parameter tells the function to combine data from multiple files into a single table or DataFrame. When set to False, the function would return a list of individual tables/DataFrames.\nFormat flexibility: The same interface can be used to read different file formats (Parquet, CSV, JSON), making it easy to work with heterogeneous data sources.\n\nStep-by-step explanation:\n\nCreating sample data: We create two subdirectories and populate them with sample data in three different formats (Parquet, CSV, JSON). Each format contains the same structured data but in different serialization formats.\nReading Parquet files: Using fs.read_parquet(\"**/*.parquet\", concat=True), we read all Parquet files recursively and combine them into a single PyArrow Table. Parquet is a columnar storage format that is highly efficient for analytical workloads.\nReading CSV files: Using fs.read_csv(\"**/*.csv\", concat=True), we read all CSV files and combine them into a Polars DataFrame, which we then convert to a PyArrow Table for consistency.\nReading JSON files: Using fs.read_json(\"**/*.json\", as_dataframe=True, concat=True), we read all JSON files and combine them into a Polars DataFrame, then convert it to a PyArrow Table.\nVerification: Finally, we verify that all three tables have the same number of rows, confirming that the data was correctly read and combined across all files and formats.\n\nThe flexibility of fsspec-utils allows you to use the same approach with different data sources, including remote filesystems like S3, GCS, or Azure Blob Storage, simply by changing the filesystem path.\n\n\nFirst, we’ll create a temporary directory with sample data in different formats.\nSetup steps:\n\nCreate a temporary directory for our example\nCreate sample data in subdirectories\n\nimport tempfile\nimport shutil\nimport os\nfrom examples.read_folder.create_dataset import create_sample_dataset\n\ntemp_dir = tempfile.mkdtemp()\nprint(f\"Created temporary directory: {temp_dir}\")\n\ncreate_sample_dataset(temp_dir)\nThis step sets up the environment by creating a temporary directory and populating it with sample data files.\n\n\n\nNow, let’s read all the Parquet files from the directory and its subdirectories into a single PyArrow Table.\nReading Parquet files:\n\nRead Parquet files using glob pattern\nDisplay table information and sample data\n\nprint(\"\\n=== Reading Parquet Files ===\")\nfrom fsspec_utils import filesystem\nfs = filesystem(temp_dir)\nparquet_table = fs.read_parquet(\"**/*.parquet\", concat=True)\nprint(f\"Successfully read Parquet files into PyArrow Table\")\nprint(f\"Table shape: {parquet_table.num_rows} rows x {parquet_table.num_columns} columns\")\nprint(\"First 3 rows:\")\nprint(parquet_table.slice(0, 3).to_pandas())\nWe use the read_parquet method with a glob pattern **/*.parquet to find all Parquet files recursively. The concat=True parameter combines them into a single table.\n\n\n\nNext, we’ll read all the CSV files into a Polars DataFrame and then convert it to a PyArrow Table.\nReading CSV files:\n\nRead CSV files using glob pattern\nDisplay DataFrame information and sample data\nConvert to PyArrow Table for consistency\n\nprint(\"\\n=== Reading CSV Files ===\")\ncsv_df = fs.read_csv(\"**/*.csv\", concat=True)\nprint(f\"Successfully read CSV files into Polars DataFrame\")\nprint(f\"DataFrame shape: {csv_df.shape}\")\nprint(\"First 3 rows:\")\nprint(csv_df.head(3))\ncsv_table = csv_df.to_arrow()\nSimilarly, we use read_csv with the same glob pattern to read all CSV files.\n\n\n\nFinally, let’s read all the JSON files.\nReading JSON files:\n\nRead JSON files using glob pattern\nDisplay DataFrame information and sample data\nConvert to PyArrow Table for consistency\n\nprint(\"\\n=== Reading JSON Files ===\")\njson_df = fs.read_json(\"**/*.json\", as_dataframe=True, concat=True)\nprint(f\"Successfully read JSON files into Polars DataFrame\")\nprint(f\"DataFrame shape: {json_df.shape}\")\nprint(\"First 3 rows:\")\nprint(json_df.head(3))\njson_table = json_df.to_arrow()\nThe read_json method is used to read all JSON files. We set as_dataframe=True to get a Polars DataFrame.\n\n\n\nLet’s verify that all the tables have the same number of rows.\nprint(\"\\n=== Verification ===\")\nprint(f\"All tables have the same number of rows: {parquet_table.num_rows == csv_table.num_rows == json_table.num_rows}\")\n\nshutil.rmtree(temp_dir)\nprint(f\"\\nCleaned up temporary directory: {temp_dir}\")\nThis final step confirms that our data reading and concatenation were successful.\nThis example shows how to read various file formats from a directory, including subdirectories, into a unified PyArrow Table or Polars DataFrame. It highlights the flexibility of fsspec-utils in handling different data sources and formats.\nfsspec-utils enables efficient batch processing of large datasets by reading files in smaller, manageable chunks. This is particularly useful for memory-constrained environments or when processing streaming data.\nBatch processing is a technique for handling large datasets by dividing them into smaller, manageable chunks. This is particularly important for:\n\nMemory-constrained environments: When working with datasets that are too large to fit in memory, batch processing allows you to process the data incrementally.\nStreaming data: When data is continuously generated (e.g., from IoT devices or real-time applications), batch processing enables you to process data as it arrives.\nDistributed processing: In distributed computing environments, batch processing allows different nodes to work on different chunks of data simultaneously.\n\nThe batch_size parameter controls how many files or records are processed together in each batch. A smaller batch size reduces memory usage but may increase processing overhead, while a larger batch size improves throughput but requires more memory.\nStep-by-step walkthrough:\n\nCreating sample batched data: We generate sample data and distribute it across multiple files in each format (Parquet, CSV, JSON). Each file contains a subset of the total data, simulating a real-world scenario where data is split across multiple files.\nReading Parquet files in batches: Using fs.read_parquet(parquet_path, batch_size=2), we read all Parquet files in batches of 2 files at a time. Each iteration of the loop processes a batch of files, and the batch variable contains the combined data from those files.\nReading CSV files in batches: Similarly, we use fs.read_csv(csv_path, batch_size=2) to read CSV files in batches. The result is a Polars DataFrame for each batch, which we can process individually.\nReading JSON files in batches: Finally, we use fs.read_json(json_path, batch_size=2) to read JSON files in batches. The JSON data is automatically converted to Polars DataFrames for easy processing.\n\nBenefits of batch processing:\n\nReduced memory footprint: Instead of loading all files into memory at once, you only load the current batch.\nProgressive processing: You can start processing data as soon as the first batch is available, without waiting for all data to be loaded.\nFault tolerance: If processing fails on one batch, you can restart from that batch without reprocessing all previous batches.\nScalability: Batch processing scales well with both the size of the dataset and the available computational resources.\n\nThis approach is particularly valuable when working with large datasets in cloud storage, where downloading the entire dataset would be impractical due to network constraints or memory limitations.\n\n\n\nFirst, we’ll create a temporary directory with sample data files.\nThis step sets up the environment by creating a temporary directory and populating it with sample data files in batches.\nSetup steps:\n\nCreate a temporary directory for our example\nCreate sample batched data\n\nimport tempfile\nimport shutil\nimport os\nfrom examples.batch_processing.generate_batched_data import create_batched_dataset\n\ntemp_dir = tempfile.mkdtemp()\nprint(f\"Created temporary directory: {temp_dir}\")\n\ncreate_batched_dataset(temp_dir)\n\n\n\nNow, let’s read the Parquet files in batches.\nReading Parquet files in batches:\n\nInitialize filesystem\nSet up path pattern for Parquet files\nProcess files in batches of 2\n\nprint(\"\\n=== Parquet Batch Reading ===\")\nfrom fsspec_utils import filesystem\nfs = filesystem(\"file\")\nparquet_path = os.path.join(temp_dir, \"*.parquet\")\nprint(\"\\nReading Parquet files in batches (batch_size=2):\")\nfor i, batch in enumerate(fs.read_parquet(parquet_path, batch_size=2)):\n    print(f\"   Batch {i+1}: rows={batch.num_rows}\")\n    print(f\"   - Data preview: {batch.to_pandas().head(1).to_dict('records')}\")\n\n\n\nNext, we’ll read the CSV files in batches.\nReading CSV files in batches:\n\nSet up path pattern for CSV files\nProcess files in batches of 2\n\nprint(\"\\n=== CSV Batch Reading ===\")\ncsv_path = os.path.join(temp_dir, \"*.csv\")\nprint(\"\\nReading CSV files in batches (batch_size=2):\")\nfor i, batch in enumerate(fs.read_csv(csv_path, batch_size=2)):\n    print(f\"   Batch {i+1}: shape={batch.shape}\")\n    print(f\"   - Data preview: {batch.head(1).to_dicts()}\")\nSimilarly, we use read_csv with the same glob pattern to read all CSV files.\n\n\n\nFinally, let’s read the JSON files in batches.\nReading JSON files in batches:\n\nSet up path pattern for JSON files\nProcess files in batches of 2\nClean up the temporary directory\n\nprint(\"\\n=== JSON Batch Reading ===\")\njson_path = os.path.join(temp_dir, \"*.json\")\nprint(\"\\nReading JSON files in batches (batch_size=2):\")\nfor i, batch in enumerate(fs.read_json(json_path, batch_size=2)):\n    print(f\"   Batch {i+1}: shape={batch.shape}\")\n    print(f\"   - Data preview: {batch.head(1).to_dicts()}\")\n\nshutil.rmtree(temp_dir)\nprint(f\"\\nCleaned up temporary directory: {temp_dir}\")\nThe read_json method is also used with batch_size=2 to process JSON files in batches.\nThis example illustrates how to read Parquet, CSV, and JSON files in batches using the batch_size parameter. This approach allows for processing of large datasets without loading the entire dataset into memory at once.\n\n\n\n\nfsspec-utils enhances Parquet operations with deep integration with PyArrow and Pydala, enabling efficient dataset management, partitioning, and delta lake capabilities.\n\npyarrow_dataset: Create PyArrow datasets for optimized querying, partitioning, and predicate pushdown.\npyarrow_parquet_dataset: Specialized for Parquet, handling _metadata files for overall dataset schemas.\npydala_dataset: Integrates with pydala for advanced features like Delta Lake operations (upserts, schema evolution).\n\nExample: Writing to a PyArrow Dataset with Partitioning\nimport polars as pl\nfrom fsspec_utils.core import filesystem\n\nfs = filesystem(\"file\")\nbase_path = \"output/my_partitioned_data\"\n\n# Sample data\ndata = pl.DataFrame({\n    \"id\": [1, 2, 3, 4],\n    \"value\": [\"A\", \"B\", \"C\", \"D\"],\n    \"year\": [2023, 2023, 2024, 2024],\n    \"month\": [10, 11, 1, 2]\n})\n\n# Write data as a partitioned PyArrow dataset\nfs.write_pyarrow_dataset(\n    data=data,\n    path=base_path,\n    partition_by=[\"year\", \"month\"], # Partition by year and month\n    format=\"parquet\",\n    compression=\"zstd\",\n    mode=\"overwrite\" # Overwrite if path exists\n)\n\nprint(f\"Data written to {base_path} partitioned by year/month.\")\n# Expected structure: output/my_partitioned_data/year=2023/month=10/data-*.parquet\nExample: Delta Lake Operations with Pydala Dataset\nimport polars as pl\nfrom fsspec_utils.core import filesystem\n\nfs = filesystem(\"file\")\ndelta_path = \"output/my_delta_table\"\n\n# Initial data\ninitial_data = pl.DataFrame({\n    \"id\": [1, 2],\n    \"name\": [\"Alice\", \"Bob\"],\n    \"version\": [1, 1]\n})\n\n# Write initial data to a Pydala dataset\nfs.write_pydala_dataset(\n    data=initial_data,\n    path=delta_path,\n    mode=\"overwrite\"\n)\nprint(\"Initial Delta table created.\")\n\n# New data for an upsert: update Alice, add Charlie\nnew_data = pl.DataFrame({\n    \"id\": [1, 3],\n    \"name\": [\"Alicia\", \"Charlie\"],\n    \"version\": [2, 1]\n})\n\n# Perform a delta merge (upsert)\nfs.write_pydala_dataset(\n    data=new_data,\n    path=delta_path,\n    mode=\"delta\",\n    delta_subset=[\"id\"] # Column(s) to use for merging\n)\nprint(\"Delta merge completed.\")\n\n# Read the updated table\nupdated_df = fs.pydala_dataset(delta_path).to_polars()\nprint(\"Updated Delta table:\")\nprint(updated_df)\n# Expected: id=1 Alicia version=2, id=2 Bob version=1, id=3 Charlie version=1\nfsspec-utils facilitates integration with Delta Lake by providing StorageOptions that can be used to configure deltalake’s DeltaTable for various storage backends.\nThis example demonstrates how to use LocalStorageOptions with deltalake’s DeltaTable. It shows how to initialize a DeltaTable instance by passing the fsspec-utils storage options, enabling seamless interaction with Delta Lake tables across different storage types.\nStep-by-step walkthrough:\n\nCreate a temporary directory for our example\nCreate a simple Polars DataFrame\nWrite initial data to create the Delta table\nCreate a LocalStorageOptions object for the temporary directory\nCreate a DeltaTable instance, passing storage options\n\nNote: deltalake expects storage_options as a dict, which to_object_store_kwargs provides\n\nRead data from the DeltaTable\nClean up the temporary directory\n\nDelta Lake is an open-source storage layer that brings ACID transactions to Apache Spark and big data workloads. It provides a reliable, scalable, and performant way to work with data lakes, combining the benefits of data lakes (low cost, flexibility) with data warehouses (reliability, performance).\nfrom deltalake import DeltaTable\nfrom fsspec_utils.storage_options import LocalStorageOptions\nimport tempfile\nimport shutil\nimport os\nimport polars as pl\n\ntemp_dir = tempfile.mkdtemp()\nprint(f\"Created temporary directory: {temp_dir}\")\n\ndelta_table_path = os.path.join(temp_dir, \"my_delta_table\")\nprint(f\"Creating a dummy Delta table at: {delta_table_path}\")\n\ndata = pl.DataFrame({\n    \"id\": [1, 2, 3],\n    \"value\": [\"A\", \"B\", \"C\"]\n})\n\ndata.write_delta(delta_table_path, mode=\"overwrite\")\nprint(\"Initial data written to Delta table.\")\n\nlocal_options = LocalStorageOptions(path=temp_dir)\n\ndt = DeltaTable(delta_table_path, storage_options=local_options.to_object_store_kwargs())\nprint(f\"\\nSuccessfully created DeltaTable instance from: {delta_table_path}\")\nprint(f\"DeltaTable version: {dt.version()}\")\nprint(f\"DeltaTable files: {dt.files()}\")\n\ntable_data = dt.to_pyarrow_table()\nprint(\"\\nData read from DeltaTable:\")\nprint(table_data.to_pandas())\n\nshutil.rmtree(temp_dir)\nprint(f\"Cleaned up temporary directory: {temp_dir}\")\nKey features of Delta Lake:\n\nACID transactions: Ensures data integrity even with concurrent operations\nTime travel: Allows querying data as it existed at any point in time\nSchema enforcement: Maintains data consistency with schema validation\nScalable metadata: Handles billions of files efficiently\nUnified analytics: Supports both batch and streaming workloads\n\nIntegrating fsspec-utils with Delta Lake:\nThe fsspec-utils StorageOptions classes can be used to configure deltalake’s DeltaTable for various storage backends. This integration allows you to:\n\nUse consistent configuration patterns across different storage systems\nLeverage the benefits of fsspec’s unified filesystem interface\nSeamlessly switch between local and cloud storage without changing your Delta Lake code\n\nThe to_object_store_kwargs() method converts fsspec-utils storage options into a dictionary format that deltalake expects for its storage_options parameter. This is necessary because deltalake requires storage options as a dictionary, while fsspec-utils provides them as structured objects.\nStep-by-step walkthrough:\n\nCreating a temporary directory: We create a temporary directory to store our Delta table, ensuring the example is self-contained and doesn’t leave artifacts on your system.\nCreating sample data: We create a simple Polars DataFrame with sample data that will be written to our Delta table.\nWriting to Delta table: Using the write_delta method, we convert our DataFrame into a Delta table. This creates the necessary Delta Lake metadata alongside the data files.\nConfiguring storage options: We create a LocalStorageOptions object that points to our temporary directory. This object contains all the information needed to access the Delta table.\nInitializing DeltaTable: We create a DeltaTable instance by passing the table path and the storage options converted to a dictionary via to_object_store_kwargs(). This allows deltalake to locate and access the Delta table files.\nVerifying the DeltaTable: We check the version and files of our Delta table to confirm it was created correctly. Delta tables maintain version history, allowing you to track changes over time.\nReading data: Finally, we read the data from our Delta table back into a PyArrow Table, demonstrating that we can successfully interact with the Delta Lake table using the fsspec-utils configuration.\n\nThis integration is particularly valuable when working with Delta Lake in cloud environments, as it allows you to use the same configuration approach for local development and production deployments across different cloud providers.\n\n\n\n\nfsspec-utils provides a robust system for managing storage configurations, simplifying credential handling and environment setup.\n\n\nInstead of hardcoding credentials, you can load storage options directly from environment variables.\nExample: Loading AWS S3 Configuration from Environment\nSet these environment variables before running your script:\nexport AWS_ACCESS_KEY_ID=\"YOUR_ACCESS_KEY\"\nexport AWS_SECRET_ACCESS_KEY=\"YOUR_SECRET_KEY\"\nexport AWS_DEFAULT_REGION=\"us-west-2\"\nThen in Python:\nfrom fsspec_utils.storage_options import AwsStorageOptions\n\n# Load AWS options directly from environment variables\naws_opts = AwsStorageOptions.from_env()\nprint(f\"Loaded AWS region: {aws_opts.region}\")\n\n# Use it to create a filesystem\n# fs = aws_opts.to_filesystem()\n\n\n\nCombine multiple storage option configurations, useful for layering default settings with user-specific overrides.\nExample: Merging S3 Options\nfrom fsspec_utils.storage_options import AwsStorageOptions, merge_storage_options\n\n# Base configuration\nbase_opts = AwsStorageOptions(\n    protocol=\"s3\",\n    region=\"us-east-1\",\n    access_key_id=\"DEFAULT_KEY\"\n)\n\n# User-provided overrides\nuser_overrides = {\n    \"access_key_id\": \"USER_KEY\",\n    \"allow_http\": True # New setting\n}\n\n# Merge, with user_overrides taking precedence\nmerged_opts = merge_storage_options(base_opts, user_overrides, overwrite=True)\n\nprint(f\"Merged Access Key ID: {merged_opts.access_key_id}\") # USER_KEY\nprint(f\"Merged Region: {merged_opts.region}\") # us-east-1\nprint(f\"Allow HTTP: {merged_opts.allow_http}\") # True\n\n\n\nFor a comprehensive collection of executable examples demonstrating various functionalities and advanced patterns of fsspec-utils, including those discussed in this document, please refer to the examples directory on GitHub. Each example is designed to be runnable and provides detailed insights into practical usage.\n\n\n\n\n\nCaching: Always consider using cached=True with the filesystem function, especially for remote filesystems, to minimize repeated downloads.\nParallel Reading: For multiple files, set use_threads=True in read_json, read_csv, and read_parquet to leverage concurrent I/O.\nBatch Processing: When dealing with a very large number of files or extremely large individual files, use the batch_size parameter in reading functions to process data in chunks, reducing memory footprint.\nopt_dtypes: Utilize opt_dtypes=True in reading functions when converting to Polars or PyArrow to automatically optimize column data types, leading to more efficient memory usage and faster subsequent operations.\nParquet Datasets: For large, partitioned Parquet datasets, use pyarrow_dataset or pydala_dataset. These leverage PyArrow’s dataset API for efficient metadata handling, partition pruning, and predicate pushdown, reading only the necessary data.\nCompression: When writing Parquet files, choose an appropriate compression codec (e.g., zstd, snappy) to reduce file size and improve I/O performance. zstd often provides a good balance of compression ratio and speed.\n\n\n\n\nfsspec-utils simplifies configuring connections to various storage systems, including local filesystems, AWS S3, Azure Storage, and Google Cloud Storage, using StorageOptions classes. These options can then be converted into fsspec filesystems.\n\n\nThis example demonstrates how to initialize LocalStorageOptions and use it to interact with the local filesystem.\nStep-by-step walkthrough:\n\nCreate a temporary directory for our test\nCreate a test file and write content to it\nList files in the directory to verify our file was created\nRead the content back to verify it was written correctly\nClean up the temporary directory\n\nStorageOptions classes simplify configuration for different storage systems and provide a consistent interface for creating fsspec filesystem objects.\nimport os\nimport tempfile\nimport shutil\nfrom fsspec_utils.storage_options import LocalStorageOptions\n\nprint(\"=== LocalStorageOptions Example ===\\n\")\n\nlocal_options = LocalStorageOptions(auto_mkdir=True)\nlocal_fs = local_options.to_filesystem()\n\ntemp_dir = tempfile.mkdtemp()\nprint(f\"Working in temporary directory: {temp_dir}\")\n\ntemp_file = os.path.join(temp_dir, \"test_file.txt\")\nwith local_fs.open(temp_file, \"w\") as f:\n    f.write(\"Hello, LocalStorageOptions!\")\nprint(f\"Created test file: {temp_file}\")\n\nfiles = local_fs.ls(temp_dir)\nprint(f\"Files in {temp_dir}: {[os.path.basename(f) for f in files]}\")\n\nwith local_fs.open(temp_file, \"r\") as f:\n    content = f.read()\nprint(f\"File content: '{content}'\")\n\nshutil.rmtree(temp_dir)\nprint(f\"Cleaned up temporary directory: {temp_dir}\")\nprint(\"Local storage example completed.\\n\")\n\n\n\nThis example demonstrates the configuration pattern for AwsStorageOptions. It is expected to fail when attempting to connect to actual cloud services because it uses dummy credentials.\nNote: The to_filesystem() method converts StorageOptions into fsspec-compatible objects, allowing seamless integration with any fsspec-compatible library.\nfrom fsspec_utils.storage_options import AwsStorageOptions\n\nprint(\"=== Conceptual AwsStorageOptions Example (using a dummy endpoint) ===\\n\")\n\naws_options = AwsStorageOptions(\n    endpoint_url=\"http://s3.dummy-endpoint.com\",\n    access_key_id=\"DUMMY_KEY\",\n    secret_access_key=\"DUMMY_SECRET\",\n    allow_http=True,\n    region=\"us-east-1\"\n)\n\naws_fs = aws_options.to_filesystem()\nprint(f\"Created fsspec filesystem for S3: {type(aws_fs).__name__}\")\nprint(\"AWS storage example completed.\\n\")\n\n\n\nThis example shows how to configure AzureStorageOptions. It is expected to fail when attempting to connect to actual cloud services because it uses dummy credentials.\nfrom fsspec_utils.storage_options import AzureStorageOptions\n\nprint(\"=== Conceptual AzureStorageOptions Example (using a dummy connection string) ===\\n\")\nazure_options = AzureStorageOptions(\n    protocol=\"az\",\n    account_name=\"demoaccount\",\n    connection_string=\"DefaultEndpointsProtocol=https;AccountName=demoaccount;AccountKey=demokey==;EndpointSuffix=core.windows.net\"\n)\n\nazure_fs = azure_options.to_filesystem()\nprint(f\"Created fsspec filesystem for Azure: {type(azure_fs).__name__}\")\nprint(\"Azure storage example completed.\\n\")\n\n\n\nThis example shows how to configure GcsStorageOptions. It is expected to fail when attempting to connect to actual cloud services because it uses dummy credentials.\nStorageOptions classes provide a simplified, consistent interface for configuring connections to various storage systems. They abstract away the complexity of different storage backends and provide a unified way to create fsspec filesystem objects.\nThe to_filesystem() method converts these options into fsspec compatible objects, enabling seamless integration with any fsspec-compatible library or tool.\nImportant Note: The AWS, Azure, and GCS examples use dummy credentials and are for illustrative purposes only. These examples are expected to fail when attempting to connect to actual cloud services because:\n\nThe endpoint URLs are not real service endpoints\nThe credentials are placeholder values that don’t correspond to actual accounts\nThe connection strings and tokens are examples, not valid credentials\n\nThis approach allows you to understand the configuration pattern without needing actual cloud credentials. When using these examples in production, you would replace the dummy values with your real credentials and service endpoints.\n```python from fsspec_utils.storage_options import GcsStorageOptions\nprint(“=== Conceptual GcsStorageOptions Example (using a dummy token path) ===”) gcs_options = GcsStorageOptions( protocol=“gs”, project=“demo-project”, token=“path/to/dummy-service-account.json” )\ngcs_fs = gcs_options.to_filesystem() print(f”Created fsspec filesystem for GCS: {type(gcs_fs).__name__}“) print(”GCS storage example completed.“)"
  },
  {
    "objectID": "advanced.html#unified-filesystem-creation-with-filesystem",
    "href": "advanced.html#unified-filesystem-creation-with-filesystem",
    "title": "Advanced Usage",
    "section": "",
    "text": "The fsspec_utils.core.filesystem function offers a centralized and enhanced way to instantiate fsspec filesystem objects. It supports:\n\nIntelligent Caching: Automatically wraps filesystems with MonitoredSimpleCacheFileSystem for improved performance and verbose logging of cache operations.\nStructured Storage Options: Integrates seamlessly with fsspec_utils.storage_options classes, allowing for type-safe and organized configuration of cloud and Git-based storage.\nProtocol Inference: Can infer the filesystem protocol directly from a URI or path, reducing boilerplate.\n\nExample: Cached S3 Filesystem with Structured Options\nfrom fsspec_utils.core import filesystem\nfrom fsspec_utils.storage_options import AwsStorageOptions\n\n# Configure S3 options using the structured class\ns3_opts = AwsStorageOptions(\n    region=\"us-east-1\",\n    access_key_id=\"YOUR_ACCESS_KEY\",\n    secret_access_key=\"YOUR_SECRET_KEY\"\n)\n\n# Create a cached S3 filesystem using the 'filesystem' helper\nfs = filesystem(\n    \"s3\",\n    storage_options=s3_opts,\n    cached=True,\n    cache_storage=\"/tmp/s3_cache\", # Optional: specify cache directory\n    verbose=True # Enable verbose cache logging\n)\n\n# Use the filesystem as usual\nprint(fs.ls(\"s3://your-bucket/\"))\n\n\nfsspec-utils provides an enhanced caching mechanism that improves performance for repeated file operations, especially useful for remote filesystems.\nThis example demonstrates how caching improves read performance. The first read populates the cache, while subsequent reads retrieve data directly from the cache, significantly reducing access time. It also shows that data can still be retrieved from the cache even if the original source becomes unavailable.\nCaching in fsspec-utils is an enhanced mechanism that improves performance for repeated file operations, especially useful for remote filesystems where network latency can significantly impact performance.\nThe filesystem() function provides several parameters for configuring caching:\n\ncached: When set to True, enables caching for all read operations\ncache_storage: Specifies the directory where cached files will be stored\nverbose: When set to True, provides detailed logging about cache operations\n\nStep-by-step walkthrough:\n\nFirst read (populating cache): When reading a file for the first time, the data is retrieved from the source (disk, network, etc.) and stored in the cache directory. This takes longer than subsequent reads because it involves both reading from the source and writing to the cache.\nSecond read (using cache): When the same file is read again, the data is retrieved directly from the cache instead of the source. This is significantly faster because it avoids network latency or disk I/O.\nDemonstrating cache effectiveness: Even after the original file is removed, the cached version can still be accessed. This demonstrates that the cache acts as a persistent copy of the data, independent of the source file.\nPerformance comparison: The timing results clearly show the performance benefits of caching, with subsequent reads being orders of magnitude faster than the initial read.\n\nThis caching mechanism is particularly valuable when working with:\n\nRemote filesystems (S3, GCS, Azure) where network latency is a bottleneck\nFrequently accessed files that don’t change often\nApplications that read the same data multiple times\nEnvironments with unreliable network connections\n\n\n\nIn this step, we create a sample JSON file and initialize the fsspec-utils filesystem with caching enabled. The first read operation retrieves data from the source and populates the cache.\nSetup steps:\n\nCreate a temporary directory for our example\nCreate sample data file\nConfigure filesystem with caching\n\nimport tempfile\nimport time\nimport os\nimport shutil\nfrom fsspec_utils import filesystem\nfrom examples.caching.setup_data import create_sample_data_file\n\ntmpdir = tempfile.mkdtemp()\nprint(f\"Created temporary directory: {tmpdir}\")\n\nsample_file = create_sample_data_file(tmpdir)\n\ncache_dir = os.path.join(tmpdir, \"cache\")\nfs = filesystem(\n    protocol_or_path=\"file\",\n    cached=True,\n    cache_storage=cache_dir,\n    verbose=True\n)\n\nprint(\"\\n=== First read (populating cache) ===\")\nstart_time = time.time()\ndata1 = fs.read_json(sample_file)\nfirst_read_time = time.time() - start_time\nprint(f\"First read completed in {first_read_time:.4f} seconds\")\n\n\n\nNow, let’s read the same file again to see the performance improvement from using the cache.\nprint(\"\\n=== Second read (using cache) ===\")\nstart_time = time.time()\ndata2 = fs.read_json(sample_file)\nsecond_read_time = time.time() - start_time\nprint(f\"Second read completed in {second_read_time:.4f} seconds\")\nThe second read retrieves data directly from the cache, which is significantly faster than reading from the source again.\n\n\n\nTo demonstrate that the cache is persistent, we’ll remove the original file and try to read it again.\nprint(\"\\n=== Demonstrating cache effectiveness ===\")\nprint(\"Removing original file...\")\nos.remove(sample_file)\nprint(f\"Original file exists: {os.path.exists(sample_file)}\")\n\nprint(\"\\n=== Third read (from cache only) ===\")\nstart_time = time.time()\ndata3 = fs.read_json(sample_file)\nthird_read_time = time.time() - start_time\nprint(f\"Third read completed in {third_read_time:.4f} seconds\")\nprint(\"✓ Successfully read from cache even after original file was removed\")\n\nprint(\"\\n=== Performance Comparison ===\")\nprint(f\"First read (from disk): {first_read_time:.4f} seconds\")\nprint(f\"Second read (from cache): {second_read_time:.4f} seconds\")\nprint(f\"Third read (from cache): {third_read_time:.4f} seconds\")\n\nshutil.rmtree(tmpdir)\nprint(f\"Cleaned up temporary directory: {tmpdir}\")\nThis step proves that the cache acts as a persistent copy of the data, allowing access even if the original source is unavailable."
  },
  {
    "objectID": "advanced.html#custom-filesystem-implementations",
    "href": "advanced.html#custom-filesystem-implementations",
    "title": "Advanced Usage",
    "section": "",
    "text": "fsspec-utils provides specialized filesystem implementations for unique use cases:\n\n\nAccess files directly from GitLab repositories. This is particularly useful for configuration files, datasets, or code stored in private or public GitLab instances.\nExample: Reading from a GitLab Repository\nfrom fsspec_utils.core import filesystem\n\n# Instantiate a GitLab filesystem\ngitlab_fs = filesystem(\n    \"gitlab\",\n    storage_options={\n        \"project_name\": \"your-group/your-project\", # Or \"project_id\": 12345\n        \"ref\": \"main\", # Branch, tag, or commit SHA\n        \"token\": \"glpat_YOUR_PRIVATE_TOKEN\" # Required for private repos\n    }\n)\n\n# List files in the repository root\nprint(gitlab_fs.ls(\"/\"))\n\n# Read a specific file\ncontent = gitlab_fs.cat(\"README.md\").decode(\"utf-8\")\nprint(content[:200]) # Print first 200 characters"
  },
  {
    "objectID": "advanced.html#advanced-data-reading-and-writing-read_files-write_files",
    "href": "advanced.html#advanced-data-reading-and-writing-read_files-write_files",
    "title": "Advanced Usage",
    "section": "",
    "text": "The fsspec_utils.core.ext module (exposed via AbstractFileSystem extensions) provides powerful functions for reading and writing various data formats (JSON, CSV, Parquet) with advanced features like:\n\nBatch Processing: Efficiently handle large datasets by processing files in configurable batches.\nParallel Processing: Leverage multi-threading to speed up file I/O operations.\nSchema Unification & Optimization: Automatically unifies schemas when concatenating multiple files and optimizes data types for memory efficiency (e.g., using Polars’ opt_dtypes or PyArrow’s schema casting).\nFile Path Tracking: Optionally include the source file path as a column in the resulting DataFrame/Table.\n\n\n\nThe read_files function acts as a universal reader, delegating to format-specific readers (JSON, CSV, Parquet) while maintaining consistent options.\nExample: Reading CSVs in Batches with Parallelism\nfrom fsspec_utils.core import filesystem\n\n# Assuming you have multiple CSV files like 'data/part_0.csv', 'data/part_1.csv', etc.\n# on your local filesystem\nfs = filesystem(\"file\")\n\n# Read CSV files in batches of 10, using multiple threads, and including file path\nfor batch_df in fs.read_files(\n    \"data/*.csv\",\n    format=\"csv\",\n    batch_size=10,\n    include_file_path=True,\n    use_threads=True,\n    verbose=True\n):\n    print(f\"Processed batch with {len(batch_df)} rows. Columns: {batch_df.columns}\")\n    print(batch_df.head(2))\n\n\n\nfsspec-utils simplifies reading multiple files of various formats (Parquet, CSV, JSON) from a folder into a single PyArrow Table or Polars DataFrame.\nReading multiple files into a single table is a powerful feature that allows you to efficiently process data distributed across multiple files. This is particularly useful when dealing with large datasets that are split into smaller files for better organization or parallel processing.\nKey concepts demonstrated:\n\nGlob patterns: The **/*.parquet, **/*.csv, and **/*.json patterns are used to select files recursively from the directory and its subdirectories. The ** pattern matches any directories, allowing the function to find files in nested directories.\nConcat parameter: The concat=True parameter tells the function to combine data from multiple files into a single table or DataFrame. When set to False, the function would return a list of individual tables/DataFrames.\nFormat flexibility: The same interface can be used to read different file formats (Parquet, CSV, JSON), making it easy to work with heterogeneous data sources.\n\nStep-by-step explanation:\n\nCreating sample data: We create two subdirectories and populate them with sample data in three different formats (Parquet, CSV, JSON). Each format contains the same structured data but in different serialization formats.\nReading Parquet files: Using fs.read_parquet(\"**/*.parquet\", concat=True), we read all Parquet files recursively and combine them into a single PyArrow Table. Parquet is a columnar storage format that is highly efficient for analytical workloads.\nReading CSV files: Using fs.read_csv(\"**/*.csv\", concat=True), we read all CSV files and combine them into a Polars DataFrame, which we then convert to a PyArrow Table for consistency.\nReading JSON files: Using fs.read_json(\"**/*.json\", as_dataframe=True, concat=True), we read all JSON files and combine them into a Polars DataFrame, then convert it to a PyArrow Table.\nVerification: Finally, we verify that all three tables have the same number of rows, confirming that the data was correctly read and combined across all files and formats.\n\nThe flexibility of fsspec-utils allows you to use the same approach with different data sources, including remote filesystems like S3, GCS, or Azure Blob Storage, simply by changing the filesystem path.\n\n\nFirst, we’ll create a temporary directory with sample data in different formats.\nSetup steps:\n\nCreate a temporary directory for our example\nCreate sample data in subdirectories\n\nimport tempfile\nimport shutil\nimport os\nfrom examples.read_folder.create_dataset import create_sample_dataset\n\ntemp_dir = tempfile.mkdtemp()\nprint(f\"Created temporary directory: {temp_dir}\")\n\ncreate_sample_dataset(temp_dir)\nThis step sets up the environment by creating a temporary directory and populating it with sample data files.\n\n\n\nNow, let’s read all the Parquet files from the directory and its subdirectories into a single PyArrow Table.\nReading Parquet files:\n\nRead Parquet files using glob pattern\nDisplay table information and sample data\n\nprint(\"\\n=== Reading Parquet Files ===\")\nfrom fsspec_utils import filesystem\nfs = filesystem(temp_dir)\nparquet_table = fs.read_parquet(\"**/*.parquet\", concat=True)\nprint(f\"Successfully read Parquet files into PyArrow Table\")\nprint(f\"Table shape: {parquet_table.num_rows} rows x {parquet_table.num_columns} columns\")\nprint(\"First 3 rows:\")\nprint(parquet_table.slice(0, 3).to_pandas())\nWe use the read_parquet method with a glob pattern **/*.parquet to find all Parquet files recursively. The concat=True parameter combines them into a single table.\n\n\n\nNext, we’ll read all the CSV files into a Polars DataFrame and then convert it to a PyArrow Table.\nReading CSV files:\n\nRead CSV files using glob pattern\nDisplay DataFrame information and sample data\nConvert to PyArrow Table for consistency\n\nprint(\"\\n=== Reading CSV Files ===\")\ncsv_df = fs.read_csv(\"**/*.csv\", concat=True)\nprint(f\"Successfully read CSV files into Polars DataFrame\")\nprint(f\"DataFrame shape: {csv_df.shape}\")\nprint(\"First 3 rows:\")\nprint(csv_df.head(3))\ncsv_table = csv_df.to_arrow()\nSimilarly, we use read_csv with the same glob pattern to read all CSV files.\n\n\n\nFinally, let’s read all the JSON files.\nReading JSON files:\n\nRead JSON files using glob pattern\nDisplay DataFrame information and sample data\nConvert to PyArrow Table for consistency\n\nprint(\"\\n=== Reading JSON Files ===\")\njson_df = fs.read_json(\"**/*.json\", as_dataframe=True, concat=True)\nprint(f\"Successfully read JSON files into Polars DataFrame\")\nprint(f\"DataFrame shape: {json_df.shape}\")\nprint(\"First 3 rows:\")\nprint(json_df.head(3))\njson_table = json_df.to_arrow()\nThe read_json method is used to read all JSON files. We set as_dataframe=True to get a Polars DataFrame.\n\n\n\nLet’s verify that all the tables have the same number of rows.\nprint(\"\\n=== Verification ===\")\nprint(f\"All tables have the same number of rows: {parquet_table.num_rows == csv_table.num_rows == json_table.num_rows}\")\n\nshutil.rmtree(temp_dir)\nprint(f\"\\nCleaned up temporary directory: {temp_dir}\")\nThis final step confirms that our data reading and concatenation were successful.\nThis example shows how to read various file formats from a directory, including subdirectories, into a unified PyArrow Table or Polars DataFrame. It highlights the flexibility of fsspec-utils in handling different data sources and formats.\nfsspec-utils enables efficient batch processing of large datasets by reading files in smaller, manageable chunks. This is particularly useful for memory-constrained environments or when processing streaming data.\nBatch processing is a technique for handling large datasets by dividing them into smaller, manageable chunks. This is particularly important for:\n\nMemory-constrained environments: When working with datasets that are too large to fit in memory, batch processing allows you to process the data incrementally.\nStreaming data: When data is continuously generated (e.g., from IoT devices or real-time applications), batch processing enables you to process data as it arrives.\nDistributed processing: In distributed computing environments, batch processing allows different nodes to work on different chunks of data simultaneously.\n\nThe batch_size parameter controls how many files or records are processed together in each batch. A smaller batch size reduces memory usage but may increase processing overhead, while a larger batch size improves throughput but requires more memory.\nStep-by-step walkthrough:\n\nCreating sample batched data: We generate sample data and distribute it across multiple files in each format (Parquet, CSV, JSON). Each file contains a subset of the total data, simulating a real-world scenario where data is split across multiple files.\nReading Parquet files in batches: Using fs.read_parquet(parquet_path, batch_size=2), we read all Parquet files in batches of 2 files at a time. Each iteration of the loop processes a batch of files, and the batch variable contains the combined data from those files.\nReading CSV files in batches: Similarly, we use fs.read_csv(csv_path, batch_size=2) to read CSV files in batches. The result is a Polars DataFrame for each batch, which we can process individually.\nReading JSON files in batches: Finally, we use fs.read_json(json_path, batch_size=2) to read JSON files in batches. The JSON data is automatically converted to Polars DataFrames for easy processing.\n\nBenefits of batch processing:\n\nReduced memory footprint: Instead of loading all files into memory at once, you only load the current batch.\nProgressive processing: You can start processing data as soon as the first batch is available, without waiting for all data to be loaded.\nFault tolerance: If processing fails on one batch, you can restart from that batch without reprocessing all previous batches.\nScalability: Batch processing scales well with both the size of the dataset and the available computational resources.\n\nThis approach is particularly valuable when working with large datasets in cloud storage, where downloading the entire dataset would be impractical due to network constraints or memory limitations.\n\n\n\nFirst, we’ll create a temporary directory with sample data files.\nThis step sets up the environment by creating a temporary directory and populating it with sample data files in batches.\nSetup steps:\n\nCreate a temporary directory for our example\nCreate sample batched data\n\nimport tempfile\nimport shutil\nimport os\nfrom examples.batch_processing.generate_batched_data import create_batched_dataset\n\ntemp_dir = tempfile.mkdtemp()\nprint(f\"Created temporary directory: {temp_dir}\")\n\ncreate_batched_dataset(temp_dir)\n\n\n\nNow, let’s read the Parquet files in batches.\nReading Parquet files in batches:\n\nInitialize filesystem\nSet up path pattern for Parquet files\nProcess files in batches of 2\n\nprint(\"\\n=== Parquet Batch Reading ===\")\nfrom fsspec_utils import filesystem\nfs = filesystem(\"file\")\nparquet_path = os.path.join(temp_dir, \"*.parquet\")\nprint(\"\\nReading Parquet files in batches (batch_size=2):\")\nfor i, batch in enumerate(fs.read_parquet(parquet_path, batch_size=2)):\n    print(f\"   Batch {i+1}: rows={batch.num_rows}\")\n    print(f\"   - Data preview: {batch.to_pandas().head(1).to_dict('records')}\")\n\n\n\nNext, we’ll read the CSV files in batches.\nReading CSV files in batches:\n\nSet up path pattern for CSV files\nProcess files in batches of 2\n\nprint(\"\\n=== CSV Batch Reading ===\")\ncsv_path = os.path.join(temp_dir, \"*.csv\")\nprint(\"\\nReading CSV files in batches (batch_size=2):\")\nfor i, batch in enumerate(fs.read_csv(csv_path, batch_size=2)):\n    print(f\"   Batch {i+1}: shape={batch.shape}\")\n    print(f\"   - Data preview: {batch.head(1).to_dicts()}\")\nSimilarly, we use read_csv with the same glob pattern to read all CSV files.\n\n\n\nFinally, let’s read the JSON files in batches.\nReading JSON files in batches:\n\nSet up path pattern for JSON files\nProcess files in batches of 2\nClean up the temporary directory\n\nprint(\"\\n=== JSON Batch Reading ===\")\njson_path = os.path.join(temp_dir, \"*.json\")\nprint(\"\\nReading JSON files in batches (batch_size=2):\")\nfor i, batch in enumerate(fs.read_json(json_path, batch_size=2)):\n    print(f\"   Batch {i+1}: shape={batch.shape}\")\n    print(f\"   - Data preview: {batch.head(1).to_dicts()}\")\n\nshutil.rmtree(temp_dir)\nprint(f\"\\nCleaned up temporary directory: {temp_dir}\")\nThe read_json method is also used with batch_size=2 to process JSON files in batches.\nThis example illustrates how to read Parquet, CSV, and JSON files in batches using the batch_size parameter. This approach allows for processing of large datasets without loading the entire dataset into memory at once.\n\n\n\n\nfsspec-utils enhances Parquet operations with deep integration with PyArrow and Pydala, enabling efficient dataset management, partitioning, and delta lake capabilities.\n\npyarrow_dataset: Create PyArrow datasets for optimized querying, partitioning, and predicate pushdown.\npyarrow_parquet_dataset: Specialized for Parquet, handling _metadata files for overall dataset schemas.\npydala_dataset: Integrates with pydala for advanced features like Delta Lake operations (upserts, schema evolution).\n\nExample: Writing to a PyArrow Dataset with Partitioning\nimport polars as pl\nfrom fsspec_utils.core import filesystem\n\nfs = filesystem(\"file\")\nbase_path = \"output/my_partitioned_data\"\n\n# Sample data\ndata = pl.DataFrame({\n    \"id\": [1, 2, 3, 4],\n    \"value\": [\"A\", \"B\", \"C\", \"D\"],\n    \"year\": [2023, 2023, 2024, 2024],\n    \"month\": [10, 11, 1, 2]\n})\n\n# Write data as a partitioned PyArrow dataset\nfs.write_pyarrow_dataset(\n    data=data,\n    path=base_path,\n    partition_by=[\"year\", \"month\"], # Partition by year and month\n    format=\"parquet\",\n    compression=\"zstd\",\n    mode=\"overwrite\" # Overwrite if path exists\n)\n\nprint(f\"Data written to {base_path} partitioned by year/month.\")\n# Expected structure: output/my_partitioned_data/year=2023/month=10/data-*.parquet\nExample: Delta Lake Operations with Pydala Dataset\nimport polars as pl\nfrom fsspec_utils.core import filesystem\n\nfs = filesystem(\"file\")\ndelta_path = \"output/my_delta_table\"\n\n# Initial data\ninitial_data = pl.DataFrame({\n    \"id\": [1, 2],\n    \"name\": [\"Alice\", \"Bob\"],\n    \"version\": [1, 1]\n})\n\n# Write initial data to a Pydala dataset\nfs.write_pydala_dataset(\n    data=initial_data,\n    path=delta_path,\n    mode=\"overwrite\"\n)\nprint(\"Initial Delta table created.\")\n\n# New data for an upsert: update Alice, add Charlie\nnew_data = pl.DataFrame({\n    \"id\": [1, 3],\n    \"name\": [\"Alicia\", \"Charlie\"],\n    \"version\": [2, 1]\n})\n\n# Perform a delta merge (upsert)\nfs.write_pydala_dataset(\n    data=new_data,\n    path=delta_path,\n    mode=\"delta\",\n    delta_subset=[\"id\"] # Column(s) to use for merging\n)\nprint(\"Delta merge completed.\")\n\n# Read the updated table\nupdated_df = fs.pydala_dataset(delta_path).to_polars()\nprint(\"Updated Delta table:\")\nprint(updated_df)\n# Expected: id=1 Alicia version=2, id=2 Bob version=1, id=3 Charlie version=1\nfsspec-utils facilitates integration with Delta Lake by providing StorageOptions that can be used to configure deltalake’s DeltaTable for various storage backends.\nThis example demonstrates how to use LocalStorageOptions with deltalake’s DeltaTable. It shows how to initialize a DeltaTable instance by passing the fsspec-utils storage options, enabling seamless interaction with Delta Lake tables across different storage types.\nStep-by-step walkthrough:\n\nCreate a temporary directory for our example\nCreate a simple Polars DataFrame\nWrite initial data to create the Delta table\nCreate a LocalStorageOptions object for the temporary directory\nCreate a DeltaTable instance, passing storage options\n\nNote: deltalake expects storage_options as a dict, which to_object_store_kwargs provides\n\nRead data from the DeltaTable\nClean up the temporary directory\n\nDelta Lake is an open-source storage layer that brings ACID transactions to Apache Spark and big data workloads. It provides a reliable, scalable, and performant way to work with data lakes, combining the benefits of data lakes (low cost, flexibility) with data warehouses (reliability, performance).\nfrom deltalake import DeltaTable\nfrom fsspec_utils.storage_options import LocalStorageOptions\nimport tempfile\nimport shutil\nimport os\nimport polars as pl\n\ntemp_dir = tempfile.mkdtemp()\nprint(f\"Created temporary directory: {temp_dir}\")\n\ndelta_table_path = os.path.join(temp_dir, \"my_delta_table\")\nprint(f\"Creating a dummy Delta table at: {delta_table_path}\")\n\ndata = pl.DataFrame({\n    \"id\": [1, 2, 3],\n    \"value\": [\"A\", \"B\", \"C\"]\n})\n\ndata.write_delta(delta_table_path, mode=\"overwrite\")\nprint(\"Initial data written to Delta table.\")\n\nlocal_options = LocalStorageOptions(path=temp_dir)\n\ndt = DeltaTable(delta_table_path, storage_options=local_options.to_object_store_kwargs())\nprint(f\"\\nSuccessfully created DeltaTable instance from: {delta_table_path}\")\nprint(f\"DeltaTable version: {dt.version()}\")\nprint(f\"DeltaTable files: {dt.files()}\")\n\ntable_data = dt.to_pyarrow_table()\nprint(\"\\nData read from DeltaTable:\")\nprint(table_data.to_pandas())\n\nshutil.rmtree(temp_dir)\nprint(f\"Cleaned up temporary directory: {temp_dir}\")\nKey features of Delta Lake:\n\nACID transactions: Ensures data integrity even with concurrent operations\nTime travel: Allows querying data as it existed at any point in time\nSchema enforcement: Maintains data consistency with schema validation\nScalable metadata: Handles billions of files efficiently\nUnified analytics: Supports both batch and streaming workloads\n\nIntegrating fsspec-utils with Delta Lake:\nThe fsspec-utils StorageOptions classes can be used to configure deltalake’s DeltaTable for various storage backends. This integration allows you to:\n\nUse consistent configuration patterns across different storage systems\nLeverage the benefits of fsspec’s unified filesystem interface\nSeamlessly switch between local and cloud storage without changing your Delta Lake code\n\nThe to_object_store_kwargs() method converts fsspec-utils storage options into a dictionary format that deltalake expects for its storage_options parameter. This is necessary because deltalake requires storage options as a dictionary, while fsspec-utils provides them as structured objects.\nStep-by-step walkthrough:\n\nCreating a temporary directory: We create a temporary directory to store our Delta table, ensuring the example is self-contained and doesn’t leave artifacts on your system.\nCreating sample data: We create a simple Polars DataFrame with sample data that will be written to our Delta table.\nWriting to Delta table: Using the write_delta method, we convert our DataFrame into a Delta table. This creates the necessary Delta Lake metadata alongside the data files.\nConfiguring storage options: We create a LocalStorageOptions object that points to our temporary directory. This object contains all the information needed to access the Delta table.\nInitializing DeltaTable: We create a DeltaTable instance by passing the table path and the storage options converted to a dictionary via to_object_store_kwargs(). This allows deltalake to locate and access the Delta table files.\nVerifying the DeltaTable: We check the version and files of our Delta table to confirm it was created correctly. Delta tables maintain version history, allowing you to track changes over time.\nReading data: Finally, we read the data from our Delta table back into a PyArrow Table, demonstrating that we can successfully interact with the Delta Lake table using the fsspec-utils configuration.\n\nThis integration is particularly valuable when working with Delta Lake in cloud environments, as it allows you to use the same configuration approach for local development and production deployments across different cloud providers."
  },
  {
    "objectID": "advanced.html#storage-options-management",
    "href": "advanced.html#storage-options-management",
    "title": "Advanced Usage",
    "section": "",
    "text": "fsspec-utils provides a robust system for managing storage configurations, simplifying credential handling and environment setup.\n\n\nInstead of hardcoding credentials, you can load storage options directly from environment variables.\nExample: Loading AWS S3 Configuration from Environment\nSet these environment variables before running your script:\nexport AWS_ACCESS_KEY_ID=\"YOUR_ACCESS_KEY\"\nexport AWS_SECRET_ACCESS_KEY=\"YOUR_SECRET_KEY\"\nexport AWS_DEFAULT_REGION=\"us-west-2\"\nThen in Python:\nfrom fsspec_utils.storage_options import AwsStorageOptions\n\n# Load AWS options directly from environment variables\naws_opts = AwsStorageOptions.from_env()\nprint(f\"Loaded AWS region: {aws_opts.region}\")\n\n# Use it to create a filesystem\n# fs = aws_opts.to_filesystem()\n\n\n\nCombine multiple storage option configurations, useful for layering default settings with user-specific overrides.\nExample: Merging S3 Options\nfrom fsspec_utils.storage_options import AwsStorageOptions, merge_storage_options\n\n# Base configuration\nbase_opts = AwsStorageOptions(\n    protocol=\"s3\",\n    region=\"us-east-1\",\n    access_key_id=\"DEFAULT_KEY\"\n)\n\n# User-provided overrides\nuser_overrides = {\n    \"access_key_id\": \"USER_KEY\",\n    \"allow_http\": True # New setting\n}\n\n# Merge, with user_overrides taking precedence\nmerged_opts = merge_storage_options(base_opts, user_overrides, overwrite=True)\n\nprint(f\"Merged Access Key ID: {merged_opts.access_key_id}\") # USER_KEY\nprint(f\"Merged Region: {merged_opts.region}\") # us-east-1\nprint(f\"Allow HTTP: {merged_opts.allow_http}\") # True\n\n\n\nFor a comprehensive collection of executable examples demonstrating various functionalities and advanced patterns of fsspec-utils, including those discussed in this document, please refer to the examples directory on GitHub. Each example is designed to be runnable and provides detailed insights into practical usage."
  },
  {
    "objectID": "advanced.html#performance-tips",
    "href": "advanced.html#performance-tips",
    "title": "Advanced Usage",
    "section": "",
    "text": "Caching: Always consider using cached=True with the filesystem function, especially for remote filesystems, to minimize repeated downloads.\nParallel Reading: For multiple files, set use_threads=True in read_json, read_csv, and read_parquet to leverage concurrent I/O.\nBatch Processing: When dealing with a very large number of files or extremely large individual files, use the batch_size parameter in reading functions to process data in chunks, reducing memory footprint.\nopt_dtypes: Utilize opt_dtypes=True in reading functions when converting to Polars or PyArrow to automatically optimize column data types, leading to more efficient memory usage and faster subsequent operations.\nParquet Datasets: For large, partitioned Parquet datasets, use pyarrow_dataset or pydala_dataset. These leverage PyArrow’s dataset API for efficient metadata handling, partition pruning, and predicate pushdown, reading only the necessary data.\nCompression: When writing Parquet files, choose an appropriate compression codec (e.g., zstd, snappy) to reduce file size and improve I/O performance. zstd often provides a good balance of compression ratio and speed."
  },
  {
    "objectID": "advanced.html#flexible-storage-configuration",
    "href": "advanced.html#flexible-storage-configuration",
    "title": "Advanced Usage",
    "section": "",
    "text": "fsspec-utils simplifies configuring connections to various storage systems, including local filesystems, AWS S3, Azure Storage, and Google Cloud Storage, using StorageOptions classes. These options can then be converted into fsspec filesystems.\n\n\nThis example demonstrates how to initialize LocalStorageOptions and use it to interact with the local filesystem.\nStep-by-step walkthrough:\n\nCreate a temporary directory for our test\nCreate a test file and write content to it\nList files in the directory to verify our file was created\nRead the content back to verify it was written correctly\nClean up the temporary directory\n\nStorageOptions classes simplify configuration for different storage systems and provide a consistent interface for creating fsspec filesystem objects.\nimport os\nimport tempfile\nimport shutil\nfrom fsspec_utils.storage_options import LocalStorageOptions\n\nprint(\"=== LocalStorageOptions Example ===\\n\")\n\nlocal_options = LocalStorageOptions(auto_mkdir=True)\nlocal_fs = local_options.to_filesystem()\n\ntemp_dir = tempfile.mkdtemp()\nprint(f\"Working in temporary directory: {temp_dir}\")\n\ntemp_file = os.path.join(temp_dir, \"test_file.txt\")\nwith local_fs.open(temp_file, \"w\") as f:\n    f.write(\"Hello, LocalStorageOptions!\")\nprint(f\"Created test file: {temp_file}\")\n\nfiles = local_fs.ls(temp_dir)\nprint(f\"Files in {temp_dir}: {[os.path.basename(f) for f in files]}\")\n\nwith local_fs.open(temp_file, \"r\") as f:\n    content = f.read()\nprint(f\"File content: '{content}'\")\n\nshutil.rmtree(temp_dir)\nprint(f\"Cleaned up temporary directory: {temp_dir}\")\nprint(\"Local storage example completed.\\n\")\n\n\n\nThis example demonstrates the configuration pattern for AwsStorageOptions. It is expected to fail when attempting to connect to actual cloud services because it uses dummy credentials.\nNote: The to_filesystem() method converts StorageOptions into fsspec-compatible objects, allowing seamless integration with any fsspec-compatible library.\nfrom fsspec_utils.storage_options import AwsStorageOptions\n\nprint(\"=== Conceptual AwsStorageOptions Example (using a dummy endpoint) ===\\n\")\n\naws_options = AwsStorageOptions(\n    endpoint_url=\"http://s3.dummy-endpoint.com\",\n    access_key_id=\"DUMMY_KEY\",\n    secret_access_key=\"DUMMY_SECRET\",\n    allow_http=True,\n    region=\"us-east-1\"\n)\n\naws_fs = aws_options.to_filesystem()\nprint(f\"Created fsspec filesystem for S3: {type(aws_fs).__name__}\")\nprint(\"AWS storage example completed.\\n\")\n\n\n\nThis example shows how to configure AzureStorageOptions. It is expected to fail when attempting to connect to actual cloud services because it uses dummy credentials.\nfrom fsspec_utils.storage_options import AzureStorageOptions\n\nprint(\"=== Conceptual AzureStorageOptions Example (using a dummy connection string) ===\\n\")\nazure_options = AzureStorageOptions(\n    protocol=\"az\",\n    account_name=\"demoaccount\",\n    connection_string=\"DefaultEndpointsProtocol=https;AccountName=demoaccount;AccountKey=demokey==;EndpointSuffix=core.windows.net\"\n)\n\nazure_fs = azure_options.to_filesystem()\nprint(f\"Created fsspec filesystem for Azure: {type(azure_fs).__name__}\")\nprint(\"Azure storage example completed.\\n\")\n\n\n\nThis example shows how to configure GcsStorageOptions. It is expected to fail when attempting to connect to actual cloud services because it uses dummy credentials.\nStorageOptions classes provide a simplified, consistent interface for configuring connections to various storage systems. They abstract away the complexity of different storage backends and provide a unified way to create fsspec filesystem objects.\nThe to_filesystem() method converts these options into fsspec compatible objects, enabling seamless integration with any fsspec-compatible library or tool.\nImportant Note: The AWS, Azure, and GCS examples use dummy credentials and are for illustrative purposes only. These examples are expected to fail when attempting to connect to actual cloud services because:\n\nThe endpoint URLs are not real service endpoints\nThe credentials are placeholder values that don’t correspond to actual accounts\nThe connection strings and tokens are examples, not valid credentials\n\nThis approach allows you to understand the configuration pattern without needing actual cloud credentials. When using these examples in production, you would replace the dummy values with your real credentials and service endpoints.\n```python from fsspec_utils.storage_options import GcsStorageOptions\nprint(“=== Conceptual GcsStorageOptions Example (using a dummy token path) ===”) gcs_options = GcsStorageOptions( protocol=“gs”, project=“demo-project”, token=“path/to/dummy-service-account.json” )\ngcs_fs = gcs_options.to_filesystem() print(f”Created fsspec filesystem for GCS: {type(gcs_fs).__name__}“) print(”GCS storage example completed.“)"
  },
  {
    "objectID": "api/fsspec_utils.storage_options.core.html",
    "href": "api/fsspec_utils.storage_options.core.html",
    "title": "fsspec_utils.storage_options.core API Reference",
    "section": "",
    "text": "Local filesystem configuration options.\nProvides basic configuration for local file access. While this class is simple, it maintains consistency with other storage options and enables transparent switching between local and remote storage.\nAttributes:\n\nprotocol (str): Always “file” for local filesystem\nauto_mkdir (bool): Create directories automatically\nmode (int): Default file creation mode (unix-style)\n\nExample:\n# Basic local access\noptions = LocalStorageOptions()\nfs = options.to_filesystem()\nfiles = fs.ls(\"/path/to/data\")\n\n# With auto directory creation\noptions = LocalStorageOptions(auto_mkdir=True)\nfs = options.to_filesystem()\nwith fs.open(\"/new/path/file.txt\", \"w\") as f:\n    f.write(\"test\")  # Creates /new/path/ automatically\n\n\nConvert options to fsspec filesystem arguments.\nReturns:\n\ndict: Arguments suitable for LocalFileSystem\n\nExample:\noptions = LocalStorageOptions(auto_mkdir=True)\nkwargs = options.to_fsspec_kwargs()\nfs = filesystem(\"file\", **kwargs)\n\n\n\n\nCreate appropriate storage options instance from dictionary.\nFactory function that creates the correct storage options class based on protocol.\nParameters:\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nprotocol\nstr\nStorage protocol identifier (e.g., “s3”, “gs”, “file”)\n\n\nstorage_options\ndict\nDictionary of configuration options\n\n\n\nReturns:\n\nBaseStorageOptions: Appropriate storage options instance\n\nRaises:\n\nValueError: If protocol is not supported\n\nExample:\n# Create S3 options\noptions = from_dict(\"s3\", {\n    \"access_key_id\": \"KEY\",\n    \"secret_access_key\": \"SECRET\"\n})\nprint(type(options).__name__)\n\n\n\nCreate storage options from environment variables.\nFactory function that creates and configures storage options from protocol-specific environment variables.\nParameters:\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nprotocol\nstr\nStorage protocol identifier (e.g., “s3”, “github”)\n\n\n\nReturns:\n\nBaseStorageOptions: Configured storage options instance\n\nRaises:\n\nValueError: If protocol is not supported\n\nExample:\n# With AWS credentials in environment\noptions = from_env(\"s3\")\nprint(options.access_key_id)\n\n\n\nInfer the storage protocol from a URI string.\nAnalyzes the URI to determine the appropriate storage protocol based on the scheme or path format.\nParameters:\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nuri\nstr\nURI or path string to analyze. Examples: - “s3://bucket/path” - “gs://bucket/path” - “github://org/repo” - “/local/path”\n\n\n\nReturns:\n\nstr: Inferred protocol identifier\n\nExample:\n# S3 protocol\ninfer_protocol_from_uri(\"s3://my-bucket/data\")\n\n# Local file\ninfer_protocol_from_uri(\"/home/user/data\")\n\n# GitHub repository\ninfer_protocol_from_uri(\"github://microsoft/vscode\")\n\n\n\nCreate storage options instance from a URI string.\nInfers the protocol and extracts relevant configuration from the URI to create appropriate storage options.\nParameters:\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nuri\nstr\nURI string containing protocol and optional configuration. Examples: - “s3://bucket/path” - “gs://project/bucket/path” - “github://org/repo”\n\n\n\nReturns:\n\nBaseStorageOptions: Configured storage options instance\n\nExample:\n# S3 options\nopts = storage_options_from_uri(\"s3://my-bucket/data\")\nprint(opts.protocol)\n\n# GitHub options\nopts = storage_options_from_uri(\"github://microsoft/vscode\")\nprint(opts.org)\nprint(opts.repo)\n\n\n\nMerge multiple storage options into a single configuration.\nCombines options from multiple sources with control over precedence.\nParameters:\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n*options\nBaseStorageOptions or dict\nStorage options to merge. Can be: - BaseStorageOptions instances - Dictionaries of options - None values (ignored)\n\n\noverwrite\nbool\nWhether later options override earlier ones\n\n\n\nReturns:\n\nBaseStorageOptions: Combined storage options\n\nExample:\n# Merge with overwrite\nbase = AwsStorageOptions(\n    region=\"us-east-1\",\n    access_key_id=\"OLD_KEY\"\n)\noverride = {\"access_key_id\": \"NEW_KEY\"}\nmerged = merge_storage_options(base, override)\nprint(merged.access_key_id)\n\n# Preserve existing values\nmerged = merge_storage_options(\n    base,\n    override,\n    overwrite=False\n)\nprint(merged.access_key_id)\n\n\n\nHigh-level storage options container and factory.\nProvides a unified interface for creating and managing storage options for different protocols.\nAttributes:\n\nstorage_options (BaseStorageOptions): Underlying storage options instance\n\nExample:\n# Create from protocol\noptions = StorageOptions.create(\n    protocol=\"s3\",\n    access_key_id=\"KEY\",\n    secret_access_key=\"SECRET\"\n)\n\n# Create from existing options\ns3_opts = AwsStorageOptions(access_key_id=\"KEY\")\noptions = StorageOptions(storage_options=s3_opts)\n\n\nCreate storage options from arguments.\nParameters:\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n**data\ndict\nEither: - protocol and configuration options - storage_options=pre-configured instance\n\n\n\nReturns:\n\nStorageOptions: Configured storage options instance\n\nRaises:\n\nValueError: If protocol missing or invalid\n\nExample:\n# Direct protocol config\noptions = StorageOptions.create(\n    protocol=\"s3\",\n    region=\"us-east-1\"\n)\n\n\n\nCreate storage options from YAML configuration.\nParameters:\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\npath\nstr\nPath to YAML configuration file\n\n\nfs\nfsspec.AbstractFileSystem, optional\nFilesystem for reading configuration\n\n\n\nReturns:\n\nStorageOptions: Configured storage options\n\nExample:\n# Load from config file\noptions = StorageOptions.from_yaml(\"storage.yml\")\nprint(options.storage_options.protocol)\n\n\n\nCreate storage options from environment variables.\nParameters:\n\n\n\nName\nType\nDescription\n\n\n\n\nprotocol\nstr\nStorage protocol to configure\n\n\n\nReturns:\n\nStorageOptions: Environment-configured options\n\nExample:\n# Load AWS config from environment\noptions = StorageOptions.from_env(\"s3\")\n\n\n\nCreate fsspec filesystem instance.\nReturns:\n\nAbstractFileSystem: Configured filesystem instance\n\nExample:\noptions = StorageOptions.create(protocol=\"file\")\nfs = options.to_filesystem()\nfiles = fs.ls(\"/data\")\n\n\n\nConvert storage options to dictionary.\nParameters:\n\n\n\nName\nType\nDescription\n\n\n\n\nwith_protocol\nbool\nWhether to include protocol in output\n\n\n\nReturns:\n\ndict: Storage options as dictionary\n\nExample:\noptions = StorageOptions.create(\n    protocol=\"s3\",\n    region=\"us-east-1\"\n)\nprint(options.to_dict())\n\n\n\nGet options formatted for object store clients.\nParameters:\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nwith_conditional_put\nbool\nAdd etag-based conditional put support\n\n\n\nReturns:\n\ndict: Object store configuration dictionary\n\nExample:\noptions = StorageOptions.create(protocol=\"s3\")\nkwargs = options.to_object_store_kwargs()\n# store = ObjectStore(**kwargs)\n\n\n\n\nBase class for filesystem storage configuration options.\nProvides common functionality for all storage option classes including: - YAML serialization/deserialization - Dictionary conversion - Filesystem instance creation - Configuration updates\nAttributes:\n\nprotocol (str): Storage protocol identifier (e.g., “s3”, “gs”, “file”)\n\nExample:\n# Create and save options\noptions = BaseStorageOptions(protocol=\"s3\")\noptions.to_yaml(\"config.yml\")\n\n# Load from YAML\nloaded = BaseStorageOptions.from_yaml(\"config.yml\")\nprint(loaded.protocol)\n\n\nConvert storage options to dictionary.\nParameters:\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nwith_protocol\nbool\nWhether to include protocol in output dictionary\n\n\n\nReturns:\n\ndict: Dictionary of storage options with non-None values\n\nExample:\noptions = BaseStorageOptions(protocol=\"s3\")\nprint(options.to_dict())\nprint(options.to_dict(with_protocol=True))\n\n\n\nLoad storage options from YAML file.\nParameters:\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\npath\nstr\nPath to YAML configuration file\n\n\nfs\nfsspec.AbstractFileSystem, optional\nFilesystem to use for reading file\n\n\n\nReturns:\n\nBaseStorageOptions: Loaded storage options instance\n\nExample:\n# Load from local file\noptions = BaseStorageOptions.from_yaml(\"config.yml\")\nprint(options.protocol)\n\n\n\nSave storage options to YAML file.\nParameters:\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\npath\nstr\nPath where to save configuration\n\n\nfs\nfsspec.AbstractFileSystem, optional\nFilesystem to use for writing\n\n\n\nExample:\noptions = BaseStorageOptions(protocol=\"s3\")\noptions.to_yaml(\"config.yml\")\n\n\n\nCreate fsspec filesystem instance from options.\nReturns:\n\nAbstractFileSystem: Configured filesystem instance\n\nExample:\noptions = BaseStorageOptions(protocol=\"file\")\nfs = options.to_filesystem()\nfiles = fs.ls(\"/path/to/data\")\n\n\n\nUpdate storage options with new values.\nParameters:\n\n\n\nName\nType\nDescription\n\n\n\n\n**kwargs\ndict\nNew option values to set\n\n\n\nReturns:\n\nBaseStorageOptions: Updated instance\n\nExample:\noptions = BaseStorageOptions(protocol=\"s3\")\noptions = options.update(region=\"us-east-1\")\nprint(options.region)"
  },
  {
    "objectID": "api/fsspec_utils.storage_options.core.html#localstorageoptions",
    "href": "api/fsspec_utils.storage_options.core.html#localstorageoptions",
    "title": "fsspec_utils.storage_options.core API Reference",
    "section": "",
    "text": "Local filesystem configuration options.\nProvides basic configuration for local file access. While this class is simple, it maintains consistency with other storage options and enables transparent switching between local and remote storage.\nAttributes:\n\nprotocol (str): Always “file” for local filesystem\nauto_mkdir (bool): Create directories automatically\nmode (int): Default file creation mode (unix-style)\n\nExample:\n# Basic local access\noptions = LocalStorageOptions()\nfs = options.to_filesystem()\nfiles = fs.ls(\"/path/to/data\")\n\n# With auto directory creation\noptions = LocalStorageOptions(auto_mkdir=True)\nfs = options.to_filesystem()\nwith fs.open(\"/new/path/file.txt\", \"w\") as f:\n    f.write(\"test\")  # Creates /new/path/ automatically\n\n\nConvert options to fsspec filesystem arguments.\nReturns:\n\ndict: Arguments suitable for LocalFileSystem\n\nExample:\noptions = LocalStorageOptions(auto_mkdir=True)\nkwargs = options.to_fsspec_kwargs()\nfs = filesystem(\"file\", **kwargs)"
  },
  {
    "objectID": "api/fsspec_utils.storage_options.core.html#from_dict",
    "href": "api/fsspec_utils.storage_options.core.html#from_dict",
    "title": "fsspec_utils.storage_options.core API Reference",
    "section": "",
    "text": "Create appropriate storage options instance from dictionary.\nFactory function that creates the correct storage options class based on protocol.\nParameters:\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nprotocol\nstr\nStorage protocol identifier (e.g., “s3”, “gs”, “file”)\n\n\nstorage_options\ndict\nDictionary of configuration options\n\n\n\nReturns:\n\nBaseStorageOptions: Appropriate storage options instance\n\nRaises:\n\nValueError: If protocol is not supported\n\nExample:\n# Create S3 options\noptions = from_dict(\"s3\", {\n    \"access_key_id\": \"KEY\",\n    \"secret_access_key\": \"SECRET\"\n})\nprint(type(options).__name__)"
  },
  {
    "objectID": "api/fsspec_utils.storage_options.core.html#from_env",
    "href": "api/fsspec_utils.storage_options.core.html#from_env",
    "title": "fsspec_utils.storage_options.core API Reference",
    "section": "",
    "text": "Create storage options from environment variables.\nFactory function that creates and configures storage options from protocol-specific environment variables.\nParameters:\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nprotocol\nstr\nStorage protocol identifier (e.g., “s3”, “github”)\n\n\n\nReturns:\n\nBaseStorageOptions: Configured storage options instance\n\nRaises:\n\nValueError: If protocol is not supported\n\nExample:\n# With AWS credentials in environment\noptions = from_env(\"s3\")\nprint(options.access_key_id)"
  },
  {
    "objectID": "api/fsspec_utils.storage_options.core.html#infer_protocol_from_uri",
    "href": "api/fsspec_utils.storage_options.core.html#infer_protocol_from_uri",
    "title": "fsspec_utils.storage_options.core API Reference",
    "section": "",
    "text": "Infer the storage protocol from a URI string.\nAnalyzes the URI to determine the appropriate storage protocol based on the scheme or path format.\nParameters:\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nuri\nstr\nURI or path string to analyze. Examples: - “s3://bucket/path” - “gs://bucket/path” - “github://org/repo” - “/local/path”\n\n\n\nReturns:\n\nstr: Inferred protocol identifier\n\nExample:\n# S3 protocol\ninfer_protocol_from_uri(\"s3://my-bucket/data\")\n\n# Local file\ninfer_protocol_from_uri(\"/home/user/data\")\n\n# GitHub repository\ninfer_protocol_from_uri(\"github://microsoft/vscode\")"
  },
  {
    "objectID": "api/fsspec_utils.storage_options.core.html#storage_options_from_uri",
    "href": "api/fsspec_utils.storage_options.core.html#storage_options_from_uri",
    "title": "fsspec_utils.storage_options.core API Reference",
    "section": "",
    "text": "Create storage options instance from a URI string.\nInfers the protocol and extracts relevant configuration from the URI to create appropriate storage options.\nParameters:\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nuri\nstr\nURI string containing protocol and optional configuration. Examples: - “s3://bucket/path” - “gs://project/bucket/path” - “github://org/repo”\n\n\n\nReturns:\n\nBaseStorageOptions: Configured storage options instance\n\nExample:\n# S3 options\nopts = storage_options_from_uri(\"s3://my-bucket/data\")\nprint(opts.protocol)\n\n# GitHub options\nopts = storage_options_from_uri(\"github://microsoft/vscode\")\nprint(opts.org)\nprint(opts.repo)"
  },
  {
    "objectID": "api/fsspec_utils.storage_options.core.html#merge_storage_options",
    "href": "api/fsspec_utils.storage_options.core.html#merge_storage_options",
    "title": "fsspec_utils.storage_options.core API Reference",
    "section": "",
    "text": "Merge multiple storage options into a single configuration.\nCombines options from multiple sources with control over precedence.\nParameters:\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n*options\nBaseStorageOptions or dict\nStorage options to merge. Can be: - BaseStorageOptions instances - Dictionaries of options - None values (ignored)\n\n\noverwrite\nbool\nWhether later options override earlier ones\n\n\n\nReturns:\n\nBaseStorageOptions: Combined storage options\n\nExample:\n# Merge with overwrite\nbase = AwsStorageOptions(\n    region=\"us-east-1\",\n    access_key_id=\"OLD_KEY\"\n)\noverride = {\"access_key_id\": \"NEW_KEY\"}\nmerged = merge_storage_options(base, override)\nprint(merged.access_key_id)\n\n# Preserve existing values\nmerged = merge_storage_options(\n    base,\n    override,\n    overwrite=False\n)\nprint(merged.access_key_id)"
  },
  {
    "objectID": "api/fsspec_utils.storage_options.core.html#storageoptions",
    "href": "api/fsspec_utils.storage_options.core.html#storageoptions",
    "title": "fsspec_utils.storage_options.core API Reference",
    "section": "",
    "text": "High-level storage options container and factory.\nProvides a unified interface for creating and managing storage options for different protocols.\nAttributes:\n\nstorage_options (BaseStorageOptions): Underlying storage options instance\n\nExample:\n# Create from protocol\noptions = StorageOptions.create(\n    protocol=\"s3\",\n    access_key_id=\"KEY\",\n    secret_access_key=\"SECRET\"\n)\n\n# Create from existing options\ns3_opts = AwsStorageOptions(access_key_id=\"KEY\")\noptions = StorageOptions(storage_options=s3_opts)\n\n\nCreate storage options from arguments.\nParameters:\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n**data\ndict\nEither: - protocol and configuration options - storage_options=pre-configured instance\n\n\n\nReturns:\n\nStorageOptions: Configured storage options instance\n\nRaises:\n\nValueError: If protocol missing or invalid\n\nExample:\n# Direct protocol config\noptions = StorageOptions.create(\n    protocol=\"s3\",\n    region=\"us-east-1\"\n)\n\n\n\nCreate storage options from YAML configuration.\nParameters:\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\npath\nstr\nPath to YAML configuration file\n\n\nfs\nfsspec.AbstractFileSystem, optional\nFilesystem for reading configuration\n\n\n\nReturns:\n\nStorageOptions: Configured storage options\n\nExample:\n# Load from config file\noptions = StorageOptions.from_yaml(\"storage.yml\")\nprint(options.storage_options.protocol)\n\n\n\nCreate storage options from environment variables.\nParameters:\n\n\n\nName\nType\nDescription\n\n\n\n\nprotocol\nstr\nStorage protocol to configure\n\n\n\nReturns:\n\nStorageOptions: Environment-configured options\n\nExample:\n# Load AWS config from environment\noptions = StorageOptions.from_env(\"s3\")\n\n\n\nCreate fsspec filesystem instance.\nReturns:\n\nAbstractFileSystem: Configured filesystem instance\n\nExample:\noptions = StorageOptions.create(protocol=\"file\")\nfs = options.to_filesystem()\nfiles = fs.ls(\"/data\")\n\n\n\nConvert storage options to dictionary.\nParameters:\n\n\n\nName\nType\nDescription\n\n\n\n\nwith_protocol\nbool\nWhether to include protocol in output\n\n\n\nReturns:\n\ndict: Storage options as dictionary\n\nExample:\noptions = StorageOptions.create(\n    protocol=\"s3\",\n    region=\"us-east-1\"\n)\nprint(options.to_dict())\n\n\n\nGet options formatted for object store clients.\nParameters:\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nwith_conditional_put\nbool\nAdd etag-based conditional put support\n\n\n\nReturns:\n\ndict: Object store configuration dictionary\n\nExample:\noptions = StorageOptions.create(protocol=\"s3\")\nkwargs = options.to_object_store_kwargs()\n# store = ObjectStore(**kwargs)"
  },
  {
    "objectID": "api/fsspec_utils.storage_options.core.html#basestorageoptions",
    "href": "api/fsspec_utils.storage_options.core.html#basestorageoptions",
    "title": "fsspec_utils.storage_options.core API Reference",
    "section": "",
    "text": "Base class for filesystem storage configuration options.\nProvides common functionality for all storage option classes including: - YAML serialization/deserialization - Dictionary conversion - Filesystem instance creation - Configuration updates\nAttributes:\n\nprotocol (str): Storage protocol identifier (e.g., “s3”, “gs”, “file”)\n\nExample:\n# Create and save options\noptions = BaseStorageOptions(protocol=\"s3\")\noptions.to_yaml(\"config.yml\")\n\n# Load from YAML\nloaded = BaseStorageOptions.from_yaml(\"config.yml\")\nprint(loaded.protocol)\n\n\nConvert storage options to dictionary.\nParameters:\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nwith_protocol\nbool\nWhether to include protocol in output dictionary\n\n\n\nReturns:\n\ndict: Dictionary of storage options with non-None values\n\nExample:\noptions = BaseStorageOptions(protocol=\"s3\")\nprint(options.to_dict())\nprint(options.to_dict(with_protocol=True))\n\n\n\nLoad storage options from YAML file.\nParameters:\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\npath\nstr\nPath to YAML configuration file\n\n\nfs\nfsspec.AbstractFileSystem, optional\nFilesystem to use for reading file\n\n\n\nReturns:\n\nBaseStorageOptions: Loaded storage options instance\n\nExample:\n# Load from local file\noptions = BaseStorageOptions.from_yaml(\"config.yml\")\nprint(options.protocol)\n\n\n\nSave storage options to YAML file.\nParameters:\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\npath\nstr\nPath where to save configuration\n\n\nfs\nfsspec.AbstractFileSystem, optional\nFilesystem to use for writing\n\n\n\nExample:\noptions = BaseStorageOptions(protocol=\"s3\")\noptions.to_yaml(\"config.yml\")\n\n\n\nCreate fsspec filesystem instance from options.\nReturns:\n\nAbstractFileSystem: Configured filesystem instance\n\nExample:\noptions = BaseStorageOptions(protocol=\"file\")\nfs = options.to_filesystem()\nfiles = fs.ls(\"/path/to/data\")\n\n\n\nUpdate storage options with new values.\nParameters:\n\n\n\nName\nType\nDescription\n\n\n\n\n**kwargs\ndict\nNew option values to set\n\n\n\nReturns:\n\nBaseStorageOptions: Updated instance\n\nExample:\noptions = BaseStorageOptions(protocol=\"s3\")\noptions = options.update(region=\"us-east-1\")\nprint(options.region)"
  },
  {
    "objectID": "api/fsspec_utils.storage_options.base.html",
    "href": "api/fsspec_utils.storage_options.base.html",
    "title": "fsspec_utils.storage_options.base API Documentation",
    "section": "",
    "text": "This module defines the base class for filesystem storage configuration options.\n\n\n\nBase class for filesystem storage configuration options.\nProvides common functionality for all storage option classes including:\n\nYAML serialization/deserialization\nDictionary conversion\nFilesystem instance creation\nConfiguration updates\n\nAttributes:\n\nprotocol (str): Storage protocol identifier (e.g., “s3”, “gs”, “file”)\n\nExample:\nfrom fsspec_utils.storage_options.base import BaseStorageOptions\n\n# Create and save options\noptions = BaseStorageOptions(protocol=\"s3\")\noptions.to_yaml(\"config.yml\")\n\n# Load from YAML\nloaded = BaseStorageOptions.from_yaml(\"config.yml\")\nprint(loaded.protocol)\n# 's3'\n\n\nConvert storage options to dictionary.\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\n\n\n\n\nwith_protocol\nbool\nWhether to include protocol in output dictionary\n\n\n\n\n\n\nReturns\nType\nDescription\n\n\n\n\ndict\ndict\nDictionary of storage options with non-None values\n\n\n\nExample:\nfrom fsspec_utils.storage_options.base import BaseStorageOptions\n\noptions = BaseStorageOptions(protocol=\"s3\")\nprint(options.to_dict())\n# {}\nprint(options.to_dict(with_protocol=True))\n# {'protocol': 's3'}\n\n\n\nLoad storage options from YAML file.\n\n\n\nParameter\nType\nDescription\n\n\n\n\npath\nstr\nPath to YAML configuration file\n\n\nfs\nAbstractFileSystem\nFilesystem to use for reading file\n\n\n\n\n\n\n\n\n\n\n\nReturns\nType\nDescription\n\n\n\n\nBaseStorageOptions\nBaseStorageOptions\nLoaded storage options instance\n\n\n\nExample:\n# Load from local file\nfrom fsspec_utils.storage_options.base import BaseStorageOptions\nfrom fsspec.implementations.local import LocalFileSystem\n\n# Assuming 'config.yml' exists and contains valid YAML for BaseStorageOptions\n# For example, a file named config.yml with content:\n# protocol: s3\n#\n# To make this example runnable, we'll create a dummy config.yml\nfs_local = LocalFileSystem()\nfs_local.write_text(\"config.yml\", \"protocol: s3\")\n\noptions = BaseStorageOptions.from_yaml(\"config.yml\")\nprint(options.protocol)\n# 's3'\n\n# Clean up the dummy file\nfs_local.rm(\"config.yml\")\n\n\n\nSave storage options to YAML file.\n\n\n\nParameter\nType\nDescription\n\n\n\n\npath\nstr\nPath where to save configuration\n\n\nfs\nAbstractFileSystem\nFilesystem to use for writing\n\n\n\nExample:\nfrom fsspec_utils.storage_options.base import BaseStorageOptions\nfrom fsspec.implementations.local import LocalFileSystem\n\noptions = BaseStorageOptions(protocol=\"s3\")\nfs_local = LocalFileSystem()\noptions.to_yaml(\"config.yml\", fs=fs_local) # Specify filesystem for writing\n\n\n\nCreate fsspec filesystem instance from options.\n\n\n\n\n\n\n\n\nReturns\nType\nDescription\n\n\n\n\nAbstractFileSystem\nAbstractFileSystem\nConfigured filesystem instance\n\n\n\nExample:\nfrom fsspec_utils.storage_options.base import BaseStorageOptions\n\noptions = BaseStorageOptions(protocol=\"file\")\nfs = options.to_filesystem()\n# Example usage: list files in a dummy directory\nimport os\nimport tempfile\n\nwith tempfile.TemporaryDirectory() as tmpdir:\n    dummy_file_path = os.path.join(tmpdir, \"test.txt\")\n    with open(dummy_file_path, \"w\") as f:\n        f.write(\"dummy content\")\n    fs_temp = options.to_filesystem()\n    files = fs_temp.ls(tmpdir)\n    print(files)\n\n\n\nUpdate storage options with new values.\n\n\n\nParameter\nType\nDescription\n\n\n\n\n**kwargs\nAny\nNew option values to set\n\n\n\n\n\n\nReturns\nType\nDescription\n\n\n\n\nBaseStorageOptions\nBaseStorageOptions\nUpdated instance\n\n\n\nExample:\nfrom fsspec_utils.storage_options.base import BaseStorageOptions\n\noptions = BaseStorageOptions(protocol=\"s3\")\noptions = options.update(region=\"us-east-1\")\nprint(options.region)\n# 'us-east-1'"
  },
  {
    "objectID": "api/fsspec_utils.storage_options.base.html#basestorageoptions",
    "href": "api/fsspec_utils.storage_options.base.html#basestorageoptions",
    "title": "fsspec_utils.storage_options.base API Documentation",
    "section": "",
    "text": "Base class for filesystem storage configuration options.\nProvides common functionality for all storage option classes including:\n\nYAML serialization/deserialization\nDictionary conversion\nFilesystem instance creation\nConfiguration updates\n\nAttributes:\n\nprotocol (str): Storage protocol identifier (e.g., “s3”, “gs”, “file”)\n\nExample:\nfrom fsspec_utils.storage_options.base import BaseStorageOptions\n\n# Create and save options\noptions = BaseStorageOptions(protocol=\"s3\")\noptions.to_yaml(\"config.yml\")\n\n# Load from YAML\nloaded = BaseStorageOptions.from_yaml(\"config.yml\")\nprint(loaded.protocol)\n# 's3'\n\n\nConvert storage options to dictionary.\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\n\n\n\n\nwith_protocol\nbool\nWhether to include protocol in output dictionary\n\n\n\n\n\n\nReturns\nType\nDescription\n\n\n\n\ndict\ndict\nDictionary of storage options with non-None values\n\n\n\nExample:\nfrom fsspec_utils.storage_options.base import BaseStorageOptions\n\noptions = BaseStorageOptions(protocol=\"s3\")\nprint(options.to_dict())\n# {}\nprint(options.to_dict(with_protocol=True))\n# {'protocol': 's3'}\n\n\n\nLoad storage options from YAML file.\n\n\n\nParameter\nType\nDescription\n\n\n\n\npath\nstr\nPath to YAML configuration file\n\n\nfs\nAbstractFileSystem\nFilesystem to use for reading file\n\n\n\n\n\n\n\n\n\n\n\nReturns\nType\nDescription\n\n\n\n\nBaseStorageOptions\nBaseStorageOptions\nLoaded storage options instance\n\n\n\nExample:\n# Load from local file\nfrom fsspec_utils.storage_options.base import BaseStorageOptions\nfrom fsspec.implementations.local import LocalFileSystem\n\n# Assuming 'config.yml' exists and contains valid YAML for BaseStorageOptions\n# For example, a file named config.yml with content:\n# protocol: s3\n#\n# To make this example runnable, we'll create a dummy config.yml\nfs_local = LocalFileSystem()\nfs_local.write_text(\"config.yml\", \"protocol: s3\")\n\noptions = BaseStorageOptions.from_yaml(\"config.yml\")\nprint(options.protocol)\n# 's3'\n\n# Clean up the dummy file\nfs_local.rm(\"config.yml\")\n\n\n\nSave storage options to YAML file.\n\n\n\nParameter\nType\nDescription\n\n\n\n\npath\nstr\nPath where to save configuration\n\n\nfs\nAbstractFileSystem\nFilesystem to use for writing\n\n\n\nExample:\nfrom fsspec_utils.storage_options.base import BaseStorageOptions\nfrom fsspec.implementations.local import LocalFileSystem\n\noptions = BaseStorageOptions(protocol=\"s3\")\nfs_local = LocalFileSystem()\noptions.to_yaml(\"config.yml\", fs=fs_local) # Specify filesystem for writing\n\n\n\nCreate fsspec filesystem instance from options.\n\n\n\n\n\n\n\n\nReturns\nType\nDescription\n\n\n\n\nAbstractFileSystem\nAbstractFileSystem\nConfigured filesystem instance\n\n\n\nExample:\nfrom fsspec_utils.storage_options.base import BaseStorageOptions\n\noptions = BaseStorageOptions(protocol=\"file\")\nfs = options.to_filesystem()\n# Example usage: list files in a dummy directory\nimport os\nimport tempfile\n\nwith tempfile.TemporaryDirectory() as tmpdir:\n    dummy_file_path = os.path.join(tmpdir, \"test.txt\")\n    with open(dummy_file_path, \"w\") as f:\n        f.write(\"dummy content\")\n    fs_temp = options.to_filesystem()\n    files = fs_temp.ls(tmpdir)\n    print(files)\n\n\n\nUpdate storage options with new values.\n\n\n\nParameter\nType\nDescription\n\n\n\n\n**kwargs\nAny\nNew option values to set\n\n\n\n\n\n\nReturns\nType\nDescription\n\n\n\n\nBaseStorageOptions\nBaseStorageOptions\nUpdated instance\n\n\n\nExample:\nfrom fsspec_utils.storage_options.base import BaseStorageOptions\n\noptions = BaseStorageOptions(protocol=\"s3\")\noptions = options.update(region=\"us-east-1\")\nprint(options.region)\n# 'us-east-1'"
  },
  {
    "objectID": "api/fsspec_utils.utils.types.html",
    "href": "api/fsspec_utils.utils.types.html",
    "title": "fsspec_utils.utils.types API Reference",
    "section": "",
    "text": "Convert a dictionary or list of dictionaries to a Polars DataFrame.\nHandles various input formats: - Single dict with list values -&gt; DataFrame rows - Single dict with scalar values -&gt; Single row DataFrame - List of dicts with scalar values -&gt; Multi-row DataFrame - List of dicts with list values -&gt; DataFrame with list columns\nParameters:\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\ndata\ndict or list[dict]\nThe input data, either a dictionary or a list of dictionaries.\n\n\nunique\nbool\nIf True, duplicate rows will be removed from the resulting DataFrame.\n\n\n\nReturns:\n\npolars.DataFrame: Polars DataFrame containing the converted data.\n\nExamples:\n# Single dict with list values\ndata = {'a': [1, 2, 3], 'b': [4, 5, 6]}\ndict_to_dataframe(data)\n\n# Single dict with scalar values\ndata = {'a': 1, 'b': 2}\ndict_to_dataframe(data)\n\n# List of dicts with scalar values\ndata = [{'a': 1, 'b': 2}, {'a': 3, 'b': 4}]\ndict_to_dataframe(data)\n\n\n\nConvert various data formats to PyArrow Table.\nHandles conversion from Polars DataFrames, Pandas DataFrames, dictionaries, and lists of these types to PyArrow Tables.\nParameters:\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\ndata\nAny\nInput data to convert.\n\n\nconcat\nbool\nWhether to concatenate multiple inputs into single table.\n\n\nunique\nbool\nIf True, duplicate rows will be removed from the resulting Table.\n\n\n\nExample:\nimport polars as pl\nimport pyarrow as pa\nfrom fsspec_utils.utils.types import to_pyarrow_table\n\n# Convert Polars DataFrame to PyArrow Table\ndf = pl.DataFrame({\"a\": [1, 2], \"b\": [3, 4]})\ntable = to_pyarrow_table(df)\nprint(table.schema)\n\n# Convert list of dicts to PyArrow Table\ndata = [{\"a\": 1, \"b\": 10}, {\"a\": 2, \"b\": 20}]\ntable_from_dict = to_pyarrow_table(data)\nprint(table_from_dict.to_pydf())\nReturns:\n\npyarrow.Table: PyArrow Table containing the converted data.\n\nExample:\ndf = pl.DataFrame({\"a\": [1, 2], \"b\": [3, 4]})\ntable = to_pyarrow_table(df)\nprint(table.schema)"
  },
  {
    "objectID": "api/fsspec_utils.utils.types.html#dict_to_dataframe",
    "href": "api/fsspec_utils.utils.types.html#dict_to_dataframe",
    "title": "fsspec_utils.utils.types API Reference",
    "section": "",
    "text": "Convert a dictionary or list of dictionaries to a Polars DataFrame.\nHandles various input formats: - Single dict with list values -&gt; DataFrame rows - Single dict with scalar values -&gt; Single row DataFrame - List of dicts with scalar values -&gt; Multi-row DataFrame - List of dicts with list values -&gt; DataFrame with list columns\nParameters:\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\ndata\ndict or list[dict]\nThe input data, either a dictionary or a list of dictionaries.\n\n\nunique\nbool\nIf True, duplicate rows will be removed from the resulting DataFrame.\n\n\n\nReturns:\n\npolars.DataFrame: Polars DataFrame containing the converted data.\n\nExamples:\n# Single dict with list values\ndata = {'a': [1, 2, 3], 'b': [4, 5, 6]}\ndict_to_dataframe(data)\n\n# Single dict with scalar values\ndata = {'a': 1, 'b': 2}\ndict_to_dataframe(data)\n\n# List of dicts with scalar values\ndata = [{'a': 1, 'b': 2}, {'a': 3, 'b': 4}]\ndict_to_dataframe(data)"
  },
  {
    "objectID": "api/fsspec_utils.utils.types.html#to_pyarrow_table",
    "href": "api/fsspec_utils.utils.types.html#to_pyarrow_table",
    "title": "fsspec_utils.utils.types API Reference",
    "section": "",
    "text": "Convert various data formats to PyArrow Table.\nHandles conversion from Polars DataFrames, Pandas DataFrames, dictionaries, and lists of these types to PyArrow Tables.\nParameters:\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\ndata\nAny\nInput data to convert.\n\n\nconcat\nbool\nWhether to concatenate multiple inputs into single table.\n\n\nunique\nbool\nIf True, duplicate rows will be removed from the resulting Table.\n\n\n\nExample:\nimport polars as pl\nimport pyarrow as pa\nfrom fsspec_utils.utils.types import to_pyarrow_table\n\n# Convert Polars DataFrame to PyArrow Table\ndf = pl.DataFrame({\"a\": [1, 2], \"b\": [3, 4]})\ntable = to_pyarrow_table(df)\nprint(table.schema)\n\n# Convert list of dicts to PyArrow Table\ndata = [{\"a\": 1, \"b\": 10}, {\"a\": 2, \"b\": 20}]\ntable_from_dict = to_pyarrow_table(data)\nprint(table_from_dict.to_pydf())\nReturns:\n\npyarrow.Table: PyArrow Table containing the converted data.\n\nExample:\ndf = pl.DataFrame({\"a\": [1, 2], \"b\": [3, 4]})\ntable = to_pyarrow_table(df)\nprint(table.schema)"
  },
  {
    "objectID": "api/fsspec_utils.utils.pyarrow.html",
    "href": "api/fsspec_utils.utils.pyarrow.html",
    "title": "fsspec_utils.utils.pyarrow API Reference",
    "section": "",
    "text": "For each timestamp column (by name) across all schemas, detect the most frequent timezone (including None).\nIf None and a timezone are tied, prefer the timezone. Returns a dict: {column_name: dominant_timezone}\nParameters:\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nschemas\nlist[pyarrow.Schema]\nA list of PyArrow schemas to analyze.\n\n\n\nExample:\nimport pyarrow as pa\nfrom fsspec_utils.utils.pyarrow import dominant_timezone_per_column\n\nschema1 = pa.schema([(\"ts\", pa.timestamp(\"ns\", tz=\"UTC\"))])\nschema2 = pa.schema([(\"ts\", pa.timestamp(\"ns\", tz=\"Europe/Berlin\"))])\nschema3 = pa.schema([(\"ts\", pa.timestamp(\"ns\"))]) # naive\nschemas = [schema1, schema2, schema3]\n\ndominant_tz = dominant_timezone_per_column(schemas)\nprint(dominant_tz)\n# Expected: {'ts': 'UTC'} (or 'Europe/Berlin' depending on logic)\nReturns:\n\ndict: {column_name: dominant_timezone}\n\n\n\n\nFor each timestamp column (by name) across all schemas, set the timezone to the most frequent (with tie-breaking).\nParameters:\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nschemas\nlist[pyarrow.Schema]\nA list of PyArrow schemas to standardize.\n\n\n\nExample:\nimport pyarrow as pa\nfrom fsspec_utils.utils.pyarrow import standardize_schema_timezones_by_majority\n\nschema1 = pa.schema([(\"ts\", pa.timestamp(\"ns\", tz=\"UTC\"))])\nschema2 = pa.schema([(\"ts\", pa.timestamp(\"ns\", tz=\"Europe/Berlin\"))])\nschemas = [schema1, schema2]\n\nstandardized_schemas = standardize_schema_timezones_by_majority(schemas)\nprint(standardized_schemas[0].field(\"ts\").type)\nprint(standardized_schemas[1].field(\"ts\").type)\n# Expected: timestamp[ns, tz=Europe/Berlin] (or UTC, depending on tie-breaking)\nReturns:\n\nlist[pyarrow.Schema]: A new list of schemas with updated timestamp timezones.\n\n\n\n\nStandardize timezone info for all timestamp columns in a list of PyArrow schemas.\nParameters:\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nschemas\nlist[pyarrow.Schema]\nThe list of PyArrow schemas to process.\n\n\ntimezone\nstr or None\nThe target timezone to apply to timestamp columns. If None, timezones are removed. If “auto”, the most frequent timezone across schemas is used.\n\n\n\nExample:\nimport pyarrow as pa\nfrom fsspec_utils.utils.pyarrow import standardize_schema_timezones\n\nschema1 = pa.schema([(\"ts\", pa.timestamp(\"ns\", tz=\"UTC\"))])\nschema2 = pa.schema([(\"ts\", pa.timestamp(\"ns\"))]) # naive\nschemas = [schema1, schema2]\n\n# Remove timezones\nnew_schemas_naive = standardize_schema_timezones(schemas, timezone=None)\nprint(new_schemas_naive[0].field(\"ts\").type)\n# Expected: timestamp[ns]\n\n# Set a specific timezone\nnew_schemas_berlin = standardize_schema_timezones(schemas, timezone=\"Europe/Berlin\")\nprint(new_schemas_berlin[0].field(\"ts\").type)\n# Expected: timestamp[ns, tz=Europe/Berlin]\nReturns:\n\nlist[pyarrow.Schema]: New schemas with standardized timezone info.\n\n\n\n\nUnify a list of PyArrow schemas into a single schema.\nParameters:\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nschemas\nlist[pyarrow.Schema]\nList of PyArrow schemas to unify.\n\n\nuse_large_dtypes\nbool\nIf True, keep large types like large_string.\n\n\ntimezone\nstr or None\nIf specified, standardize all timestamp columns to this timezone. If “auto”, use the most frequent timezone across schemas. If None, remove timezone from all timestamp columns.\n\n\nstandardize_timezones\nbool\nIf True, standardize all timestamp columns to the most frequent timezone.\n\n\n\nReturns:\n\npyarrow.Schema: A unified PyArrow schema.\n\n\n\n\nCast a PyArrow table to a given schema, updating the schema to match the table’s columns.\nParameters:\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\ntable\npyarrow.Table\nThe PyArrow table to cast.\n\n\nschema\npyarrow.Schema\nThe target schema to cast the table to.\n\n\n\nReturns:\n\npyarrow.Table: A new PyArrow table with the specified schema.\n\n\n\n\nConvert large types in a PyArrow schema to their standard types.\nParameters:\n\n\n\nName\nType\nDescription\n\n\n\n\nschema\npyarrow.Schema\nThe PyArrow schema to convert.\n\n\n\nReturns:\n\npyarrow.Schema: A new PyArrow schema with large types converted to standard types.\n\n\n\n\nOptimize data types of a PyArrow Table for performance and memory efficiency.\nParameters:\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\ntable\npyarrow.Table\n\n\n\ninclude\nlist[str], optional\n\n\n\nexclude\nlist[str], optional\n\n\n\ntime_zone\nstr, optional\n\n\n\nshrink_numerics\nbool\n\n\n\nallow_unsigned\nbool\n\n\n\nuse_large_dtypes\nbool\n\n\n\nstrict\nbool\n\n\n\nallow_null\nbool\nIf False, columns that only hold null-like values will not be converted to pyarrow.null().\n\n\n\nReturns:\n\npyarrow.Table: A new table casted to the optimal schema."
  },
  {
    "objectID": "api/fsspec_utils.utils.pyarrow.html#dominant_timezone_per_column",
    "href": "api/fsspec_utils.utils.pyarrow.html#dominant_timezone_per_column",
    "title": "fsspec_utils.utils.pyarrow API Reference",
    "section": "",
    "text": "For each timestamp column (by name) across all schemas, detect the most frequent timezone (including None).\nIf None and a timezone are tied, prefer the timezone. Returns a dict: {column_name: dominant_timezone}\nParameters:\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nschemas\nlist[pyarrow.Schema]\nA list of PyArrow schemas to analyze.\n\n\n\nExample:\nimport pyarrow as pa\nfrom fsspec_utils.utils.pyarrow import dominant_timezone_per_column\n\nschema1 = pa.schema([(\"ts\", pa.timestamp(\"ns\", tz=\"UTC\"))])\nschema2 = pa.schema([(\"ts\", pa.timestamp(\"ns\", tz=\"Europe/Berlin\"))])\nschema3 = pa.schema([(\"ts\", pa.timestamp(\"ns\"))]) # naive\nschemas = [schema1, schema2, schema3]\n\ndominant_tz = dominant_timezone_per_column(schemas)\nprint(dominant_tz)\n# Expected: {'ts': 'UTC'} (or 'Europe/Berlin' depending on logic)\nReturns:\n\ndict: {column_name: dominant_timezone}"
  },
  {
    "objectID": "api/fsspec_utils.utils.pyarrow.html#standardize_schema_timezones_by_majority",
    "href": "api/fsspec_utils.utils.pyarrow.html#standardize_schema_timezones_by_majority",
    "title": "fsspec_utils.utils.pyarrow API Reference",
    "section": "",
    "text": "For each timestamp column (by name) across all schemas, set the timezone to the most frequent (with tie-breaking).\nParameters:\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nschemas\nlist[pyarrow.Schema]\nA list of PyArrow schemas to standardize.\n\n\n\nExample:\nimport pyarrow as pa\nfrom fsspec_utils.utils.pyarrow import standardize_schema_timezones_by_majority\n\nschema1 = pa.schema([(\"ts\", pa.timestamp(\"ns\", tz=\"UTC\"))])\nschema2 = pa.schema([(\"ts\", pa.timestamp(\"ns\", tz=\"Europe/Berlin\"))])\nschemas = [schema1, schema2]\n\nstandardized_schemas = standardize_schema_timezones_by_majority(schemas)\nprint(standardized_schemas[0].field(\"ts\").type)\nprint(standardized_schemas[1].field(\"ts\").type)\n# Expected: timestamp[ns, tz=Europe/Berlin] (or UTC, depending on tie-breaking)\nReturns:\n\nlist[pyarrow.Schema]: A new list of schemas with updated timestamp timezones."
  },
  {
    "objectID": "api/fsspec_utils.utils.pyarrow.html#standardize_schema_timezones",
    "href": "api/fsspec_utils.utils.pyarrow.html#standardize_schema_timezones",
    "title": "fsspec_utils.utils.pyarrow API Reference",
    "section": "",
    "text": "Standardize timezone info for all timestamp columns in a list of PyArrow schemas.\nParameters:\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nschemas\nlist[pyarrow.Schema]\nThe list of PyArrow schemas to process.\n\n\ntimezone\nstr or None\nThe target timezone to apply to timestamp columns. If None, timezones are removed. If “auto”, the most frequent timezone across schemas is used.\n\n\n\nExample:\nimport pyarrow as pa\nfrom fsspec_utils.utils.pyarrow import standardize_schema_timezones\n\nschema1 = pa.schema([(\"ts\", pa.timestamp(\"ns\", tz=\"UTC\"))])\nschema2 = pa.schema([(\"ts\", pa.timestamp(\"ns\"))]) # naive\nschemas = [schema1, schema2]\n\n# Remove timezones\nnew_schemas_naive = standardize_schema_timezones(schemas, timezone=None)\nprint(new_schemas_naive[0].field(\"ts\").type)\n# Expected: timestamp[ns]\n\n# Set a specific timezone\nnew_schemas_berlin = standardize_schema_timezones(schemas, timezone=\"Europe/Berlin\")\nprint(new_schemas_berlin[0].field(\"ts\").type)\n# Expected: timestamp[ns, tz=Europe/Berlin]\nReturns:\n\nlist[pyarrow.Schema]: New schemas with standardized timezone info."
  },
  {
    "objectID": "api/fsspec_utils.utils.pyarrow.html#unify_schemas",
    "href": "api/fsspec_utils.utils.pyarrow.html#unify_schemas",
    "title": "fsspec_utils.utils.pyarrow API Reference",
    "section": "",
    "text": "Unify a list of PyArrow schemas into a single schema.\nParameters:\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nschemas\nlist[pyarrow.Schema]\nList of PyArrow schemas to unify.\n\n\nuse_large_dtypes\nbool\nIf True, keep large types like large_string.\n\n\ntimezone\nstr or None\nIf specified, standardize all timestamp columns to this timezone. If “auto”, use the most frequent timezone across schemas. If None, remove timezone from all timestamp columns.\n\n\nstandardize_timezones\nbool\nIf True, standardize all timestamp columns to the most frequent timezone.\n\n\n\nReturns:\n\npyarrow.Schema: A unified PyArrow schema."
  },
  {
    "objectID": "api/fsspec_utils.utils.pyarrow.html#cast_schema",
    "href": "api/fsspec_utils.utils.pyarrow.html#cast_schema",
    "title": "fsspec_utils.utils.pyarrow API Reference",
    "section": "",
    "text": "Cast a PyArrow table to a given schema, updating the schema to match the table’s columns.\nParameters:\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\ntable\npyarrow.Table\nThe PyArrow table to cast.\n\n\nschema\npyarrow.Schema\nThe target schema to cast the table to.\n\n\n\nReturns:\n\npyarrow.Table: A new PyArrow table with the specified schema."
  },
  {
    "objectID": "api/fsspec_utils.utils.pyarrow.html#convert_large_types_to_normal",
    "href": "api/fsspec_utils.utils.pyarrow.html#convert_large_types_to_normal",
    "title": "fsspec_utils.utils.pyarrow API Reference",
    "section": "",
    "text": "Convert large types in a PyArrow schema to their standard types.\nParameters:\n\n\n\nName\nType\nDescription\n\n\n\n\nschema\npyarrow.Schema\nThe PyArrow schema to convert.\n\n\n\nReturns:\n\npyarrow.Schema: A new PyArrow schema with large types converted to standard types."
  },
  {
    "objectID": "api/fsspec_utils.utils.pyarrow.html#opt_dtype",
    "href": "api/fsspec_utils.utils.pyarrow.html#opt_dtype",
    "title": "fsspec_utils.utils.pyarrow API Reference",
    "section": "",
    "text": "Optimize data types of a PyArrow Table for performance and memory efficiency.\nParameters:\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\ntable\npyarrow.Table\n\n\n\ninclude\nlist[str], optional\n\n\n\nexclude\nlist[str], optional\n\n\n\ntime_zone\nstr, optional\n\n\n\nshrink_numerics\nbool\n\n\n\nallow_unsigned\nbool\n\n\n\nuse_large_dtypes\nbool\n\n\n\nstrict\nbool\n\n\n\nallow_null\nbool\nIf False, columns that only hold null-like values will not be converted to pyarrow.null().\n\n\n\nReturns:\n\npyarrow.Table: A new table casted to the optimal schema."
  },
  {
    "objectID": "api/fsspec_utils.utils.datetime.html",
    "href": "api/fsspec_utils.utils.datetime.html",
    "title": "fsspec_utils.utils.datetime API Reference",
    "section": "",
    "text": "Parameters:\n\n\n\nName\nType\nDescription\n\n\n\n\ndf\npolars.DataFrame\nInput DataFrame.\n\n\n\nExample:\nimport polars as pl\nfrom fsspec_utils.utils.datetime import get_timestamp_column\n\ndf = pl.DataFrame({\n    \"timestamp_col\": [1678886400, 1678972800],\n    \"value\": [10, 20]\n})\ncol_name = get_timestamp_column(df)\nprint(col_name)\n# \"timestamp_col\"\nReturns:\n\nNone\n\n\n\n\nParameters:\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\ntimedelta_string\nstr\nTimedelta string (e.g., “1h”, “2d”, “3w”).\n\n\n\nExample:\nfrom fsspec_utils.utils.datetime import get_timedelta_str\n\n# Convert to Polars duration string\npolars_duration = get_timedelta_str(\"1h\")\nprint(polars_duration)\n# \"1h\"\n\n# Convert to Pandas timedelta string\npandas_timedelta = get_timedelta_str(\"2d\", to=\"pandas\")\nprint(pandas_timedelta)\n# \"2 days\"\nto | str | Defaults to ‘polars’ |\nReturns:\n\nNone\n\n\n\n\nConverts a timestamp string (ISO 8601 format) into a datetime, date, or time object\nusing only standard Python libraries. Handles strings with or without timezone information (e.g., ‘2023-01-01T10:00:00+02:00’, ‘2023-01-01’, ‘10:00:00’). Supports timezone offsets like ‘+HH:MM’ or ‘+HHMM’. For named timezones (e.g., ‘Europe/Paris’), requires Python 3.9+ and the ‘tzdata’ package to be installed.\nParameters:\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\ntimestamp_str\nstr\nThe string representation of the timestamp (ISO 8601 format).\n\n\ntz\nstr, optional\nTarget timezone identifier (e.g., ‘UTC’, ‘+02:00’, ‘Europe/Paris’). If provided, the output datetime/time will be localized or converted to this timezone. Defaults to None.\n\n\nnaive\nbool, optional\nIf True, return a naive datetime/time (no timezone info), even if the input string or tz parameter specifies one. Defaults to False.\n\n\n\nReturns:\n\nUnion[dt.datetime, dt.date, dt.time]: The parsed datetime, date, or time object.\n\nExample:\nfrom fsspec_utils.utils.datetime import timestamp_from_string\n\n# Parse a timestamp string with timezone\ndt_obj = timestamp_from_string(\"2023-01-01T10:00:00+02:00\")\nprint(dt_obj)\n# 2023-01-01 10:00:00+02:00\n\n# Parse a date string\ndate_obj = timestamp_from_string(\"2023-01-01\")\nprint(date_obj)\n# 2023-01-01\n\n# Parse a time string and localize to UTC\ntime_obj = timestamp_from_string(\"15:30:00\", tz=\"UTC\")\nprint(time_obj)\n# 15:30:00+00:00\n\n# Parse a timestamp and return as naive datetime\nnaive_dt_obj = timestamp_from_string(\"2023-01-01T10:00:00+02:00\", naive=True)\nprint(naive_dt_obj)\n# 2023-01-01 10:00:00\nRaises:\n\nValueError: If the timestamp string format is invalid or the timezone is invalid/unsupported."
  },
  {
    "objectID": "api/fsspec_utils.utils.datetime.html#get_timestamp_column",
    "href": "api/fsspec_utils.utils.datetime.html#get_timestamp_column",
    "title": "fsspec_utils.utils.datetime API Reference",
    "section": "",
    "text": "Parameters:\n\n\n\nName\nType\nDescription\n\n\n\n\ndf\npolars.DataFrame\nInput DataFrame.\n\n\n\nExample:\nimport polars as pl\nfrom fsspec_utils.utils.datetime import get_timestamp_column\n\ndf = pl.DataFrame({\n    \"timestamp_col\": [1678886400, 1678972800],\n    \"value\": [10, 20]\n})\ncol_name = get_timestamp_column(df)\nprint(col_name)\n# \"timestamp_col\"\nReturns:\n\nNone"
  },
  {
    "objectID": "api/fsspec_utils.utils.datetime.html#get_timedelta_str",
    "href": "api/fsspec_utils.utils.datetime.html#get_timedelta_str",
    "title": "fsspec_utils.utils.datetime API Reference",
    "section": "",
    "text": "Parameters:\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\ntimedelta_string\nstr\nTimedelta string (e.g., “1h”, “2d”, “3w”).\n\n\n\nExample:\nfrom fsspec_utils.utils.datetime import get_timedelta_str\n\n# Convert to Polars duration string\npolars_duration = get_timedelta_str(\"1h\")\nprint(polars_duration)\n# \"1h\"\n\n# Convert to Pandas timedelta string\npandas_timedelta = get_timedelta_str(\"2d\", to=\"pandas\")\nprint(pandas_timedelta)\n# \"2 days\"\nto | str | Defaults to ‘polars’ |\nReturns:\n\nNone"
  },
  {
    "objectID": "api/fsspec_utils.utils.datetime.html#timestamp_from_string",
    "href": "api/fsspec_utils.utils.datetime.html#timestamp_from_string",
    "title": "fsspec_utils.utils.datetime API Reference",
    "section": "",
    "text": "Converts a timestamp string (ISO 8601 format) into a datetime, date, or time object\nusing only standard Python libraries. Handles strings with or without timezone information (e.g., ‘2023-01-01T10:00:00+02:00’, ‘2023-01-01’, ‘10:00:00’). Supports timezone offsets like ‘+HH:MM’ or ‘+HHMM’. For named timezones (e.g., ‘Europe/Paris’), requires Python 3.9+ and the ‘tzdata’ package to be installed.\nParameters:\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\ntimestamp_str\nstr\nThe string representation of the timestamp (ISO 8601 format).\n\n\ntz\nstr, optional\nTarget timezone identifier (e.g., ‘UTC’, ‘+02:00’, ‘Europe/Paris’). If provided, the output datetime/time will be localized or converted to this timezone. Defaults to None.\n\n\nnaive\nbool, optional\nIf True, return a naive datetime/time (no timezone info), even if the input string or tz parameter specifies one. Defaults to False.\n\n\n\nReturns:\n\nUnion[dt.datetime, dt.date, dt.time]: The parsed datetime, date, or time object.\n\nExample:\nfrom fsspec_utils.utils.datetime import timestamp_from_string\n\n# Parse a timestamp string with timezone\ndt_obj = timestamp_from_string(\"2023-01-01T10:00:00+02:00\")\nprint(dt_obj)\n# 2023-01-01 10:00:00+02:00\n\n# Parse a date string\ndate_obj = timestamp_from_string(\"2023-01-01\")\nprint(date_obj)\n# 2023-01-01\n\n# Parse a time string and localize to UTC\ntime_obj = timestamp_from_string(\"15:30:00\", tz=\"UTC\")\nprint(time_obj)\n# 15:30:00+00:00\n\n# Parse a timestamp and return as naive datetime\nnaive_dt_obj = timestamp_from_string(\"2023-01-01T10:00:00+02:00\", naive=True)\nprint(naive_dt_obj)\n# 2023-01-01 10:00:00\nRaises:\n\nValueError: If the timestamp string format is invalid or the timezone is invalid/unsupported."
  },
  {
    "objectID": "api/fsspec_utils.storage_options.git.html",
    "href": "api/fsspec_utils.storage_options.git.html",
    "title": "fsspec_utils.storage_options.git API Reference",
    "section": "",
    "text": "GitHub repository storage configuration options.\nProvides access to files in GitHub repositories with support for: - Public and private repositories - Branch/tag/commit selection - Token-based authentication - Custom GitHub Enterprise instances\nAttributes:\n\nprotocol (str): Always “github” for GitHub storage\norg (str): Organization or user name\nrepo (str): Repository name\nref (str): Git reference (branch, tag, or commit SHA\ntoken (str): GitHub personal access token\napi_url (str): Custom GitHub API URL for enterprise instances\n\nExample:\n# Public repository\noptions = GitHubStorageOptions(\n    org=\"microsoft\",\n    repo=\"vscode\",\n    ref=\"main\"\n)\n\n# Private repository\noptions = GitHubStorageOptions(\n    org=\"myorg\",\n    repo=\"private-repo\",\n    token=\"ghp_xxxx\",\n    ref=\"develop\"\n)\n\n# Enterprise instance\noptions = GitHubStorageOptions(\n    org=\"company\",\n    repo=\"internal\",\n    api_url=\"https://github.company.com/api/v3\",\n    token=\"ghp_xxxx\"\n)\n\n\nCreate storage options from environment variables.\nReads standard GitHub environment variables: - GITHUB_ORG: Organization or user name - GITHUB_REPO: Repository name - GITHUB_REF: Git reference - GITHUB_TOKEN: Personal access token - GITHUB_API_URL: Custom API URL\nReturns:\n\nGitHubStorageOptions: Configured storage options\n\nExample:\n# With environment variables set:\noptions = GitHubStorageOptions.from_env()\nprint(options.org)  # From GITHUB_ORG 'microsoft'\n\n\n\nExport options to environment variables.\nSets standard GitHub environment variables.\nExample:\noptions = GitHubStorageOptions(\n    org=\"microsoft\",\n    repo=\"vscode\",\n    token=\"ghp_xxxx\"\n)\noptions.to_env()\nprint(os.getenv(\"GITHUB_ORG\"))  # 'microsoft'\n\n\n\nConvert options to fsspec filesystem arguments.\nReturns:\n\ndict: Arguments suitable for GitHubFileSystem\n\nExample:\noptions = GitHubStorageOptions(\n    org=\"microsoft\",\n    repo=\"vscode\",\n    token=\"ghp_xxxx\"\n)\nkwargs = options.to_fsspec_kwargs()\nfs = filesystem(\"github\", **kwargs)\n\n\n\n\nGitLab repository storage configuration options.\nProvides access to files in GitLab repositories with support for: - Public and private repositories - Self-hosted GitLab instances - Project ID or name-based access - Branch/tag/commit selection - Token-based authentication\nAttributes:\n\nprotocol (str): Always “gitlab” for GitLab storage\nbase_url (str): GitLab instance URL, defaults to gitlab.com\nproject_id (str | int): Project ID number\nproject_name (str): Project name/path\nref (str): Git reference (branch, tag, or commit SHA)\ntoken (str): GitLab personal access token\napi_version (str): API version to use\n\nExample:\n# Public project on gitlab.com\noptions = GitLabStorageOptions(\n    project_name=\"group/project\",\n    ref=\"main\"\n)\n\n# Private project with token\noptions = GitLabStorageOptions(\n    project_id=12345,\n    token=\"glpat_xxxx\",\n    ref=\"develop\"\n)\n\n# Self-hosted instance\noptions = GitLabStorageOptions(\n    base_url=\"https://gitlab.company.com\",\n    project_name=\"internal/project\",\n    token=\"glpat_xxxx\"\n)\n\n\nCreate storage options from environment variables.\nReads standard GitLab environment variables: - GITLAB_URL: Instance URL - GITLAB_PROJECT_ID: Project ID - GITLAB_PROJECT_NAME: Project name/path - GITLAB_REF: Git reference - GITLAB_TOKEN: Personal access token - GITLAB_API_VERSION: API version\nReturns:\n\nGitLabStorageOptions: Configured storage options\n\nExample:\n# With environment variables set:\noptions = GitLabStorageOptions.from_env()\nprint(options.project_id)  # From GITLAB_PROJECT_ID '12345'\n\n\n\nExport options to environment variables.\nSets standard GitLab environment variables.\nExample:\noptions = GitLabStorageOptions(\n    project_id=12345,\n    token=\"glpat_xxxx\"\n)\noptions.to_env()\nprint(os.getenv(\"GITLAB_PROJECT_ID\"))  # '12345'\n\n\n\nConvert options to fsspec filesystem arguments.\nReturns:\n\ndict: Arguments suitable for GitLabFileSystem\n\nExample:\noptions = GitLabStorageOptions(\n    project_id=12345,\n    token=\"glpat_xxxx\"\n)\nkwargs = options.to_fsspec_kwargs()\nfs = filesystem(\"gitlab\", **kwargs)"
  },
  {
    "objectID": "api/fsspec_utils.storage_options.git.html#githubstorageoptions",
    "href": "api/fsspec_utils.storage_options.git.html#githubstorageoptions",
    "title": "fsspec_utils.storage_options.git API Reference",
    "section": "",
    "text": "GitHub repository storage configuration options.\nProvides access to files in GitHub repositories with support for: - Public and private repositories - Branch/tag/commit selection - Token-based authentication - Custom GitHub Enterprise instances\nAttributes:\n\nprotocol (str): Always “github” for GitHub storage\norg (str): Organization or user name\nrepo (str): Repository name\nref (str): Git reference (branch, tag, or commit SHA\ntoken (str): GitHub personal access token\napi_url (str): Custom GitHub API URL for enterprise instances\n\nExample:\n# Public repository\noptions = GitHubStorageOptions(\n    org=\"microsoft\",\n    repo=\"vscode\",\n    ref=\"main\"\n)\n\n# Private repository\noptions = GitHubStorageOptions(\n    org=\"myorg\",\n    repo=\"private-repo\",\n    token=\"ghp_xxxx\",\n    ref=\"develop\"\n)\n\n# Enterprise instance\noptions = GitHubStorageOptions(\n    org=\"company\",\n    repo=\"internal\",\n    api_url=\"https://github.company.com/api/v3\",\n    token=\"ghp_xxxx\"\n)\n\n\nCreate storage options from environment variables.\nReads standard GitHub environment variables: - GITHUB_ORG: Organization or user name - GITHUB_REPO: Repository name - GITHUB_REF: Git reference - GITHUB_TOKEN: Personal access token - GITHUB_API_URL: Custom API URL\nReturns:\n\nGitHubStorageOptions: Configured storage options\n\nExample:\n# With environment variables set:\noptions = GitHubStorageOptions.from_env()\nprint(options.org)  # From GITHUB_ORG 'microsoft'\n\n\n\nExport options to environment variables.\nSets standard GitHub environment variables.\nExample:\noptions = GitHubStorageOptions(\n    org=\"microsoft\",\n    repo=\"vscode\",\n    token=\"ghp_xxxx\"\n)\noptions.to_env()\nprint(os.getenv(\"GITHUB_ORG\"))  # 'microsoft'\n\n\n\nConvert options to fsspec filesystem arguments.\nReturns:\n\ndict: Arguments suitable for GitHubFileSystem\n\nExample:\noptions = GitHubStorageOptions(\n    org=\"microsoft\",\n    repo=\"vscode\",\n    token=\"ghp_xxxx\"\n)\nkwargs = options.to_fsspec_kwargs()\nfs = filesystem(\"github\", **kwargs)"
  },
  {
    "objectID": "api/fsspec_utils.storage_options.git.html#gitlabstorageoptions",
    "href": "api/fsspec_utils.storage_options.git.html#gitlabstorageoptions",
    "title": "fsspec_utils.storage_options.git API Reference",
    "section": "",
    "text": "GitLab repository storage configuration options.\nProvides access to files in GitLab repositories with support for: - Public and private repositories - Self-hosted GitLab instances - Project ID or name-based access - Branch/tag/commit selection - Token-based authentication\nAttributes:\n\nprotocol (str): Always “gitlab” for GitLab storage\nbase_url (str): GitLab instance URL, defaults to gitlab.com\nproject_id (str | int): Project ID number\nproject_name (str): Project name/path\nref (str): Git reference (branch, tag, or commit SHA)\ntoken (str): GitLab personal access token\napi_version (str): API version to use\n\nExample:\n# Public project on gitlab.com\noptions = GitLabStorageOptions(\n    project_name=\"group/project\",\n    ref=\"main\"\n)\n\n# Private project with token\noptions = GitLabStorageOptions(\n    project_id=12345,\n    token=\"glpat_xxxx\",\n    ref=\"develop\"\n)\n\n# Self-hosted instance\noptions = GitLabStorageOptions(\n    base_url=\"https://gitlab.company.com\",\n    project_name=\"internal/project\",\n    token=\"glpat_xxxx\"\n)\n\n\nCreate storage options from environment variables.\nReads standard GitLab environment variables: - GITLAB_URL: Instance URL - GITLAB_PROJECT_ID: Project ID - GITLAB_PROJECT_NAME: Project name/path - GITLAB_REF: Git reference - GITLAB_TOKEN: Personal access token - GITLAB_API_VERSION: API version\nReturns:\n\nGitLabStorageOptions: Configured storage options\n\nExample:\n# With environment variables set:\noptions = GitLabStorageOptions.from_env()\nprint(options.project_id)  # From GITLAB_PROJECT_ID '12345'\n\n\n\nExport options to environment variables.\nSets standard GitLab environment variables.\nExample:\noptions = GitLabStorageOptions(\n    project_id=12345,\n    token=\"glpat_xxxx\"\n)\noptions.to_env()\nprint(os.getenv(\"GITLAB_PROJECT_ID\"))  # '12345'\n\n\n\nConvert options to fsspec filesystem arguments.\nReturns:\n\ndict: Arguments suitable for GitLabFileSystem\n\nExample:\noptions = GitLabStorageOptions(\n    project_id=12345,\n    token=\"glpat_xxxx\"\n)\nkwargs = options.to_fsspec_kwargs()\nfs = filesystem(\"gitlab\", **kwargs)"
  },
  {
    "objectID": "api/fsspec_utils.core.base.html",
    "href": "api/fsspec_utils.core.base.html",
    "title": "fsspec_utils.core.base API Documentation",
    "section": "",
    "text": "This module provides core filesystem functionalities and utilities, including custom cache mappers, enhanced cached filesystems, and a GitLab filesystem implementation.\n\n\n\nMaps remote file paths to local cache paths while preserving directory structure.\nThis cache mapper maintains the original file path structure in the cache directory, creating necessary subdirectories as needed.\nAttributes:\n\ndirectory (str): Base directory for cached files\n\nExample:\nfrom fsspec_utils.core.base import FileNameCacheMapper\n\n# Create cache mapper for S3 files\nmapper = FileNameCacheMapper(\"/tmp/cache\")\n\n# Map remote path to cache path\ncache_path = mapper(\"bucket/data/file.csv\")\nprint(cache_path)  # Preserves structure\n\n\nInitialize cache mapper with base directory.\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\n\n\n\n\ndirectory\nstr\nBase directory where cached files will be stored\n\n\n\n\n\n\nMap remote file path to cache file path.\nCreates necessary subdirectories in the cache directory to maintain the original path structure.\n\n\n\nParameter\nType\nDescription\n\n\n\n\npath\nstr\nOriginal file path from remote filesystem\n\n\n\n\n\n\nReturns\nType\nDescription\n\n\n\n\nstr\nstr\nCache file path that preserves original structure\n\n\n\nExample:\nfrom fsspec_utils.core.base import FileNameCacheMapper\n\nmapper = FileNameCacheMapper(\"/tmp/cache\")\n# Maps maintain directory structure\nprint(mapper(\"data/nested/file.txt\"))\n\n\n\n\n\nEnhanced caching filesystem with monitoring and improved path handling.\nThis filesystem extends SimpleCacheFileSystem to provide:\n\nVerbose logging of cache operations\nImproved path mapping for cache files\nEnhanced synchronization capabilities\nBetter handling of parallel operations\n\nAttributes:\n\n_verbose (bool): Whether to print verbose cache operations\n_mapper (FileNameCacheMapper): Maps remote paths to cache paths\nstorage (list[str]): List of cache storage locations\nfs (AbstractFileSystem): Underlying filesystem being cached\n\nExample:\nfrom fsspec import filesystem\nfrom fsspec_utils.core.base import MonitoredSimpleCacheFileSystem\n\ns3_fs = filesystem(\"s3\")\ncached_fs = MonitoredSimpleCacheFileSystem(\n    fs=s3_fs,\n    cache_storage=\"/tmp/cache\",\n    verbose=True\n)\n# Use cached_fs like any other filesystem\nfiles = cached_fs.ls(\"my-bucket/\")\n\n\nInitialize monitored cache filesystem.\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\n\n\n\n\nfs\nOptional[fsspec.AbstractFileSystem]\nUnderlying filesystem to cache. If None, creates a local filesystem.\n\n\ncache_storage\nUnion[str, list[str]]\nCache storage location(s). Can be string path or list of paths.\n\n\nverbose\nbool\nWhether to enable verbose logging of cache operations.\n\n\n**kwargs\nAny\nAdditional arguments passed to SimpleCacheFileSystem.\n\n\n\nExample:\n# Cache S3 filesystem\ns3_fs = filesystem(\"s3\")\ncached = MonitoredSimpleCacheFileSystem(\n    fs=s3_fs,\n    cache_storage=\"/tmp/s3_cache\",\n    verbose=True\n)\n\n\n\nCheck if file exists in cache and return cache path if found.\n\n\n\nParameter\nType\nDescription\n\n\n\n\npath\nstr\nRemote file path to check\n\n\n\n\n\n\n\n\n\n\n\nReturns\nType\nDescript\n\n\n\n\nOptional[str]\nstr or None\nCache file path if found, None otherwise\n\n\n\nExample:\nfrom fsspec_utils.core.base import MonitoredSimpleCacheFileSystem\nfrom fsspec import filesystem\n\ncached_fs = MonitoredSimpleCacheFileSystem(fs=filesystem(\"s3\"), cache_storage=\"/tmp/cache\")\n# Check if a file is in cache\ncache_path = cached_fs._check_cache(\"my-bucket/data/file.txt\")\nif cache_path:\n    print(f\"File found in cache at: {cache_path}\")\nelse:\n    print(\"File not in cache.\")\n\n\n\nEnsure file is in cache, downloading if necessary.\n\n\n\nParameter\nType\nDescription\n\n\n\n\npath\nstr\nRemote file path\n\n\n\n\n\n\nReturns\nType\nDescription\n\n\n\n\nstr\nstr\nLocal cache path for the file\n\n\n\nExample:\nfrom fsspec_utils.core.base import MonitoredSimpleCacheFileSystem\nfrom fsspec import filesystem\n\ncached_fs = MonitoredSimpleCacheFileSystem(fs=filesystem(\"s3\"), cache_storage=\"/tmp/cache\")\n# Ensure file is in cache (downloads if not present)\nlocal_path = cached_fs._check_file(\"my-bucket/data/large_file.parquet\")\nprint(f\"File available locally at: {local_path}\")\n\n\n\n\n\nFilesystem interface for GitLab repositories.\nProvides read-only access to files in GitLab repositories, including:\n\nPublic and private repositories\nSelf-hosted GitLab instances\nBranch/tag/commit selection\nToken-based authentication\n\nAttributes:\n\nprotocol (str): Always “gitlab”\nbase_url (str): GitLab instance URL\nproject_id (str): Project ID\nproject_name (str): Project name/path\nref (str): Git reference (branch, tag, commit)\ntoken (str): Access token\napi_version (str): API version\n\nExample:\n# Public repository\nfs = GitLabFileSystem(\n    project_name=\"group/project\",\n    ref=\"main\"\n)\nfiles = fs.ls(\"/\")\n\n# Private repository with token\nfs = GitLabFileSystem(\n    project_id=\"12345\",\n    token=\"glpat_xxxx\",\n    ref=\"develop\"\n)\ncontent = fs.cat(\"README.md\")\n\n\nInitialize GitLab filesystem.\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\n\n\n\n\nbase_url\nstr\nGitLab instance URL\n\n\nproject_id\nOptional[Union[str, int]]\nProject ID number\n\n\nproject_name\nOptional[str]\nProject name/path (alternative to project_id)\n\n\nref\nstr\nGit reference (branch, tag, or commit SHA)\n\n\ntoken\nOptional[str]\nGitLab personal access token\n\n\napi_version\nstr\nAPI version to use\n\n\n\n**kwargs | Any | Additional filesystem arguments |\n\n\n\n\n\n\n\n\nRaises\nType\nDescription\n\n\n\n\nValueError\nValueError\nIf neither project_id nor project_name is provided\n\n\n\nExample:\nfrom fsspec_utils.core.base import GitLabFileSystem\n\n# Access a public repository\nfs_public = GitLabFileSystem(\n    project_name=\"gitlab-org/gitlab\",\n    ref=\"master\"\n)\nprint(fs_public.ls(\"README.md\"))\n\n# Access a private repository (replace with your token and project info)\n# fs_private = GitLabFileSystem(\n#     project_id=\"12345\",\n# #    token=\"your_private_token\",\n#     ref=\"main\"\n# )\n# print(fs_private.ls(\"/\"))\n\n\n\nGet file content from GitLab API.\n\n\n\nParameter\nType\nDescription\n\n\n\n\npath\nstr\nFile path in repository\n\n\n\n\n\n\nReturns\nType\nDescription\n\n\n\n\nbytes\nbytes\nFile content as bytes\n\n\n\nExample:\nfrom fsspec_utils.core.base import GitLabFileSystem\n\nfs = GitLabFileSystem(project_name=\"gitlab-org/gitlab\")\ncontent = fs.cat(\"README.md\")\nprint(content[:50])\n\n\n\nRaises\nType\nDescription\n\n\n\n\nFileNotFoundError\nFileNotFoundError\nIf file doesn’t exist\n\n\nrequests.HTTPError\nrequests.HTTPError\nFor other HTTP errors\n\n\n\n\n\n\nOpen file for reading.\n\n\n\nParameter\nType\nDescription\n\n\n\n\npath\nstr\nFile path to open\n\n\nmode\nstr\nFile mode (only ‘rb’ and ‘r’ supported)\n\n\nblock_size\nOptional[int]\nBlock size for reading (unused)\n\n\ncache_options\nOptional[dict]\nCache options (unused)\n\n\n**kwargs\nAny\nAdditional options\n\n\n\n\n\n\nReturns\nType\nDescription\n\n\n\n\nFile-like object\nFile-like object\nFile-like object for reading\n\n\n\n\n\n\nRaises\nType\nDescription\n\n\n\n\nValueError\nValueError\nIf mode is not supported\n\n\n\n\n\n\nGet file contents as bytes.\n\n\n\nParameter\nType\nDescription\n\n\n\n\npath\nstr\nFile path\n\n\n**kwargs\nAny\nAdditional options\n\n\n\n\n\n\nReturns\nType\nDescription\n\n\n\n\nbytes\nbytes\nFile content as bytes\n\n\n\n\n\n\nList directory contents.\n\n\n\nParameter\nType\nDescription\n\n\n\n\npath\nstr\nDirectory path to list\n\n\ndetail\nbool\nWhether to return detailed information\n\n\n**kwargs\nAny\nAdditional options\n\n\n\n\n\n\nReturns\nType\nDescription\n\n\n\n\nlist\nlist\nList of files/directories or their details\n\n\n\n\n\n\nCheck if file or directory exists.\n\n\n\nParameter\nType\nDescription\n\n\n\n\npath\nstr\nPath to check\n\n\n**kwargs\nAny\nAdditional options\n\n\n\n\n\n\nReturns\nType\nDescription\n\n\n\n\nbool\nbool\nTrue if path exists, False otherwise\n\n\n\n\n\n\nGet file information.\n\n\n\nParameter\nType\nDescription\n\n\n\n\npath\nstr\nFile path\n\n\n**kwargs\nAny\nAdditional options\n\n\n\n\n\n\nReturns\nType\nDescription\n\n\n\n\ndict\ndict\nDictionary with file information\n\n\n\n\n\n\nRaises\nType\nDescription\n\n\n\n\nFileNotFoundError\nFileNotFoundError\nIf file not found\n\n\n\n\n\n\n\n\nGet filesystem instance with enhanced configuration options.\nCreates filesystem instances with support for storage options classes, intelligent caching, and protocol inference from paths.\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\n\n\n\n\nprotocol_or_path\nstr\nFilesystem protocol (e.g., “s3”, “file”) or path with protocol prefix\n\n\nstorage_options\nOptional[Union[BaseStorageOptions, dict]]\nStorage configuration as BaseStorageOptions instance or dict\n\n\ncached\nbool\nWhether to wrap filesystem in caching layer\n\n\ncache_storage\nOptional[str]\nCache directory path (if cached=True)\n\n\nverbose\nbool\nEnable verbose logging for cache operations\n\n\ndirfs\nbool\nWhether to wrap the filesystem in a DirFileSystem. Defaults to True.\n\n\nbase_fs\nAbstractFileSystem\nAn existing filesystem to wrap.\n\n\n**kwargs\nAny\nAdditional filesystem arguments\n\n\n\n\n\n\n\n\n\n\n\nRet\n\n\n\n\n\n\nAbstractFileSystem\nfsspec.AbstractFileSystem\nConfigured filesystem instance\n\n\n\nExample:\n# Basic local filesystem\nfs = filesystem(\"file\")\n\n# S3 with storage options\nfrom fsspec_utils.storage_options.cloud import AwsStorageOptions\nopts = AwsStorageOptions(region=\"us-west-2\")\nfs = filesystem(\"s3\", storage_options=opts, cached=True)\n\n# Infer protocol from path\nfs = filesystem(\"s3://my-bucket/\", cached=True)\n\n# GitLab filesystem\nfs = filesystem(\"gitlab\", storage_options={\n    \"project_name\": \"group/project\",\n    \"token\": \"glpat_xxxx\"\n})\n\n\n\n\nGet filesystem instance with enhanced configuration options.\n\n\n\n\n\n\nDeprecated\n\n\n\nUse [`filesystem`](#filesystem) instead. This function will be removed in a future version.\n\n\nCreates filesystem instances with support for storage options classes, intelligent caching, and protocol inference from paths.\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\n\n\n\n\nprotocol_or_path\nstr\nFilesystem protocol (e.g., “s3”, “file”) or path with protocol prefix\n\n\nstorage_options\nOptional[Union[BaseStorageOptions, dict]]\nStorage configuration as BaseStorageOptions instance or dict\n\n\ncached\nbool\nWhether to wrap filesystem in caching layer\n\n\ncache_storage\nOptional[str]\nCache directory path (if cached=True)\n\n\nverbose\nbool\nEnable verbose logging for cache operations\n\n\n**kwargs\nAny\nAdditional filesystem arguments\n\n\n\n\n\n\n\n\n\n\n\nReturns\nType\nDescription\n\n\n\n\nfsspec.AbstractFileSystem\nfsspec.AbstractFileSystem\nConfigured filesystem instance\n\n\n\nExample:\n# Basic local filesystem\nfs = get_filesystem(\"file\")\n\n# S3 with storage options\nfrom fsspec_utils.storage_options.cloud import AwsStorageOptions\nopts = AwsStorageOptions(region=\"us-west-2\")\nfs = get_filesystem(\"s3\", storage_options=opts, cached=True)\n\n# Infer protocol from path\nfs = get_filesystem(\"s3://my-bucket/\", cached=True)\n\n# GitLab filesystem\nfs = get_filesystem(\"gitlab\", storage_options={\n    \"project_name\": \"group/project\",\n    \"token\": \"glpat_xxxx\"\n})"
  },
  {
    "objectID": "api/fsspec_utils.core.base.html#filenamecachemapper",
    "href": "api/fsspec_utils.core.base.html#filenamecachemapper",
    "title": "fsspec_utils.core.base API Documentation",
    "section": "",
    "text": "Maps remote file paths to local cache paths while preserving directory structure.\nThis cache mapper maintains the original file path structure in the cache directory, creating necessary subdirectories as needed.\nAttributes:\n\ndirectory (str): Base directory for cached files\n\nExample:\nfrom fsspec_utils.core.base import FileNameCacheMapper\n\n# Create cache mapper for S3 files\nmapper = FileNameCacheMapper(\"/tmp/cache\")\n\n# Map remote path to cache path\ncache_path = mapper(\"bucket/data/file.csv\")\nprint(cache_path)  # Preserves structure\n\n\nInitialize cache mapper with base directory.\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\n\n\n\n\ndirectory\nstr\nBase directory where cached files will be stored\n\n\n\n\n\n\nMap remote file path to cache file path.\nCreates necessary subdirectories in the cache directory to maintain the original path structure.\n\n\n\nParameter\nType\nDescription\n\n\n\n\npath\nstr\nOriginal file path from remote filesystem\n\n\n\n\n\n\nReturns\nType\nDescription\n\n\n\n\nstr\nstr\nCache file path that preserves original structure\n\n\n\nExample:\nfrom fsspec_utils.core.base import FileNameCacheMapper\n\nmapper = FileNameCacheMapper(\"/tmp/cache\")\n# Maps maintain directory structure\nprint(mapper(\"data/nested/file.txt\"))"
  },
  {
    "objectID": "api/fsspec_utils.core.base.html#monitoredsimplecachefilesystem",
    "href": "api/fsspec_utils.core.base.html#monitoredsimplecachefilesystem",
    "title": "fsspec_utils.core.base API Documentation",
    "section": "",
    "text": "Enhanced caching filesystem with monitoring and improved path handling.\nThis filesystem extends SimpleCacheFileSystem to provide:\n\nVerbose logging of cache operations\nImproved path mapping for cache files\nEnhanced synchronization capabilities\nBetter handling of parallel operations\n\nAttributes:\n\n_verbose (bool): Whether to print verbose cache operations\n_mapper (FileNameCacheMapper): Maps remote paths to cache paths\nstorage (list[str]): List of cache storage locations\nfs (AbstractFileSystem): Underlying filesystem being cached\n\nExample:\nfrom fsspec import filesystem\nfrom fsspec_utils.core.base import MonitoredSimpleCacheFileSystem\n\ns3_fs = filesystem(\"s3\")\ncached_fs = MonitoredSimpleCacheFileSystem(\n    fs=s3_fs,\n    cache_storage=\"/tmp/cache\",\n    verbose=True\n)\n# Use cached_fs like any other filesystem\nfiles = cached_fs.ls(\"my-bucket/\")\n\n\nInitialize monitored cache filesystem.\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\n\n\n\n\nfs\nOptional[fsspec.AbstractFileSystem]\nUnderlying filesystem to cache. If None, creates a local filesystem.\n\n\ncache_storage\nUnion[str, list[str]]\nCache storage location(s). Can be string path or list of paths.\n\n\nverbose\nbool\nWhether to enable verbose logging of cache operations.\n\n\n**kwargs\nAny\nAdditional arguments passed to SimpleCacheFileSystem.\n\n\n\nExample:\n# Cache S3 filesystem\ns3_fs = filesystem(\"s3\")\ncached = MonitoredSimpleCacheFileSystem(\n    fs=s3_fs,\n    cache_storage=\"/tmp/s3_cache\",\n    verbose=True\n)\n\n\n\nCheck if file exists in cache and return cache path if found.\n\n\n\nParameter\nType\nDescription\n\n\n\n\npath\nstr\nRemote file path to check\n\n\n\n\n\n\n\n\n\n\n\nReturns\nType\nDescript\n\n\n\n\nOptional[str]\nstr or None\nCache file path if found, None otherwise\n\n\n\nExample:\nfrom fsspec_utils.core.base import MonitoredSimpleCacheFileSystem\nfrom fsspec import filesystem\n\ncached_fs = MonitoredSimpleCacheFileSystem(fs=filesystem(\"s3\"), cache_storage=\"/tmp/cache\")\n# Check if a file is in cache\ncache_path = cached_fs._check_cache(\"my-bucket/data/file.txt\")\nif cache_path:\n    print(f\"File found in cache at: {cache_path}\")\nelse:\n    print(\"File not in cache.\")\n\n\n\nEnsure file is in cache, downloading if necessary.\n\n\n\nParameter\nType\nDescription\n\n\n\n\npath\nstr\nRemote file path\n\n\n\n\n\n\nReturns\nType\nDescription\n\n\n\n\nstr\nstr\nLocal cache path for the file\n\n\n\nExample:\nfrom fsspec_utils.core.base import MonitoredSimpleCacheFileSystem\nfrom fsspec import filesystem\n\ncached_fs = MonitoredSimpleCacheFileSystem(fs=filesystem(\"s3\"), cache_storage=\"/tmp/cache\")\n# Ensure file is in cache (downloads if not present)\nlocal_path = cached_fs._check_file(\"my-bucket/data/large_file.parquet\")\nprint(f\"File available locally at: {local_path}\")"
  },
  {
    "objectID": "api/fsspec_utils.core.base.html#gitlabfilesystem",
    "href": "api/fsspec_utils.core.base.html#gitlabfilesystem",
    "title": "fsspec_utils.core.base API Documentation",
    "section": "",
    "text": "Filesystem interface for GitLab repositories.\nProvides read-only access to files in GitLab repositories, including:\n\nPublic and private repositories\nSelf-hosted GitLab instances\nBranch/tag/commit selection\nToken-based authentication\n\nAttributes:\n\nprotocol (str): Always “gitlab”\nbase_url (str): GitLab instance URL\nproject_id (str): Project ID\nproject_name (str): Project name/path\nref (str): Git reference (branch, tag, commit)\ntoken (str): Access token\napi_version (str): API version\n\nExample:\n# Public repository\nfs = GitLabFileSystem(\n    project_name=\"group/project\",\n    ref=\"main\"\n)\nfiles = fs.ls(\"/\")\n\n# Private repository with token\nfs = GitLabFileSystem(\n    project_id=\"12345\",\n    token=\"glpat_xxxx\",\n    ref=\"develop\"\n)\ncontent = fs.cat(\"README.md\")\n\n\nInitialize GitLab filesystem.\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\n\n\n\n\nbase_url\nstr\nGitLab instance URL\n\n\nproject_id\nOptional[Union[str, int]]\nProject ID number\n\n\nproject_name\nOptional[str]\nProject name/path (alternative to project_id)\n\n\nref\nstr\nGit reference (branch, tag, or commit SHA)\n\n\ntoken\nOptional[str]\nGitLab personal access token\n\n\napi_version\nstr\nAPI version to use\n\n\n\n**kwargs | Any | Additional filesystem arguments |\n\n\n\n\n\n\n\n\nRaises\nType\nDescription\n\n\n\n\nValueError\nValueError\nIf neither project_id nor project_name is provided\n\n\n\nExample:\nfrom fsspec_utils.core.base import GitLabFileSystem\n\n# Access a public repository\nfs_public = GitLabFileSystem(\n    project_name=\"gitlab-org/gitlab\",\n    ref=\"master\"\n)\nprint(fs_public.ls(\"README.md\"))\n\n# Access a private repository (replace with your token and project info)\n# fs_private = GitLabFileSystem(\n#     project_id=\"12345\",\n# #    token=\"your_private_token\",\n#     ref=\"main\"\n# )\n# print(fs_private.ls(\"/\"))\n\n\n\nGet file content from GitLab API.\n\n\n\nParameter\nType\nDescription\n\n\n\n\npath\nstr\nFile path in repository\n\n\n\n\n\n\nReturns\nType\nDescription\n\n\n\n\nbytes\nbytes\nFile content as bytes\n\n\n\nExample:\nfrom fsspec_utils.core.base import GitLabFileSystem\n\nfs = GitLabFileSystem(project_name=\"gitlab-org/gitlab\")\ncontent = fs.cat(\"README.md\")\nprint(content[:50])\n\n\n\nRaises\nType\nDescription\n\n\n\n\nFileNotFoundError\nFileNotFoundError\nIf file doesn’t exist\n\n\nrequests.HTTPError\nrequests.HTTPError\nFor other HTTP errors\n\n\n\n\n\n\nOpen file for reading.\n\n\n\nParameter\nType\nDescription\n\n\n\n\npath\nstr\nFile path to open\n\n\nmode\nstr\nFile mode (only ‘rb’ and ‘r’ supported)\n\n\nblock_size\nOptional[int]\nBlock size for reading (unused)\n\n\ncache_options\nOptional[dict]\nCache options (unused)\n\n\n**kwargs\nAny\nAdditional options\n\n\n\n\n\n\nReturns\nType\nDescription\n\n\n\n\nFile-like object\nFile-like object\nFile-like object for reading\n\n\n\n\n\n\nRaises\nType\nDescription\n\n\n\n\nValueError\nValueError\nIf mode is not supported\n\n\n\n\n\n\nGet file contents as bytes.\n\n\n\nParameter\nType\nDescription\n\n\n\n\npath\nstr\nFile path\n\n\n**kwargs\nAny\nAdditional options\n\n\n\n\n\n\nReturns\nType\nDescription\n\n\n\n\nbytes\nbytes\nFile content as bytes\n\n\n\n\n\n\nList directory contents.\n\n\n\nParameter\nType\nDescription\n\n\n\n\npath\nstr\nDirectory path to list\n\n\ndetail\nbool\nWhether to return detailed information\n\n\n**kwargs\nAny\nAdditional options\n\n\n\n\n\n\nReturns\nType\nDescription\n\n\n\n\nlist\nlist\nList of files/directories or their details\n\n\n\n\n\n\nCheck if file or directory exists.\n\n\n\nParameter\nType\nDescription\n\n\n\n\npath\nstr\nPath to check\n\n\n**kwargs\nAny\nAdditional options\n\n\n\n\n\n\nReturns\nType\nDescription\n\n\n\n\nbool\nbool\nTrue if path exists, False otherwise\n\n\n\n\n\n\nGet file information.\n\n\n\nParameter\nType\nDescription\n\n\n\n\npath\nstr\nFile path\n\n\n**kwargs\nAny\nAdditional options\n\n\n\n\n\n\nReturns\nType\nDescription\n\n\n\n\ndict\ndict\nDictionary with file information\n\n\n\n\n\n\nRaises\nType\nDescription\n\n\n\n\nFileNotFoundError\nFileNotFoundError\nIf file not found"
  },
  {
    "objectID": "api/fsspec_utils.core.base.html#filesystem",
    "href": "api/fsspec_utils.core.base.html#filesystem",
    "title": "fsspec_utils.core.base API Documentation",
    "section": "",
    "text": "Get filesystem instance with enhanced configuration options.\nCreates filesystem instances with support for storage options classes, intelligent caching, and protocol inference from paths.\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\n\n\n\n\nprotocol_or_path\nstr\nFilesystem protocol (e.g., “s3”, “file”) or path with protocol prefix\n\n\nstorage_options\nOptional[Union[BaseStorageOptions, dict]]\nStorage configuration as BaseStorageOptions instance or dict\n\n\ncached\nbool\nWhether to wrap filesystem in caching layer\n\n\ncache_storage\nOptional[str]\nCache directory path (if cached=True)\n\n\nverbose\nbool\nEnable verbose logging for cache operations\n\n\ndirfs\nbool\nWhether to wrap the filesystem in a DirFileSystem. Defaults to True.\n\n\nbase_fs\nAbstractFileSystem\nAn existing filesystem to wrap.\n\n\n**kwargs\nAny\nAdditional filesystem arguments\n\n\n\n\n\n\n\n\n\n\n\nRet\n\n\n\n\n\n\nAbstractFileSystem\nfsspec.AbstractFileSystem\nConfigured filesystem instance\n\n\n\nExample:\n# Basic local filesystem\nfs = filesystem(\"file\")\n\n# S3 with storage options\nfrom fsspec_utils.storage_options.cloud import AwsStorageOptions\nopts = AwsStorageOptions(region=\"us-west-2\")\nfs = filesystem(\"s3\", storage_options=opts, cached=True)\n\n# Infer protocol from path\nfs = filesystem(\"s3://my-bucket/\", cached=True)\n\n# GitLab filesystem\nfs = filesystem(\"gitlab\", storage_options={\n    \"project_name\": \"group/project\",\n    \"token\": \"glpat_xxxx\"\n})"
  },
  {
    "objectID": "api/fsspec_utils.core.base.html#get_filesystem",
    "href": "api/fsspec_utils.core.base.html#get_filesystem",
    "title": "fsspec_utils.core.base API Documentation",
    "section": "",
    "text": "Get filesystem instance with enhanced configuration options.\n\n\n\n\n\n\nDeprecated\n\n\n\nUse [`filesystem`](#filesystem) instead. This function will be removed in a future version.\n\n\nCreates filesystem instances with support for storage options classes, intelligent caching, and protocol inference from paths.\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\n\n\n\n\nprotocol_or_path\nstr\nFilesystem protocol (e.g., “s3”, “file”) or path with protocol prefix\n\n\nstorage_options\nOptional[Union[BaseStorageOptions, dict]]\nStorage configuration as BaseStorageOptions instance or dict\n\n\ncached\nbool\nWhether to wrap filesystem in caching layer\n\n\ncache_storage\nOptional[str]\nCache directory path (if cached=True)\n\n\nverbose\nbool\nEnable verbose logging for cache operations\n\n\n**kwargs\nAny\nAdditional filesystem arguments\n\n\n\n\n\n\n\n\n\n\n\nReturns\nType\nDescription\n\n\n\n\nfsspec.AbstractFileSystem\nfsspec.AbstractFileSystem\nConfigured filesystem instance\n\n\n\nExample:\n# Basic local filesystem\nfs = get_filesystem(\"file\")\n\n# S3 with storage options\nfrom fsspec_utils.storage_options.cloud import AwsStorageOptions\nopts = AwsStorageOptions(region=\"us-west-2\")\nfs = get_filesystem(\"s3\", storage_options=opts, cached=True)\n\n# Infer protocol from path\nfs = get_filesystem(\"s3://my-bucket/\", cached=True)\n\n# GitLab filesystem\nfs = get_filesystem(\"gitlab\", storage_options={\n    \"project_name\": \"group/project\",\n    \"token\": \"glpat_xxxx\"\n})"
  },
  {
    "objectID": "installation.html",
    "href": "installation.html",
    "title": "Installation",
    "section": "",
    "text": "fsspec-utils can be installed using pip, the Python package installer.\n\n\n\nPython 3.8 or higher is required.\n\n\n\n\nTo install fsspec-utils using pip, run the following command:\npip install fsspec-utils\n\n\nTo upgrade fsspec-utils to the latest version, use:\npip install --upgrade fsspec-utils\n\n\n\n\nFor robust dependency management and faster installations, we recommend using uv or pixi.\n\n\nuv is a fast Python package installer and resolver. To install fsspec-utils with uv:\nuv pip install fsspec-utils\n\n\n\npixi is a modern package manager for Python and other languages. To add fsspec-utils to your pixi project:\npixi add fsspec-utils\n\n\n\n\nIf you encounter any issues during installation, consider the following:\n\nPython Version: Ensure you are using Python 3.8 or higher. You can check your Python version with python --version.\nVirtual Environments: It is highly recommended to use a virtual environment (e.g., venv, conda, uv, pixi) to avoid conflicts with system-wide packages.\nPermissions: If you encounter permission errors, you might need to run the installation command with sudo (e.g., sudo pip install fsspec-utils), but this is generally not recommended in a virtual environment.\nNetwork Issues: Check your internet connection if the installation fails to download packages.\n\nFor further assistance, please refer to the official fsspec-utils GitHub repository or open an issue."
  },
  {
    "objectID": "installation.html#prerequisites",
    "href": "installation.html#prerequisites",
    "title": "Installation",
    "section": "",
    "text": "Python 3.8 or higher is required."
  },
  {
    "objectID": "installation.html#install-with-pip",
    "href": "installation.html#install-with-pip",
    "title": "Installation",
    "section": "",
    "text": "To install fsspec-utils using pip, run the following command:\npip install fsspec-utils\n\n\nTo upgrade fsspec-utils to the latest version, use:\npip install --upgrade fsspec-utils"
  },
  {
    "objectID": "installation.html#environment-management-with-uv-and-pixi",
    "href": "installation.html#environment-management-with-uv-and-pixi",
    "title": "Installation",
    "section": "",
    "text": "For robust dependency management and faster installations, we recommend using uv or pixi.\n\n\nuv is a fast Python package installer and resolver. To install fsspec-utils with uv:\nuv pip install fsspec-utils\n\n\n\npixi is a modern package manager for Python and other languages. To add fsspec-utils to your pixi project:\npixi add fsspec-utils"
  },
  {
    "objectID": "installation.html#troubleshooting",
    "href": "installation.html#troubleshooting",
    "title": "Installation",
    "section": "",
    "text": "If you encounter any issues during installation, consider the following:\n\nPython Version: Ensure you are using Python 3.8 or higher. You can check your Python version with python --version.\nVirtual Environments: It is highly recommended to use a virtual environment (e.g., venv, conda, uv, pixi) to avoid conflicts with system-wide packages.\nPermissions: If you encounter permission errors, you might need to run the installation command with sudo (e.g., sudo pip install fsspec-utils), but this is generally not recommended in a virtual environment.\nNetwork Issues: Check your internet connection if the installation fails to download packages.\n\nFor further assistance, please refer to the official fsspec-utils GitHub repository or open an issue."
  }
]